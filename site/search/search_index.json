{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the UArizona HPC Documentation Site","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The University of Arizona offers High Performance Computing (HPC) resources in the Research Data Center (RDC), a state-of-the-art facility that hosts our large computer clusters. HPC services are available at no cost to researchers. Each faculty member is eligible for a free standard allocation of CPU time and storage space. </p> <p>This documentation site provides technical details relevant to using our HPC system. Whether you are just starting your journey into computational sciences or are a seasoned programmer, we hope you will find something useful in these pages. This site is managed by the HPC Consult team. Please contact us if you have questions or comments about the content of this site.</p>"},{"location":"#featured-links","title":"Featured Links","text":"<p>  Account Creation </p> <p>If you are an active UArizona affiliate (e.g. student, post-doc, faculty), you can register an account. If you are not affiliated with UArizona but are working with collaborators here, you can register as a Designated Campus Colleague (DCC). </p> <p> HPC Quick Start </p> <p>If you are new to the UArizona HPC, or to HPC in general, our self-guided quick start tutorial will provide a solid foundation for using our system.</p> <p>  Getting Help | FAQs </p> <p> Find out how to contact the HPC Consult team, or view our FAQs, glossary, cheat sheets, and more! </p> <p>  Events Calendar |  Workshop Materials </p> <p>Every semester we host training sessions on topics including intro to HPC, machine learning, parallel computing, and beyond. Click the link above to see our workshop schedule, our old training materials, and find more workshops from around campus. </p>"},{"location":"#system-highlights-2024","title":"System Highlights 2024","text":"<p>Download Full Report</p> <p>Over the past four years, Puma has been a vital resource for researchers at the University of Arizona. In fiscal year 2024, our systems supported over 300 million compute hours enabling diverse research spanning climate modeling, genomics, neuroimaging, and more. With more than 80 active departments, 500 principal investigators, and 1,500 users, Puma continues to advance discovery and innovation.</p> <p>Researchers rely on our HPC resources not only for computational power, but also for expert support. This past year, we assisted users with nearly 2000 support requests. Explore highlights from this year, including testimonials from researchers and key statistics by downloading the full annual report.</p> <p> </p>"},{"location":"#highlighted-research","title":"Highlighted Research","text":"Ten Millionth JobPlanetary HistoryCloud ResearchHypersonic Flow <p>Puma has been a tremendous resource for our research community. Just recently it processed the 10 millionth job since we provisioned it in 2020. Just to get a perspective on that, if you took one step for each job you could walk to Niagara Falls. And back.</p> <p>David Castellano, a member of Dr. Ryan Gutenkunst's team, was the researcher who achieved this milestone. They study the evolutionary processes that generated the complex networks that comprise life. Dr. Gutenkunst told us that David\u2019s been maximizing his chances to hit this milestone with all the jobs he\u2019s been running.</p> <p>David says about his work: \u201cUnderstanding the relationship between DNA mutation rates and fitness effects is central to evolutionary biology. My work is investigating this relationship in three species: Homo sapiens, Mus musculus, and Arabidopsis thaliana. The inference of fitness effects from population genomics data requires intensive computation which could not be possible without a High Performance Computing service.\u201d</p> <p>The software used in their research is called  \u2018dadi\u2019: Diffusion Approximations for Demographic Inference. This work on three species with 96 mutation types and 1000 bootstrap replicates equates to 288,000 compute jobs.</p> <p>Reconstructing the History of the Solar System Using HPC</p> <p></p> <p>Erik Asphaug\u2019s Planetary Formation Lab in the Lunar and Planetary Laboratory uses smoothed-particle hydrodynamics (SPH) simulations to explore how collisions between bodies in the Solar System shape its evolution through time. These three-dimensional simulations, which approximate planetary bodies as collections of particles, incorporate realistic geologic properties to track their structural and thermal changes during and after giant impacts.  From Eric: \u201cThe access to increased time allocations as well as large volumes of temporary storage on xdisk provided by the HPC has revolutionized our ability to run our most complex simulations at high resolution, with enough space and time to explore the full parameter space necessary to make key discoveries that inform our understanding of Solar System evolution.\u201d</p> <p>One of their major projects has occupied a large fraction of their HPC hours and storage: the capture of Pluto\u2019s moon, Charon, from a giant impact early in the Solar System\u2019s history. High resolution is also critical to track detailed interactions between Pluto and Charon, including any material transferred between them. Without the HPC and the allocation of computation time and storage space, they would not have been able to run the hundreds of models necessary to successfully reproduce systems that look similar to Pluto and Charon today. The models have revealed new insights about how bodies like Pluto capture satellites: the dwarf planet and its proto-satellite collide, briefly merge, and then re-separate as Charon slow begins to move outward. They call this new process, which significantly redefines our understanding of giant collisions, \u201ckiss and capture.\u201d An example kiss-and-capture is shown in the image above. The simulation shown covers 60 hours of model time, which takes ~1.5 months on the HPC. The ability to run such long simulations in parallel was crucial to completing this work. </p> <p>Read more about the full story here!</p> <p> Sylvia Sullivan is an Assistant Professor in Chemical and Environmental Engineering who performs atmospherically related research and has a joint appointment to the Department of Hydrology and Atmospheric Sciences. Her academic background is in chemical engineering, but she has picked up atmospheric science and computing skills along the way to model and understand cloud and storm systems. \u201cI really liked environmental work because I felt it was very impactful,\u201d she says.  Her research includes investigating cloud ice formation. From a chemical engineering perspective, you can think about clouds as a control volume, flows in and out and phase changes occurring inside. Along with this more technical view, Sylvia says she \u201cfell in love with clouds because they are very beautiful and poetic\u201d. This blend of fields brought her to the University of Arizona as it is one of the only Universities where Chemical and Environmental Engineering are in the same department. And besides, \u201cTucson is a wonderful location\u201d.</p> <p> She is building a research group to study the impact of ice clouds, particularly their energetic and precipitation effects. Sylvia\u2019s group runs very high-resolution simulations called storm resolving simulations, where the meshes are fine enough to represent individual storms. In global climate models, the mesh has a resolution on the order of 100 km, in which several storm cells can form simultaneously. These storm-resolving computations are very expensive and produce terabytes of data, which then need to be post-processed and visualized. Currently, Sylvia and her group are very focused on working with other visualization experts on campus to illustrate the structures and evolution of clouds and storm systems.</p> <p>Faster Speeds Need Faster Computation</p> <p></p> <p>Professors Christoph Hader, Hermann Fasel, and their team are exploring the use of our GPUs to optimize Navier-Stokes codes for simulating the flow field around hypersonic vehicles traveling at size times the speed of sound (Mach 6) or more.</p> <p>In the image to the right, instantaneous flow structures obtained from a DNS for a flared cone at Mach 6 are visualized using the Q-isocontours colored with instantaneous temperature disturbance values. The small scales towards the end of the computational domain indicate the regions where the boundary layer is turbulent. </p>"},{"location":"#available-resources","title":"Available Resources","text":"Our Clusters (click to expand) PumaOceloteEl Gato <p> Implemented in the middle of 2020, Puma is the biggest cat yet. Similar to Ocelote, it has standard CPU nodes (with 94 cores and 512 GB of memory per node), GPU nodes (with Nvidia V100) and two high-memory nodes (3 TB). Local scratch storage increased to ~1.4 TB. Puma runs on Rocky Linux 9.</p> <p>As is the case for our other supercomputers, we use the RFP process to get the best value for our financial resources, that meet our technical requirements. This time Penguin Computing one with AMD processors. This is tremendously valuable as each node comes with:</p> <ul> <li>Two AMD Zen2 48 core processors</li> <li>512GB RAM</li> <li>25Gb path to storage</li> <li>25Gb path to other nodes for MPI</li> <li>2TB internal NVME disk (largely available as /tmp)</li> <li>Qumulo all flash storage array for shared filesystems</li> <li>Two large memory nodes with 3TB memory and the same processors and memory as the other nodes</li> <li>Six nodes with four Nvidia V100S GPU's each</li> </ul> <p> Ocelote arrived in 2016. Lenovo's Nextscale M5 technology was the winner of the RFP mainly on price, performance and meeting our specific requirements. Ocelote has one large memory node with 2TB of memory and 46 nodes with Nvidia P100 GPUs for GPU-accelerated workflows. This cluster is actually the next generation of the IBM cluster we call El Gato. Lenovo purchased IBM's Intel server line in 2015.</p> <p>In 2021, Ocelote's operating system was upgraded from CentOS6 to CentOS7 and was configured to use SLURM, like Puma. It will continue until it is either too expensive to maintain or it is replaced by something else.     - Intel Haswell V3 28 core processors     - 192GB RAM per node     - FDR infiniband for fast MPI interconnect     - Qumulo all flash storage array (all HPC storage is integrated into one array)     - One large memory node with 2TB RAM, Intel Ivy Bridge V2 48 cores     - 46 nodes with Nvidia P100 GPU's</p> <p> Implemented at the start of 2014, El Gato has been reprovisioned with CentOS 7 and new compilers and libraries. From July 2021 it has been using Slurm for job submission. El Gato is our smallest cluster with 130 standard nodes each with 16 CPUs. Purchased by an NSF MRI grant by researchers in Astronomy and SISTA.</p>  Compute <p>UArizona HPC systems are available to all university faculty, staff, undergraduate and graduate students, postdocs, and designated campus colleagues (DCCs) at no cost. Researchers have access to compute resources on our three clusters Puma, Ocelote, and El Gato located in our data center. Presently each research group is provided with a free standard monthly allocation on each: 100,000 CPU-hours on Puma, 70,000 CPU-hours on Ocelote, and 7,000 CPU-hours on El Gato.</p>  Funding Sources <p>UArizona HPC systems are funded through the UArizona Research Office (RII) and CIO/UITS (Chief Information Officer, and University Information Technology Services). Staff is funded to administer the systems and provide consulting services (no charge) for all researchers.</p>  Regulated Research <p>These resources specifically do not support Regulated Research, which might be ITAR, HIPAA or CUI (Controlled Unclassified Information). For more information on services that can support regulated research, see: HIPAA support services and CUI support services.</p>"},{"location":"#news","title":"News","text":"<ul> <li> <p> Puma OS Update</p> <p>As of January 29th, 2025, Puma\u2019s operating system has been updated from CentOS 7 to Rocky Linux 9. Need help transitioning to the new operating system? Visit our migration documentation for detailed instructions.</p> </li> <li> <p> Fall Semester Workshops</p> <p>This Fall semester we are conducting the workshops in a different manner.  Rather than compress them into a week, there will be one each Friday at 11am.  We plan to use a hybrid modality \u2013 you can attend in person which provides greater opportunity to engage; or attend virtually by Zoom. In person sessions will be held in Weaver Science and Engineering Library Rm 212. There will be a recorded version made available on YouTube.</p> <p>Registration Form Calendar </p> </li> <li> <p> July 2024 Maintenance</p> <ul> <li>User portal interface change for mobile compatibility. </li> <li>Open OnDemand graphical jobs limited to four days, reduced from 10 days. For workflows that need longer than four days, batch jobs can be used. Contact our consultants for help if you're unsure how to do this. </li> <li>New partitions have been introduced for GPU jobs. This will prevent non-GPU jobs from running on GPU nodes, improving availability. See batch directives for more information on how to request GPU nodes. </li> </ul> </li> <li> <p> New Ocelote GPUs</p> <p>We recently added 22 new P100 GPUs to Ocelote. Need to request multiple GPUs on a node and you're finding Puma queue times too slow? You can now request two GPUs per node on Ocelote using <code>--gres=gpu:2</code>. </p> </li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Published research that utilized UArizona HPC resources should follow our guidelines on how to acknowledge us. </p> <p>If you wish for your research to be featured in our Results page, please contact HPC consult with news of the publication!</p> <p>We respectfully acknowledge the University of Arizona is on the land and territories of Indigenous peoples. Today, Arizona is home to 22 federally recognized tribes, with Tucson being home to the O\u2019odham and the Yaqui. Committed to diversity and inclusion, the University strives to build sustainable relationships with sovereign Native Nations and Indigenous communities through education offerings, partnerships, and community service.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/10/30/a-new-era-in-research-the-nobel-prizes-showcase-ais-transformative-power/","title":"A New Era in Research: The Nobel Prizes Showcase AI\u2019s Transformative Power","text":"<p>What do two of the recent Nobel Prize awards have in common? In both cases of the Chemistry award and the Physics award, the committees recognized the transformative power of artificial intelligence, and the high-performance computing (HPC) that underpins it.</p> <p>Geoffrey Hinton of Google DeepMind and Princeton professor John J Hopfield received the physics honor for their groundbreaking work in artificial neural networks. \u201cThe laureates\u2019 work has already been of the greatest benefit. In physics we use artificial neural networks in a vast range of areas, such as developing new materials with specific properties,\u201d says Ellen Moons, Chair of the Nobel Committee for Physics.</p> <p>Demis Hassabis and John Jumper from Google's AI division DeepMind and David Baker from the University of Washington were awarded the 2024 Nobel Prize in Chemistry. Hassabis and Jumper received the award for AlphaFold2, an AI system that accurately predicts the 3D structures of proteins from their amino acid sequences in minutes. AlphaFold has predicted over 200 million protein structures, and has, so far, over 2 million users. This means it has already potentially saved millions of dollars and hundreds of millions of years in research time.</p> <p>Researchers at the University of Arizona take advantage of AlphaFold to advance their studies of protein folding using Puma, our newer supercomputer.  We host the dataset to save researchers a lot of time, storage capacity, and provide compute performance.  Our copy has over 200,000 files in 2.8TB. The latest is a lot bigger. We also host the containers with the code required.</p>"},{"location":"blog/2025/01/14/supercomputing-2024/","title":"Supercomputing 2024","text":"<p>Our research computing team was represented at the recent Supercomputing 2024 Conference in Atlanta. We typically collaborate with Arizona State University (ASU) and Northern Arizona University (NAU) for booth presence. However, this year we greatly increased our footprint and teamed up with our Institute for Computation &amp; Data-Enabled Insight (ICDI). Much work went on in preparation that enabled us to offer several VR experiences that highlighted UArizona research and iconic sites like the Biosphere 2 and telescopes that in part comprise the Event Horizon Telescope</p> <p>We also welcomed Dr. Bryan Carter and Ash Black who brought VR experiences and provided conference experience for their students. </p>"},{"location":"blog/2024/11/05/tier-2-storage/","title":"Tier 2 Storage","text":"<p>The University of Arizona\u2019s Tier 2 Storage, designed to utilize Amazon Web Service storage solutions, offers scalable storage for research data that does not require immediate access. Unlike Tier 1 storage, which is integrated into high-performance computing (HPC), Tier 2 is intended for archiving inactive data or making copies. It employs AWS\u2019s intelligent tiering, which progressively moves data to more cost-effective, long-term storage options like Glacier and Deep Glacier after periods of inactivity.</p> <p>Tier 2 AWS buckets use intelligent tiering to determine the archival status of files. When data are first uploaded to a group's bucket, they are in the standard access class. After three months of inactivity, data are automatically migrated to Glacier storage. This is less performant, and data are no longer instantly downloadable. After three months of inactivity in the Glacier access tier, data are automatically migrated to Deep Glacier, which is even less performant. Users will need to request a restore before downloading their files from Glacier and Deep Glacier. Restore requests can be submitted either in the user portal or using a command line tool available on our compute nodes.</p> <p>Did I mention that the service is highly subsidized?  You will set up an account to cover the costs of the data that is temporarily in S3, but there are no charges for data migrated to Glacier and Deep Glacier. And there are no ingress or egress charges.</p> <p>For detailed usage and pricing information, visit the official Tier 2 Storage page.</p>"},{"location":"events/calendar/","title":"UArizona HPC Events Calendar","text":"<p>Every semester, we host training workshops on topics ranging from intro to HPC, to machine learning, to parallel computing, and beyond. In addition, there are many great opportunities for learning hosted by other organizations within the community. Whether you're interested in learning about specific programming languages, exploring data analytics techniques, or cloud computing, there's a wide array of learning opportunities available.</p> <p>Check below for a list of upcoming workshops.</p>"},{"location":"events/calendar/#hpc-calendar","title":"HPC Calendar","text":"<p>We are excited to host a full semester's worth of workshops this Fall. All workshops will be held on Fridays throughout the semester from 11am to noon in the Weaver Science and Engineering Library, Rm 212. The first two workshops have been moved to the Catalyst Studio Learning Center Room 254/252 in the Main Library! The registration button below will allow you to register for any/all of the workshops listed below. Please note that you will need to be logged into your University of Arizona account in order to access the registration form.</p> <p>Register</p>"},{"location":"events/calendar/#fall-2024-workshop-schedule","title":"Fall 2024 Workshop Schedule","text":"Date Topic Room Instructor Recording Slides Sep 6 HPC Overview and AccessAn introduction to what HPC is, basics of the U of A\u2019s HPC, how to create an account and login. Motivates the usage of HPC including examples from many research areas; outlines several common misconceptions about HPC; introduces the U of A\u2019s clusters; describes the basics of the UA HPC system architecture, including node types and proper usage. This workshop is aimed at first-time HPC users, or users new to the U of A\u2019s HPC system. No programming experience is assumed or required. Basic computer literacy is recommended. Main B252/254 Ethan Jahn Sep 13 Storage and Software OverviewAn introduction to the HPC systems related to data storage and software. Outlines the details of the HPC high performance storage system, the rental storage system, Research Desktop Attached Storage (R-DAS), and AWS Tier 2 storage. Covers the basics of Linux file permissions in the context of a shared cluster, including our conventions and best practices. Outlines methods for transferring files to/from the HPC storage system. Describes the module system used to provide software to users, and provides guidelines for personal software installations. Familiarity with basic programming recommended but not required. Participants should be familiar with topics covered in Workshop 1. Main B252/254 Ethan Jahn Sep 20 Scheduling and Running JobsThe ins-and-outs of running jobs on the HPC system. Provides details related to partitions and CPU-time allocations in relation to the scheduling system. Describes HPC hardware information relevant to submitting resource requests. Covers details for each type of job, including Open OnDemand graphical jobs, interactive terminal sessions, batch jobs, and array jobs. Provides best practices and clarification related to the queueing system and Slurm terminology. A basic batch job example is included. Basic programming experience including familiarity with bash recommended but not required. Participants should be familiar with topics covered in Workshops 1 and 2. Weaver 212 Ethan Jahn Sep 27 Managing and Optimizing JobsMotivates, defines, and describes tools/techniques for managing jobs, including Open OnDemand and command-line tools (both provided by Slurm \u2013 e.g. <code>squeue</code> \u2013 and other tools such as <code>seff</code> and <code>job-history</code>). Provides detailed examples and use cases for these tools. Additionally provides clarity and motivation for job optimization, including decreasing time to result and improving overall HPC throughput. Outlines best practices and principles of constructing optimal resource requests. Basic programming experience and familiarity with previous workshop topics is recommended. Weaver 212 Ethan Jahn Oct 4 Parallel Computing TheoryAn introduction to the principles and concepts of computing. Discusses basics of computer architecture and serial computing to provide context for developments in parallel computing. Covers paradigms and models of parallel computing. Other topics include scaling laws, load balancing, task decomposition, asynchronous computation, overhead, meta-parallelism, etc. Basics of writing parallel algorithms will be discussed. Basic programming experience and familiarity with general computing concepts are recommended. Weaver 212 Ethan Jahn Oct 11 Parallel Computing in PracticeImplementing parallelization often looks very different than describing it in abstract terms. Building on the previous session\u2019s material, this workshop will aim to provide some guidelines and examples of parallelization in practice. Software discussed includes OpenMPI, Python (multiprocessing &amp; mpi4py), R (parallel), Matlab, and potentially others. Some basic programming experience and familiarity with all previous workshop topics are highly recommended. Weaver 212 Ethan Jahn Oct 18 Software and EnvironmentsDebugging problems on the HPC is easier if you have reproducible environments. This workshop will provide some general guidelines and examples that you can follow to create and maintain such environments. It will focus on Python and R projects, and potentially an example for compiled languages. Online Only Soham Pal Oct 25 Machine Learning on HPCThis workshop will introduce you to using HPC resources for machine learning. It will focus on PyTorch examples, with general guidelines that maybe applicable to other frameworks. This is not an introduction to machine learning in general, and assumes that you have some basic knowledge of machine learning concepts. Online Only Soham Pal Nov 1 Intro to ContainersThis workshop introduces the concept of containers, and why they are useful in research computing. It focuses on the Apptainer container engine, and provides examples to show how you can build and run containers with Apptainer. Online Only Soham Pal Nov 8 Error Handling and DebuggingProvides an overview of concepts and techniques related to detecting and fixing errors, both in general and in the context of HPC. Includes discussion of best practices when coding and using HPC to avoid errors, a general approach to fixing errors, common types of errors, and how to use various tools to assist in the debugging process. We recommend that participants have some background experience with coding and using the HPC, though the workshop will be aimed at novice and intermediate HPC users. Weaver 212 Ethan Jahn Nov 13 Intro to StatisticsThis workshop combines three purposes, the first is an introduction to Statistics and its relevance to Machine Learning, the second is an introduction to the Matlab Machine Learning Toolbox, and the third is the use of HPC to run the examples from the Toolbox. This last one demonstrates how the HPC OnDemand functionality has very similar ease of use to a workstation. Online Only Chris Reidy Nov 22 Intro to VisualizationThe Intro to Data Visualization presentation aims to give attendees a select tour of the visualization field, bridging theory and practice. We will explore theory used to inform the selection of chart types, and contents that support a researchers intended message. We will then practice the standard visualization development cycle of drafting, critiquing, and refactoring for a number of examples. Attendees must bring with them material to draw with, and will leave with a general thought process that may apply to any visualization task. Optional coding exercises for applying these concepts are provided. Weaver 212 Devin Bayly Dec 6 Using GPUs on HPCThis workshop provides a basic introduction to GPUs, and how to access GPUs on UA HPC clusters. It provides some simple examples to demonstrate GPU accelerated computation, and discusses some common issues that arise when working with GPUs. Online Only Soham Pal Dec 13 Intermediate Techniques in Visualization Weaver 212 Devin Bayly <p>Room Information:</p> <ul> <li>Weaver Science Engineering Library: Room 212 Learning Studio</li> <li>Main Library: B252/254 Learning Studio</li> </ul>"},{"location":"events/calendar/#our-friends-and-partners","title":"Our Friends and Partners","text":""},{"location":"events/calendar/#upcoming-events","title":"Upcoming Events","text":"<p>Generative AI and Deep Learning with MATLAB: A Visual Approach</p> <ul> <li>When: Thursday, November 14, 11:30-1:00pm</li> <li>Location: Main Library B252/254, CATalyst Learning Studios</li> <li>Registration Link: https://libcal.library.arizona.edu/calendar/events/matlab</li> <li>Click to view flyer</li> </ul> <p>Fall 2024 U of A Datalab Workshops</p> <p>View these workshops on the DataLab Website</p> Date Topic Instructor Tues 2-3pm NextGen Geospatial: AI &amp; Cloud tools for Geographic Analysis Jeff Gillan Tues 3-4pm Exploring Tools for Data Analysis and AI Applications in Biosciences and Genomics Michele Cosi &amp; Carlos Lizarraga Weds 1-2:30pm Research Productivity Rudy Salcido Thurs 1-2pm Advanced AI for Healthcare: A Transformative Force Greg Chism Thurs 2-3pm Exploring the LLM Frontier: From Hugging Face to RAG and Beyond Carlos Lizarraga &amp; Enrique Noriega Atala Thurs 3-4pm Natural Language Processing for All Megh Krishnaswamy"},{"location":"events/calendar/#external-event-pages","title":"External Event Pages","text":"<p>Please check out these workshops and activities from our partners around campus!</p> <ul> <li>The Data Science Institute's official calendar</li> <li>Data Lab home page</li> <li>The UArizona Libraries event calendar</li> </ul>"},{"location":"events/workshop_materials/","title":"HPC Workshop Materials","text":""},{"location":"events/workshop_materials/#hpc-workshops-and-training-materials","title":"HPC Workshops and Training Materials","text":"<p>This page contains a catalogue of our previous training materials. Feel free to browse, interact, download, and share as you see fit. See our Calendar for upcoming virtual or in-person workshop events. </p>"},{"location":"events/workshop_materials/#fall-2024","title":"Fall 2024","text":"Date Topic Instructor Recording Slides Sep 6 HPC Overview and AccessAn introduction to what HPC is, basics of the U of A\u2019s HPC, how to create an account and login. Motivates the usage of HPC including examples from many research areas; outlines several common misconceptions about HPC; introduces the U of A\u2019s clusters; describes the basics of the UA HPC system architecture, including node types and proper usage. This workshop is aimed at first-time HPC users, or users new to the U of A\u2019s HPC system. No programming experience is assumed or required. Basic computer literacy is recommended. Ethan Jahn Sep 13 Storage and Software OverviewAn introduction to the HPC systems related to data storage and software. Outlines the details of the HPC high performance storage system, the rental storage system, Research Desktop Attached Storage (R-DAS), and AWS Tier 2 storage. Covers the basics of Linux file permissions in the context of a shared cluster, including our conventions and best practices. Outlines methods for transferring files to/from the HPC storage system. Describes the module system used to provide software to users, and provides guidelines for personal software installations. Familiarity with basic programming recommended but not required. Participants should be familiar with topics covered in Workshop 1. Ethan Jahn Sep 20 Scheduling and Running JobsThe ins-and-outs of running jobs on the HPC system. Provides details related to partitions and CPU-time allocations in relation to the scheduling system. Describes HPC hardware information relevant to submitting resource requests. Covers details for each type of job, including Open OnDemand graphical jobs, interactive terminal sessions, batch jobs, and array jobs. Provides best practices and clarification related to the queueing system and Slurm terminology. A basic batch job example is included. Basic programming experience including familiarity with bash recommended but not required. Participants should be familiar with topics covered in Workshops 1 and 2. Ethan Jahn Sep 27 Managing and Optimizing JobsMotivates, defines, and describes tools/techniques for managing jobs, including Open OnDemand and command-line tools (both provided by Slurm \u2013 e.g. <code>squeue</code> \u2013 and other tools such as <code>seff</code> and <code>job-history</code>). Provides detailed examples and use cases for these tools. Additionally provides clarity and motivation for job optimization, including decreasing time to result and improving overall HPC throughput. Outlines best practices and principles of constructing optimal resource requests. Basic programming experience and familiarity with previous workshop topics is recommended. Ethan Jahn Oct 4 Parallel Computing TheoryAn introduction to the principles and concepts of computing. Discusses basics of computer architecture and serial computing to provide context for developments in parallel computing. Covers paradigms and models of parallel computing. Other topics include scaling laws, load balancing, task decomposition, asynchronous computation, overhead, meta-parallelism, etc. Basics of writing parallel algorithms will be discussed. Basic programming experience and familiarity with general computing concepts are recommended. Ethan Jahn Oct 11 Parallel Computing in PracticeImplementing parallelization often looks very different than describing it in abstract terms. Building on the previous session\u2019s material, this workshop will aim to provide some guidelines and examples of parallelization in practice. Software discussed includes OpenMPI, Python (multiprocessing &amp; mpi4py), R (parallel), Matlab, and potentially others. Some basic programming experience and familiarity with all previous workshop topics are highly recommended. Ethan Jahn Oct 18 Software and EnvironmentsDebugging problems on the HPC is easier if you have reproducible environments. This workshop will provide some general guidelines and examples that you can follow to create and maintain such environments. It will focus on Python and R projects, and potentially an example for compiled languages. Soham Pal Oct 25 Machine Learning on HPCThis workshop will introduce you to using HPC resources for machine learning. It will focus on PyTorch examples, with general guidelines that maybe applicable to other frameworks. This is not an introduction to machine learning in general, and assumes that you have some basic knowledge of machine learning concepts. Soham Pal Nov 1 Intro to ContainersThis workshop introduces the concept of containers, and why they are useful in research computing. It focuses on the Apptainer container engine, and provides examples to show how you can build and run containers with Apptainer. Soham Pal Nov 8 Error Handling and DebuggingProvides an overview of concepts and techniques related to detecting and fixing errors, both in general and in the context of HPC. Includes discussion of best practices when coding and using HPC to avoid errors, a general approach to fixing errors, common types of errors, and how to use various tools to assist in the debugging process. We recommend that participants have some background experience with coding and using the HPC, though the workshop will be aimed at novice and intermediate HPC users. Ethan Jahn Nov 13 Intro to StatisticsThis workshop combines three purposes, the first is an introduction to Statistics and its relevance to Machine Learning, the second is an introduction to the Matlab Machine Learning Toolbox, and the third is the use of HPC to run the examples from the Toolbox. This last one demonstrates how the HPC OnDemand functionality has very similar ease of use to a workstation. Chris Reidy [] [] Nov 22 Intro to VisualizationThe Intro to Data Visualization presentation aims to give attendees a select tour of the visualization field, bridging theory and practice. We will explore theory used to inform the selection of chart types, and contents that support a researchers intended message. We will then practice the standard visualization development cycle of drafting, critiquing, and refactoring for a number of examples. Attendees must bring with them material to draw with, and will leave with a general thought process that may apply to any visualization task. Optional coding exercises for applying these concepts are provided. Devin Bayly -"},{"location":"events/workshop_materials/#previous-workshops","title":"Previous Workshops","text":"<ul> <li> <p> Introduction to HPC</p> <p>If you are new to our HPC system, or new to HPC in general, then this training will help guide you through everything from account creation, to transferring data, to running basic jobs.</p> <p> Introduction to HPC </p> </li> <li> <p> Introduction to Linux</p> <p>Covers the basics of Linux operating systems, including the HPC filesystem, bash commands, environment variables, and best practices. </p> <p> PDF Slides PPT Slides  Video </p> </li> <li> <p> Parallel Computing</p> <p>An introduction to the theory and practice of parallel computing.</p> <p> PDF Slides  Video </p> </li> <li> <p> Machine Learning</p> <p>This workshop introduces some of the basic concepts of ML, and techniques that can be used on the UArizona HPC.</p> <p> Overview ML in Python ML in R </p> </li> <li> <p> Containers</p> <p>Containers are self-contained software packages that include all the necessary dependencies to run a particular program. They can be useful for porting your analysis to a shared environment like the HPC. This workshop covers the basics of containers and how to use them on HPC.</p> <p> Intro to Containers  Video </p> </li> <li> <p> Visualization</p> <p>This workshop provides an introduction to some concepts of visualization. It is done in the context of HPC so you will be able to follow along with the practical examples section using your HPC Account.</p> <p> PDF Slides Jupyter notebook </p> </li> <li> <p> Statistics</p> <p>An overview of how to use the Matlab Statistics and Machine Learning toolbox on the UArizona HPC.</p> <p> PDF Slides PPT Slides Examples  Video </p> </li> <li> <p> Data Management</p> <p>Learn about managing your data on UA's HPC cluster. Co-sponsored with University Libraries.</p> <p> PDF (Part 1) PDF (Part 2) </p> </li> <li> <p> Managing and Optimizing Jobs</p> <p>Learn about how to request and schedule different types of jobs on the HPC, how to manage existing and previous jobs, and how to optimize your resource requests.</p> <p> PDF Slides  Video </p> </li> </ul>"},{"location":"events/workshop_materials/data_management_workshops/","title":"Index","text":""},{"location":"events/workshop_materials/data_management_workshops/#data-management-workshops","title":"Data Management Workshops","text":"<p>Learn about managing your data on UA's HPC cluster. Co-sponsored with University Libraries. Virtual only</p> <p>These workshops are held twice a year. This semester there will be one two-hour workshop rather than two one-hour workshops.</p> <p>Attendance will be on zoom so there is no registration needed.</p>"},{"location":"events/workshop_materials/data_management_workshops/#presentation-pdfs","title":"Presentation PDFs","text":"<p> Click here to download PDF (Part 1) Click here to download PDF (Part 2) </p>"},{"location":"events/workshop_materials/intro_to_containers/","title":"Index","text":""},{"location":"events/workshop_materials/intro_to_containers/#intro-to-containers-on-hpc","title":"Intro to Containers on HPC","text":"<p>There are times when the use of containers is more efficient for using HPC resources, and there are other times when it is the only way. This workshop will give you a quick view of some of the concepts and help you get familiar with Apptainer containers with some interactive examples.</p> <p>See a presentation of this material from our Summer 2024 workshop series here:</p>"},{"location":"events/workshop_materials/intro_to_containers/#what-is-a-container","title":"What is a container?","text":"<p>\"A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.\" \u2013 Docker</p> <pre><code>block-beta\n  columns 1\n  block:group1:1\n      %% columns auto (default)\n      a(\"App 1\")\n      b(\"App 2\")\n      c(\"App 3\")\n      d(\"App 4\")\n      e(\"App 5\")\n  end\n  x[\"Container Engine\"]:1\n  y[\"Host OS\"]:1\n  z[\"Host Hardware\"]:1</code></pre>"},{"location":"events/workshop_materials/intro_to_containers/#why-use-containers","title":"Why use containers?","text":"<ul> <li>Work in reproducible environments of your choosing and design<ul> <li>You are not limited to the libraries and tools on the host system</li> <li>You can reproduce the same behavior across different systems</li> </ul> </li> <li>Containers are immutable<ul> <li>Users cannot modify a container</li> <li>Users can create new containers based on existing ones</li> </ul> </li> <li>Limits user privileges</li> <li>Make use of GPUs and high-speed networks</li> </ul>"},{"location":"events/workshop_materials/intro_to_containers/#container-engines","title":"Container engines","text":"<ul> <li>Docker (most popular, but not suitable for shared clusters)</li> <li>Podman</li> <li>CharlieCloud</li> <li>Apptainer (earlier known as Singularity)</li> <li>... </li> </ul> <p>Apptainer was developed with shared clusters, and it is the container engine supported at UA HPC. For more information on the differences between Apptainer and the others, and why we use Apptainer, read Singularity: Scientific containers for mobility of compute.</p>"},{"location":"events/workshop_materials/intro_to_containers/#how-to-use-apptainer-containers","title":"How to use Apptainer containers","text":"<ol> <li>Pull or build an Apptainer container in an interactive session<ul> <li>Apptainer can build containers from Docker containers</li> </ul> </li> <li>Run the Apptainer container from an interactive session or a batch submission script</li> </ol>"},{"location":"events/workshop_materials/intro_to_containers/#your-first-apptainer-container","title":"Your first Apptainer container","text":"<ol> <li>Run <code>elgato</code></li> <li>Start an interactive session with <code>interactive -a &lt;pi-account-name&gt;</code></li> <li>Pull an Apptainer image with <code>apptainer pull docker://ghcr.io/apptainer/lolcow</code></li> </ol> <p>You will see something like the following:</p> <pre><code>[sohampal@cpu43 sohampal]$ apptainer pull docker://ghcr.io/apptainer/lolcow\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 16ec32c2132b done   | \nCopying blob 5ca731fc36c2 done   | \nCopying config fd0daa4d89 done   | \nWriting manifest to image destination\n2024/07/17 15:50:32  info unpack layer: sha256:16ec32c2132b43494832a05f2b02f7a822479f8250c173d0ab27b3de78b2f058\n2024/07/17 15:50:34  info unpack layer: sha256:5ca731fc36c28789c5ddc3216563e8bfca2ab3ea10347e07554ebba1c953242e\nINFO:    Creating SIF file...\n</code></pre> <p>To run the container, run <code>apptainer run lolcow_latest.sif</code> from the interactive session:</p> <pre><code>[sohampal@cpu43 sohampal]$ apptainer run lolcow_latest.sif \n ______________________________\n&lt; Wed Jul 17 15:52:49 MST 2024 &gt;\n ------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"events/workshop_materials/intro_to_containers/#container-registries","title":"Container registries","text":"<p>You can pull containers from repositories, also known as container registries. Some useful ones are:</p> <ul> <li>Docker hub<ul> <li>The most popular container registry</li> <li>Multiple providers for each application (try to stick to official containers)</li> </ul> </li> <li>NVIDIA NGC Catalog<ul> <li>Provides Docker containers for various GPU accelerated packages (including Tensorflow and PyTorch)</li> <li>For more information on using NVIDIA containers on UA HPC, see Pulling Nvidia Images</li> </ul> </li> <li>BioContainers<ul> <li>Provides tons of Docker containers for bioinformatics</li> </ul> </li> <li>Neurodesk<ul> <li>Provides Apptainer containers for neuroimaging</li> </ul> </li> <li>Sylabs<ul> <li>Provides Apptainer containers</li> </ul> </li> </ul> <p>For more Apptainer-compatible registries, see Apptainer documentation. Sometimes Apptainer containers created from Docker containers might not work exactly the same way as they would with Docker. If you find that the developers provide corresponding Apptainer containers (or definition files), then use those.</p>"},{"location":"events/workshop_materials/intro_to_containers/#what-if-no-registry-has-the-container-you-need","title":"What if no registry has the container you need?","text":"<p>Simple, build it yourself!</p> <p>We will look at a simple example of building an Apptainer container. Create a file with the following contents, and name it <code>nyancat</code>:</p> <pre><code>Bootstrap: docker\nFrom: debian:bookworm-slim\n\n%post\n  apt update -y &amp;&amp; apt install -y nyancat\n\n%runscript\n  nyancat\n</code></pre> <p>Here is some explanation of what each line in the file means:</p> <ul> <li><code>Bootstrap: docker</code>: This tells Apptainer that we want to use a container from Docker hub as a base for this container.</li> <li><code>From: debian:bookworm-slim</code>: This provides the name of container that we want to use as the base. In this case we are using a container based on the Debian Linux system as the base. Bookworm is the latest version of Debian. The part after <code>:</code> in <code>debian:bookworm-slim</code>, i.e. <code>bookworm-slim</code>, is usually called the tag, and used for versioning; <code>-slim</code> indicates that this particular container has a minimal number of packages.</li> <li><code>%post</code>: The <code>%</code> indicates a section in the file. All commands for installing software are mentioned under this section.</li> <li><code>apt update -y &amp;&amp; apt install -y nyancat</code>: These commands check for updates and then installs the <code>nyancat</code> package. Notice, we do not use <code>sudo</code> before the commands.</li> <li><code>%runscript</code>: In this section you indicate which commands will run by default when you run the container.</li> <li><code>nyancat</code>: <code>nyancat</code> under <code>%runscript</code> means that <code>apptainer run nyancat.sif</code> will run <code>nyancat</code>.</li> </ul> <p>This file is known as an Apptainer Definition file, it can be far more complex than this example. For more information, see Definition Files. You can build any container from a definition with <code>apptainer build &lt;path-to-container&gt; &lt;path-to-definition-file&gt;</code>. In this case it will be <code>apptainer build nyancat.sif nyancat</code>.</p> <p>You can run it with <code>apptainer run nyancat.sif</code>. (Run it and be amazed. You can cancel it with <code>Ctrl + C</code>.) To run something other than the default program, use <code>apptainer exec &lt;path-to-container&gt; &lt;command&gt;</code>. For example:</p> <pre><code>[sohampal@cpu37 sohampal]$ apptainer exec nyancat.sif whoami\nsohampal\n</code></pre> <p>Notice that our HPC clusters run on RedHat-based Linux OSs. However this container is based on Debian, a different OS. Containers allow you to \"mix-and-match\" certain parts of OSs.</p>"},{"location":"events/workshop_materials/intro_to_containers/#building-containers-for-python-and-r-packages","title":"Building containers for Python and R packages","text":"<p>Sometimes Python or R packages have dependencies which we cannot support on the HPC clusters. Containers can be an good alternative in such cases.</p> <ol> <li>Check if the package developers already provide containers (many often do)</li> <li>In the absence of ready-made containers try the suggestions below to build your own container</li> </ol>"},{"location":"events/workshop_materials/intro_to_containers/#python","title":"Python","text":"<p>For the base container, use:</p> <ul> <li>one of the official Python containers from Docker hub, if you do not need <code>conda</code></li> <li>one of the mambaforge containers from Docker hub, if you need <code>conda</code> (provides <code>mamba</code>, a faster implementation of <code>conda</code>)</li> </ul> <p>The example below shows a Definition file for a container containing the Astropy Python package. You can use this as a template to write your own Definition files.</p> <pre><code>Bootstrap: docker\nFrom: python:3.11-slim-bookworm\n\n%post\n  pip install ipython matplotlib astropy\n\n%runscript\n  ipython\n</code></pre> <p>You can build it as before. When you run it, it will start an IPython shell, in which you can import Astropy.</p> <pre><code>[sohampal@cpu37 sohampal]$ apptainer run astropy_latest.sif \nPython 3.11.9 (main, May 14 2024, 08:15:15) [GCC 12.2.0]\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.25.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import astropy\n\nIn [2]: \n</code></pre>"},{"location":"events/workshop_materials/intro_to_containers/#r","title":"R","text":"<p>For the base containers, use one of <code>rstudio</code> containers provided by Rocker on Docker hub. Depending on what packages you need you might even find a ready-made container provided by Rocker.</p> <p>The example below shows a Defnition file for container with the Signac R package, used for analysis of single-cell chromatin data. Signac requires installation of some OS-level libraries and some BioConductor packages.</p> <pre><code>Bootstrap: docker\nFrom: rocker/rstudio:4.3.2\n\n%post\n  apt update -y\n  apt install -y --no-install-recommends liblzma-dev libbz2-dev patch zlib1g-dev\n\n  export R_LIBS_USER=/usr/lib/R/site-library\n\n  R -e 'install.packages(\"BiocManager\")'\n  R -e 'BiocManager::install(c(\"GenomeInfoDb\", \"GenomicRanges\", \"IRanges\", \"Rsamtools\", \"S4Vectors\", \"BiocGenerics\"))'\n  R -e 'setRepositories(ind=1:3); install.packages(Signac\")'\n\n  apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n%runscript\n  R\n</code></pre> <p>After building the container if you run it from the command line, it will take to you a R prompt and you can load the Signac library.</p> <pre><code>[sohampal@cpu37 sohampal]$ ./r-signac.sif \n\nR version 4.3.2 (2023-10-31) -- \"Eye Holes\"\nCopyright (C) 2023 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt; library(Signac)\nSignac built for for SeuratObject v4 was just loaded with SeuratObject\nv5; disabling v5 assays and validation routines, and ensuring assays\nwork in strict v3/v4 compatibility mode\n&gt; \n</code></pre> <p>To use RStudio, see Using RStudio.</p>"},{"location":"events/workshop_materials/intro_to_containers/#containers-provided-by-ua-hpc","title":"Containers provided by UA HPC","text":"<p>We provide a few Apptainer containers. You can use them directly, or as a base to create your own containers. These are available at:</p> <ul> <li><code>/contrib/singularity/nvidia</code><ul> <li>Contains GPU enabled images from NVIDIA NGC</li> <li>Includes PyTorch and Tensorflow among others</li> </ul> </li> <li><code>/contrib/singularity/ua-hpc</code><ul> <li>Contains images not directly provided by NVIDIA</li> <li>Some like AlphaFold are GPU enabled</li> </ul> </li> </ul>"},{"location":"events/workshop_materials/intro_to_containers/#using-containers-in-batch-jobs","title":"Using containers in batch jobs","text":"<p>We will look at a simple Linear Regression example using one of the Tensorflow containers available under <code>/contrib/singularity/nvidia</code>. The Tensorflow, PyTorch, and AlphaFold containers are also available as modules, so you do not have to use <code>apptainer exec</code> or <code>apptainer run</code> when you run these containers. You simply have to load the corresponding module.</p> <p>First create a Python file with the following content:</p> <pre><code>#Linear Regression Example with TensorFlow v2 library \n\nfrom __future__ import absolute_import, division, print_function\n#\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras import optimizers\ntf.enable_eager_execution()\nrng = np.random\n#\n# Parameters.\nlearning_rate = 0.01\ntraining_steps = 1000\ndisplay_step = 50\n#\n# Training Data.\nX = np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n              7.042,10.791,5.313,7.997,5.654,9.27,3.1])\nY = np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n              2.827,3.465,1.65,2.904,2.42,2.94,1.3])\nn_samples = X.shape[0]\n#\n# Weight and Bias, initialized randomly.\nW = tf.Variable(rng.randn(), name=\"weight\")\nb = tf.Variable(rng.randn(), name=\"bias\")\n\n# Linear regression (Wx + b).\ndef linear_regression(x):\n    return W * x + b\n\n# Mean square error.\ndef mean_square(y_pred, y_true):\n    return tf.reduce_sum(tf.pow(y_pred-y_true, 2)) / (2 * n_samples)\n\n# Stochastic Gradient Descent Optimizer.\noptimizer = tf.keras.optimizers.SGD(learning_rate)\n#\n# Optimization process. \ndef run_optimization():\n# Wrap computation inside a GradientTape for automatic differentiation.\n    with tf.GradientTape() as g:\n        pred = linear_regression(X)\n        loss = mean_square(pred, Y)\n\n    # Compute gradients.\n    gradients = g.gradient(loss, [W, b])\n\n    # Update W and b following gradients.\n    optimizer.apply_gradients(zip(gradients, [W, b]))\n#\n# Run training for the given number of steps.\nfor step in range(1, training_steps + 1):\n    # Run the optimization to update W and b values.\n    run_optimization()\n\n    if step % display_step == 0:\n        pred = linear_regression(X)\n        loss = mean_square(pred, Y)\n        print(\"step: %i, loss: %f, W: %f, b: %f\" % (step, loss, W.numpy(), b.numpy()))\n</code></pre> <p>Now create a bash script with the following content (replace the placeholder variables with <code>&lt;&gt;</code> in their names with the actual values):</p> <pre><code>#!/bin/bash\n#SBATCH --output=Sample-tensorflow-example-%a.out\n#SBATCH --ntasks=1\n#SBATCH --nodes=1\n#SBATCH --time=00:01:00\n#SBATCH --gres=gpu:1\n#SBATCH --partition=standard\n#SBATCH --account=&lt;your-group&gt;\n\nmodule load tensorflow/nvidia/2.0.0\necho \"Tensorflow executable: \" $(which tensorflow)\necho \"The command tensorflow is a bash script added to your PATH. Contents of the executable: \" \necho\ncat $(which tensorflow)\necho\necho \"Executing: tensorflow &lt;path/to/python/script&gt;\"\necho \ntensorflow &lt;path/to/python/script&gt;\n</code></pre> <p>You can submit this with <code>sbatch &lt;path/to/bash/script&gt;</code>. For an example of using the AlphaFold container in a batch job, see AlphaFold 2.</p>"},{"location":"events/workshop_materials/intro_to_containers/#additional-resources","title":"Additional resources","text":"<ul> <li>Check out the Containers section in our docs.</li> <li>Apptainer fully supports using GPUs with the <code>--nv</code> flag. For more information on using Apptainer containers with GPUs, read GPU Support.</li> <li>If you want to build Apptainer containers with MPI support, read Apptainer and MPI applications. The MPI type and version inside the container must match the MPI type and version on the host system. Build your containers with Open MPI 3 or Intel MPI to match the MPI on our clusters.</li> </ul>"},{"location":"events/workshop_materials/intro_to_hpc/","title":"Index","text":""},{"location":"events/workshop_materials/intro_to_hpc/#intro-to-hpc-workshop-materials","title":"Intro to HPC Workshop Materials","text":"<p>Video recordings and slides of previous Intro to HPC workshops are available here on this page. The more recent workshops will have more updated information, but older workshops are also available.</p> ResBaz 2024Spring 2024Fall 2023 <p> <p> </p> <p>Download ResBaz 2024 Slides </p> <p> <p> </p> <p>Download Spring 2024 Slides </p> <p> <p>Download Fall 2023 Slides </p>"},{"location":"events/workshop_materials/intro_to_hpc/#interactive-materials","title":"Interactive Materials","text":""},{"location":"events/workshop_materials/intro_to_hpc/#logging-in","title":"Logging In","text":"<p>Instructions on Logging In</p>"},{"location":"events/workshop_materials/intro_to_hpc/#activity-lets-run-a-batch-job","title":"Activity: Let\u2019s run a batch job!","text":"<p>Let\u2019s write a basic Python script, then submit it to the queue </p> <p>Process:</p> <ol> <li>write a Python script</li> <li>write a batch script</li> <li>submit the batch script</li> </ol> <p>Step 1: Write a Python script</p> <p>You can write anything you want! It can be as simple as print(\u2018hello world\u2019), or you can make it more interesting.. up to you ;)</p> <ol> <li>If you haven't already, log onto the HPC</li> <li>create a blank python file: <code>touch myscript.py</code></li> <li> <p>edit the script: <code>nano myscript.py</code></p> <p>a. feel free to use vim or emacs or any other text editor if you are more comfortable with one of those</p> <p>b. add something simple like     <code>print(\u201chello world!\u201d)</code> 4. be sure to save it!</p> </li> </ol> <p>Step 2: Write your batch script</p> <p>Batch Script</p> <p>Part 1: Use directives to tell the scheduler what resources you want</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=test \n#SBATCH --output=test_%A.out \n#SBATCH --error=test_%A.err \n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --partition=standard \n#SBATCH --account=&lt;your_account&gt; \n#SBATCH --time=00:05:00\n</code></pre> <p>Part 2: Load your modules</p> <pre><code>module load python/3.11\n</code></pre> <p>Part 3: Run your script</p> <pre><code>python3 myscript.py\n</code></pre> <p>Step 3: Submit your job</p> <ol> <li> <p>Send to scheduler</p> <p><pre><code>sbatch myscript.slurm\n</code></pre> 2. Check the queue</p> <pre><code>squeue --user=&lt;my netid&gt;\n</code></pre> </li> <li> <p>View the output files</p> <p><pre><code>cat &lt;name&gt;.out\ncat &lt;name&gt;.err\n</code></pre> 4. View the job history report</p> <pre><code>job-history &lt;job id&gt;\n</code></pre> </li> </ol> <p>Congrats! You just ran your first batch job!</p>"},{"location":"events/workshop_materials/intro_to_hpc/#additional-resources","title":"Additional Resources","text":"<p>The HPC Consult Team is here for you!</p> <ul> <li>Documentation: docs.hpc.arizona.edu</li> <li>GitHub: ua-researchcomputing-hpc.github.io</li> <li>ServiceNow: HPC Support and Consulting Request</li> <li>Office Hours: every Wednesday 2-4pm on GatherTown</li> </ul>"},{"location":"events/workshop_materials/intro_to_linux/","title":"Index","text":""},{"location":"events/workshop_materials/intro_to_linux/#intro-to-linux-workshop-materials","title":"Intro to Linux Workshop Materials","text":""},{"location":"events/workshop_materials/intro_to_linux/#workshop-files","title":"Workshop Files","text":"<p> Click here to download PDF Version Click here to download PowerPoint Version <p></p>"},{"location":"events/workshop_materials/intro_to_linux/#video-presentation","title":"Video Presentation","text":""},{"location":"events/workshop_materials/intro_to_machine_learning/R/","title":"Index","text":""},{"location":"events/workshop_materials/intro_to_machine_learning/R/#intro-to-machine-learning-with-r-on-hpc","title":"Intro to Machine Learning with R on HPC","text":"<p>We will discuss a few hands-on examples of machine learning with R. You can run these in a RStudio notebook, or using a batch script (see Intro to HPC). Here we emphasize on using RStudio notebooks. We recommend that you try these hands-on examples.</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#why-use-r-for-machine-learning","title":"Why use R for machine learning?","text":"<ul> <li>R has been the quintessential language for Statistics for a while</li> <li>Comes batteries included -- tons of datasets and data visualization tools</li> <li>RStudio &amp; Shiny provide an excellent platform to develop and share your code</li> <li>Most HPC consultants are familiar with R</li> </ul>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#things-to-look-out-for","title":"Things to look out for","text":"<p>The popularity of R also means that there are a lot of moving pieces. Sometimes, that can complicate your workflow. For machine learning with R on HPC, keep the following in mind:</p> <ul> <li>Don't mix R versions</li> <li>Bazillion packages to do the same thing, but not all equally efficient (1)</li> <li>Installing packages on HPC clusters can sometimes be non-trivial (2)</li> <li>Using GPUs might need some extra configuration</li> <li>Not all R package managers play well with HPC environments</li> </ul> <ol> <li>Use packages from the <code>tidyverse</code> universe if possible. However to keep things simple we will not use <code>tidyverse</code> packages in this workshop, but we highly recommend that you look them up if you are using R for data-based research.</li> <li>Read the section on R in our docs.</li> </ol>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#setting-up-r-for-machine-learning-on-hpc","title":"Setting up R for machine learning on HPC","text":"<ul> <li>R and RStudio are already installed on HPC</li> <li>Read the section on R in our docs to troubleshoot package installation issues</li> </ul>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#access-rstudio-from-ood","title":"Access RStudio from OOD","text":"<ol> <li>Navigate to https://ood.hpc.arizona.edu/. After login, you will see the OOD dashboard. </li> <li>Select Interactive Apps, and then from the drop-down menu select RStudio Server. </li> <li>Fill in the details in the form that opens up (1), and select Launch. </li> <li>After the session becomes available, select Connect to RStudio Server (2). </li> </ol> <ol> <li>Use your PI's group's name for the PI Group field. You can find out your PI's group's name by running <code>va</code> in a terminal session.</li> <li> <ol> <li>After you select Launch in the previous step, OOD will take you to a page with a tile that shows your pending job. When it's first submitted, its status will show as Queued. Once it starts, it's status will change to Running and you'll see the Connect to RStudio Server link.</li> </ol> </li> </ol>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#examples","title":"Examples","text":""},{"location":"events/workshop_materials/intro_to_machine_learning/R/#incomplete-datasets","title":"Incomplete datasets","text":"<p>Click here to download R script</p> <p>Realistic datasets, like R's <code>airquality</code> dataset, often come with missing values. Not all machine learning models/algorithms are capable of dealing with datasets with missing values. Semi-supervised learning is a machine learning method that can deal with certain missing data. A more universal approach is to clean the dataset. Among other things this can involve:</p> <ul> <li>Removing observations with missing entries</li> <li>Filling the missing entries with token values</li> </ul> <p>Which method you choose will depend on the problem you are trying to solve. In this example we will look at some of the tools that R's provides to help you clean the data.</p> <p>The <code>airquality</code> dataset is part of the standard GNU R distribution. You can access it with the constant <code>airquality</code>. Instead of loading the whole dataset, we will just view the top 5 rows with the <code>head</code> function. (1)</p> <ol> <li>The <code>head</code> function is useful to quickly glimpse the first \\(N\\) rows of the dataset, without loading the whole dataset.</li> </ol> <pre><code>head(airquality, 5)\n</code></pre> <pre><code>  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n</code></pre> <p>The dataset has 6 columns. On the fifth row there are two <code>NA</code> entries. R uses <code>NA</code> (Not Available) to indicate missing values. Thus the <code>airquality</code> dataset is missing values. While in this case we easily found out that this dataset is incomplete, <code>head</code> in general is not a good way to detect if there are missing values. (1)</p> <ol> <li>Can you think why <code>head</code> is not a good missing value detector?</li> </ol> <p>R comes with other tools to detect missing values. One of the is <code>is.na</code>, which tells you if a quantity is <code>NA</code> or not. It returns <code>TRUE</code> if a quantity is <code>NA</code> else false.</p> <p><pre><code>is.na(c(\"a\", 5, NA))\n</code></pre> <pre><code>[1] FALSE FALSE  TRUE\n</code></pre></p> <p>You can combine <code>is.na</code> with the functions <code>which</code> and <code>sum</code> to see if, where, and how many <code>NA</code> quantities are there. (1)</p> <ol> <li>Find out what other tools R provides to detect missing values.</li> </ol> <p><pre><code>which(is.na(airquality)) # (1)!\n</code></pre></p> <ol> <li>Check what happens when you add the <code>arr.ind=TRUE</code> argument.</li> </ol> <pre><code> [1]   5  10  25  26  27  32  33  34  35  36  37  39  42  43  45  46  52  53  54  55\n[21]  56  57  58  59  60  61  65  72  75  83  84 102 103 107 115 119 150 158 159 164\n[41] 180 249 250 251\n</code></pre> <p>There are 44 missing values, and the numbers not in brackets indicate the rows where the missing values are (we have already seen that there are two missing values in row 5). If you do not care where the missing values are you can run <code>sum(is.na(airquality))</code> which will tell you the number of missing values. This can be useful to determine if you should drop the observations (rows) with the missing values. If dropping incomplete observations do not substantially decrease the size of your dataset then dropping them is a good option. Otherwise you might want to consider alternative solutions. </p> <pre><code>sum(is.na(airquality)) / nrow(airquality)\n</code></pre> <pre><code>[1] 0.2875817\n</code></pre> <p>Dropping the incomplete observations will reduce the <code>airquality</code> dataset by little less than 30%. This is actually an upper bound on the amount of data loss, because some rows have multiple missing entries. Whether a 30% decrease is significant or not will depend on the problem you are trying to solve and the data you have. Here we will see how we can drop incomplete observations. (1)</p> <ol> <li>Find out how to fill in the missing entries instead of dropping the incomplete observations.</li> </ol> <pre><code>airquality_no_na &lt;- na.omit(airquality)\nsum(is.na(airquality_no_na))\n</code></pre> <pre><code>[1] 0\n</code></pre> <p>Sometimes it is actually better to first visualize the dataset, including the distribution of the missing data. We will not go into that in this workshop. But we recommend that you check out packages like <code>visdat</code> and <code>VIM</code> which provide advanced functionality to visualize missing data.</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#clustering","title":"Clustering \ud83d\udc27","text":"<p>Click here to download R script</p> <p>In this example we will cluster penguins into groups based on their bill features. We will use the Palmer penguins dataset.</p> <p></p> <p>Artwork by @allison_horst</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#accessing-the-data","title":"Accessing the data","text":"<p>The R package <code>palmerpenguins</code> provides the data, along with some other goodies. You can just download the data file from the sources mentioned in the link above. But it's much easier to use the packaged version. Note that while the package is called <code>palmerpenguins</code> the dataset itself is named <code>penguins</code>. </p> <pre><code>install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n</code></pre>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#visualizing-the-data","title":"Visualizing the data","text":"<pre><code>summary(penguins)\n</code></pre> <pre><code>      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                   \n</code></pre> <p>The <code>summary</code> function is very handy to get an overall sense of a dataset. Often starting the exploration of your dataset with <code>summary</code> and <code>head</code> along with some visualizations is a good idea. (1)</p> <ol> <li>Look at the last row of each column in the output of <code>summary</code>. Do you see something useful?</li> </ol> <p>There are three unique species of penguins in the dataset. It might be tempting to think that there are three clusters in the dataset - one for each species. However, that need not be the case. Even if there are three clusters, they might not necessarily overlap with any species. Whether the clusters correspond to species will depend on the variance in intra-species differences and the variance in inter-species differences.</p> <p>In addition to \"species\" there are three other categorical variables - \"island\", \"sex\", and \"year\". The remaining four are continuous variables. Plotting the categorical variables against each other will not give much useful information. We will plot the continuous variables.</p> <pre><code>cat &lt;- c(\"species\", \"island\", \"sex\", \"year\")\nplot(penguins[, !(names(penguins) %in% cat)])\n</code></pre> <p></p> <p>The figure above shows that only when we plot <code>bill_length_mm</code> vs <code>bill_depth_mm</code>, or <code>bill_length_mm</code> vs <code>flipper_length_mm</code> we see more than two clusters. This does not mean that there are not more that two clusters. It just means that in two-dimensions these are the only two cases where we see hints of more two clusters.</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#k-means-clustering","title":"K-means clustering","text":"<p>Clustering is an unsupervised machine learning process, we do not provide any labels when we train the model. This is unlike the next examples, where we provided the labels. What this means is that a clustering model will not be able to tell if a particular cluster corresponds to a penguin species (or any other suitable label depending on the problem), but it will show the clusters that are there in the dataset. </p> <p>In K-means clustering the dataset is partitioned into K clusters, and each observation belongs to the cluster with the nearest mean (also known as centroid). The <code>stats</code> module of R comes with an implementation of K-means. See A K-means clustering algorithm for more information on the algorithm used in R's implementation of K-means clustering. K-means while one of the most popular clustering methods comes with the caveat that we have to choose K. The algorithm will then find those K clusters. There is no standard way to decide on a value of K. To keep things simple, here we will look at the <code>bill_length_mm</code> vs <code>bill_depth_mm</code> subset of the <code>penguins</code> dataset, which has at least two clusters.</p> <p><pre><code>df &lt;- data.frame(penguins$bill_length_mm, penguins$bill_depth_mm) # (1)!\ndf &lt;- na.omit(df)\ncl &lt;- kmeans(df, 3, nstart=10) # (2)!\nplot(df, col=cl$cluster)\npoints(cl$centers, col=1:3, pch=19, lwd=10)\n</code></pre></p> <ol> <li>An alternative way to choose columns.</li> <li>Why is <code>nstart</code> necessary? Try this with 2 clusters.</li> </ol> <p></p>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#classification","title":"\ud83c\udf44 classification","text":"<p>Click here to download R script</p> <p>In this example we are going to classify mushrooms as either edible or poisonous based on their physical features.</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#accessing-the-dataset","title":"Accessing the dataset","text":"<p>The original dataset is hosted on UC Irvine's Machine Learning Repository. A version of it is also available at Brett Lantz's Machine Learning with R datasets Github repository. We will download Brett's version for this example.</p> <pre><code>download.file(\n  \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/mushrooms.csv\",\n  destfile = \"Mushroom.csv\"\n)\nmushroom &lt;- read.csv(\"Mushroom.csv\", na.strings = \"?\")\n</code></pre>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#visualizing-the-dataset","title":"Visualizing the dataset","text":"<p>This is a much larger dataset than the <code>penguins</code> or the <code>airquality</code> datasets. Without some prettier printing the outputs of <code>summary</code> or <code>head</code> might be difficult to parse visually. We will look at some other R functions to help with getting a sense of the dataset.</p> <p><pre><code>c(nrow(mushroom), ncol(mushroom), sum(is.na(mushroom)))\n</code></pre> <pre><code>[1] 8124   23 2480\n</code></pre></p> <p>This dataset has 8124 observations and each observation has 23 features. There are 2480 missing entries. We also want to know what are these features and what type of data do they store. The more you know about the dataset the better. Often you will have to preprocess the data prior to train a machine learning model with it. Thus it useful to explore the dataset.</p> <p><pre><code>colnames(mushroom)\n</code></pre> <pre><code> [1] \"type\"                     \"cap_shape\"               \n [3] \"cap_surface\"              \"cap_color\"               \n [5] \"bruises\"                  \"odor\"                    \n [7] \"gill_attachment\"          \"gill_spacing\"            \n [9] \"gill_size\"                \"gill_color\"              \n[11] \"stalk_shape\"              \"stalk_root\"              \n[13] \"stalk_surface_above_ring\" \"stalk_surface_below_ring\"\n[15] \"stalk_color_above_ring\"   \"stalk_color_below_ring\"  \n[17] \"veil_type\"                \"veil_color\"              \n[19] \"ring_number\"              \"ring_type\"               \n[21] \"spore_print_color\"        \"population\"              \n[23] \"habitat\"             \n</code></pre> <pre><code>t(sapply(mushroom, summary)) # (1)!\n</code></pre></p> <ol> <li><code>sapply</code> applies a function over all the elements of a <code>List</code> or <code>Vector</code>, and <code>t</code> returns the transpose of a <code>Matrix</code>.</li> </ol> <pre><code>                         Length Class       Mode       \ntype                     \"8124\" \"character\" \"character\"\ncap_shape                \"8124\" \"character\" \"character\"\ncap_surface              \"8124\" \"character\" \"character\"\ncap_color                \"8124\" \"character\" \"character\"\nbruises                  \"8124\" \"character\" \"character\"\nodor                     \"8124\" \"character\" \"character\"\ngill_attachment          \"8124\" \"character\" \"character\"\ngill_spacing             \"8124\" \"character\" \"character\"\ngill_size                \"8124\" \"character\" \"character\"\ngill_color               \"8124\" \"character\" \"character\"\nstalk_shape              \"8124\" \"character\" \"character\"\nstalk_root               \"8124\" \"character\" \"character\"\nstalk_surface_above_ring \"8124\" \"character\" \"character\"\nstalk_surface_below_ring \"8124\" \"character\" \"character\"\nstalk_color_above_ring   \"8124\" \"character\" \"character\"\nstalk_color_below_ring   \"8124\" \"character\" \"character\"\nveil_type                \"8124\" \"character\" \"character\"\nveil_color               \"8124\" \"character\" \"character\"\nring_number              \"8124\" \"character\" \"character\"\nring_type                \"8124\" \"character\" \"character\"\nspore_print_color        \"8124\" \"character\" \"character\"\npopulation               \"8124\" \"character\" \"character\"\nhabitat                  \"8124\" \"character\" \"character\"\n</code></pre> <p>Interestingly, all the columns are categorical, and all the entries are character strings. Not all machine learning algorithms can work with character strings. You can use something like the <code>factor</code> function to convert those to numerical values. We will not have to do that in this example, but it is useful for you to know. The <code>type</code> column is our column of interest. We will take a more closer look at that.</p> <p><pre><code>unique(mushroom$type)\n</code></pre> <pre><code>[1] \"p\" \"e\"\n</code></pre> <pre><code>barplot(table(mushroom$type))\n</code></pre> </p> <p>There are only two types of mushrooms, <code>p</code> for poisonous, and <code>e</code> for edible. As the dataset is fairly balanced, we have comparable numbers of poisonous and edible mushrooms. (1) We will train a machine learning model to recognize a mushroom as either poisonous or edible based on its other features. Thus the type of the mushroom will be the label, and the rest of the columns the inputs for the model.</p> <ol> <li>Besides making a graph, how can you find out if a dataset is balanced?</li> </ol>"},{"location":"events/workshop_materials/intro_to_machine_learning/R/#naive-bayes-classifier","title":"Naive Bayes classifier","text":"<p>Naive Bayes classifiers are a class of probabilistic models that try to model the distribution of inputs of a given category. They apply Bayes' theorem with the assumption of conditional independence between every pair of inputs given the value of the category variable. Despite being a class of very simple models, they are impressively good at text classification. For more information on naive Bayes classifiers, see Idiot's Bayes: Not So Stupid after All?.</p> <p>To train a naive Bayes classifier, we will first randomly split the <code>mushroom</code> dataset into a train set containing 70% of the data, and a test set containing the remaining 30%. The <code>train_test_split</code> function implemented below is handy for this.</p> <pre><code>train_test_split &lt;- function(data, ratio) {\n  idx &lt;- sample(c(TRUE, FALSE), nrow(data), replace = TRUE, prob = c(ratio, 1 - ratio))\n  train &lt;- data[idx, ]\n  test &lt;- data[!idx, ]\n  return(list(\"train\"=train, \"test\"=test))\n}\n</code></pre> <pre><code>split &lt;- train_test_split(mushroom, 0.7)\nc(nrow(split$train), nrow(split$test))\n</code></pre> <pre><code>[1] 5629 2495\n</code></pre> <p>The <code>naivebayes</code> package provides a simple and efficient implementation of various naive Bayes classifiers.</p> <pre><code>install.packages(\"naivebayes\") # (1)!\nlibrary(naivebayes)\n</code></pre> <ol> <li>After installing the package, run <code>vignette(\"intro_naivebayes\")</code> to read a very nice introduction to this package, and to naive Bayes classifiers. You can also find the vignette on CRAN.</li> </ol> <p><pre><code>nb &lt;- naive_bayes(type ~ ., split$train) # (1)!\npred &lt;- predict(nb, split$test) # (2)!\ntab &lt;- table(pred, split$test$type) # (3)!\ntab\n</code></pre></p> <ol> <li><code>type ~ .</code> is a shorthand for indicating that the <code>type</code> column are the labels, and all the other columns are the inputs. </li> <li>When you run this, it will give a warning. Find out how you can modify <code>split$test</code> to not trigger the warning.</li> <li>Apply <code>summary</code> to <code>nb</code>, <code>pred</code>, <code>tab</code>, and see what you get.</li> </ol> <pre><code>pred    e    p\n   e 1278  130\n   p    8 1079\n</code></pre> <pre><code>sum(diag(tab)) / sum(tab)\n</code></pre> <pre><code>[1] 0.9446894\n</code></pre> <p>Just with this simple model we get around 94% accuracy. Try to improve it.</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/overview/","title":"Index","text":""},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#intro-to-machine-learning-on-hpc","title":"Intro to Machine Learning on HPC","text":"<p>This workshop introduces some of the basic concepts of machine learning, and shows how you can leverage UArizona HPC systems for machine learning. We recommend that you try the hands-on examples.</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#what-is-machine-learning","title":"What is machine learning?","text":"<p>\"The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.\"</p> <p>fast.ai book</p> <p>In a sense machine learning is very similar to doing science. <pre><code>flowchart TD\n    subgraph Science\n    A[Create a model to explain some observations] --&gt; B[Use the model to predict new observations]\n    end\n    subgraph Machine Learning\n    C[Train a program with data] --&gt; D[Use program to predict]\n    end</code></pre> In machine learning jargon, it is more common to use \"model\" or \"architecture\" in place of \"program\". Some practitioners differentiate between \"model\" and \"architecture\", depending on whether the program has been trained or not. For the purposes of this workshop, we will use \"model\".</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#machine-learning-workflow","title":"Machine learning workflow","text":""},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#training","title":"Training","text":"<pre><code>flowchart LR\n    A[Model\n    with random parameters] --&gt; B{Training}\n    C[Data] --&gt; B\n    B --&gt; D[Model\n    with trained parameters]</code></pre>"},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#inference","title":"Inference","text":"<pre><code>flowchart LR\n    E[Inputs] --&gt; F[Model\n    with trained parameters] --&gt; G[Predictions]</code></pre>"},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#what-can-we-do-with-machine-learning","title":"What can we do with machine learning?","text":"<ul> <li>Play chess, go, ..</li> <li>Classify data: \ud83d\udc36 vs. \ud83d\udc31, galaxies, \ud83d\udc26 species from their calls, ...</li> <li>Recommender systems: \ud83c\udfa5 / \ud83d\udcda / \ud83d\udcdc suggestions, ...</li> <li>Solve \\(\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu\\frac{\\partial^2 u}{\\partial x^2}\\)</li> <li>Cluster data into different groups</li> <li>Write poetry, create art</li> </ul>"},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#things-to-keep-in-mind","title":"Things to keep in mind","text":"<ul> <li>Computers aren't smart, but can be (over)confident</li> <li>While we do not manually code the individual steps in the program, we still do a lot<ul> <li>We decide which model, data, training hyperparameters, ...</li> </ul> </li> <li>Data is central to machine learning<ul> <li>Untrained models are generally no better than random chance</li> <li>Trained models often learn biases in data</li> </ul> </li> <li>Lot of machine learning has traditionally been known as Statistics</li> </ul>"},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#why-use-uarizona-hpc-for-machine-learning","title":"Why use (UArizona) HPC for machine learning?","text":"<ul> <li>Datasets are becoming larger and more varied</li> <li>Models are becoming larger</li> <li>You don't have to worry about setting up and maintaining hardware</li> </ul>"},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#resources-for-learning-machine-learning","title":"Resources for learning machine learning","text":"<ul> <li>Practical Deep Learning by fast.ai (Python)</li> <li>A Code-First Introduction to Natural Language Processing by fast.ai (Python)</li> <li>An Introduction to Statistical Learning  by James, Witten, Hastie, Tibishirani (Python and R)</li> <li>Tidy Modeling With R by Kuhn, Silge</li> <li>Deep Learning and Scientific Computing with R torch by Keydana</li> <li>The Elements of Statistical Learning by Hastie, Tibishirani, Friedman (language agnostic)</li> <li>Pattern Recognition and Machine Learning by Bishop (language agnostic)</li> </ul>"},{"location":"events/workshop_materials/intro_to_machine_learning/overview/#examples","title":"Examples","text":"<ul> <li>Go to Python version</li> <li>Go to R version</li> </ul>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/","title":"Index","text":""},{"location":"events/workshop_materials/intro_to_machine_learning/python/#intro-to-machine-learning-with-python-on-hpc","title":"Intro to Machine Learning with Python on HPC","text":"<p>We will discuss a few hands-on examples of machine learning with Python. You can run these in a Jupyter notebook, or using a batch script (see Intro to HPC). Here we emphasize on using Jupyter notebooks. We recommend that you try these hands-on examples.</p> <p>See a presentation of this material from our Summer 2024 workshop series here:</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#why-use-python-for-machine-learning","title":"Why use Python for machine learning?","text":"<ul> <li>Python is probably the most popular language for machine learning</li> <li>Python has a large ecosystem of packages for everything machine learning</li> <li>All popular machine learning platforms provide Python APIs</li> <li>All HPC consultants are very familiar with Python</li> </ul>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#things-to-look-out-for","title":"Things to look out for","text":"<p>The popularity of Python also means that there are a lot of moving pieces. Sometimes, that can complicate your workflow. For machine learning with Python on HPC, keep the following in mind:</p> <ul> <li>Don't mix Python versions</li> <li>Using GPUs might need some extra configuration</li> <li>Not all Python package managers play well with HPC environments</li> </ul>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#setting-up-python-for-machine-learning-on-hpc","title":"Setting up Python for machine learning on HPC","text":"<p>Use virtual environments to install Python packages. See Python to learn how to create virtual environments with built-in Python tools. See Anaconda to learn how to create virtual environments with Conda (or its variants).</p> <p>For the hands-on examples we will need the following packages (with GPU support):</p> <ul> <li><code>torch</code> - base PyTorch, think of this as an accelerated version of NumPy</li> <li><code>torchvision</code> - adds support for machine learning with images to PyTorch</li> <li><code>fastai</code> - layer on top of PyTorch that makes training easier</li> </ul> <p>We will install these in a Python (version 3.8) virtual environment, and then create a Jupyter kernel so that we can access them from a Jupyter notebook. Installing <code>fastai</code> will install various other packages, like <code>pandas</code> and <code>matplotlib</code>, which we will also use. But we do not need to install them separately.</p> <p>To create a Python virtual environment you need to be in an interactive session. Once in an interactive session run the following commands (1)</p> <ol> <li> <p>Choose suitable values for </p> <ul> <li><code>&lt;path-to-venv&gt;</code>: location where you want to install your Python virtual environment</li> <li><code>&lt;kernel-name&gt;</code>: name of the Jupyter kernel, e.g. <code>ml-workshop</code></li> <li><code>&lt;optional-display-name&gt;</code>: optional prettier name for the Jupyter kernel shown in the Jupyter dashboard, e.g. <code>Python (ML Workshop)</code></li> </ul> </li> </ol> <pre><code>module load python/3.8\nmodule load cuda11 cuda11-dnn cuda11-sdk # (1)!\npython3 -m venv --system-site-packages &lt;path-to-venv&gt;\nsource &lt;path-to-venv&gt;/bin/activate\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\npip3 install fastai\npip3 install jupyter --force-reinstall\nipython kernel install --name &lt;kernel-name&gt; --user --display-name &lt;optional-display-name&gt;\n</code></pre> <ol> <li>Load these modules for GPU support.</li> </ol>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#jupyter-notebooks-on-uarizona-hpc","title":"Jupyter Notebooks on UArizona HPC","text":"<p>UArizona HPC provides access to Jupyter notebooks via Open OnDemand (OOD). The following steps show how you can launch a Jupyter notebook from UArizona HPC's OOD portal:</p> <ol> <li>Navigate to https://ood.hpc.arizona.edu/. After login, you will see the OOD dashboard. </li> <li>Select Interactive Apps, and then from the drop-down menu select Jupyter Notebook. </li> <li>Fill in the details in the form that opens up (1), and select Launch. </li> <li>After the session becomes available, select Connect to Jupyter (2). </li> <li>Once you are in the Jupyter dashboard, create a notebook with the kernel you had installed. </li> </ol> <ol> <li>Use your PI's group's name for the PI Group field. You can find out your PI's group's name by running <code>va</code> in a terminal session.</li> <li>After you select Launch in the previous step, OOD will take you to a page with a tile that shows your pending job. When it's first submitted, its status will show as Queued. Once it starts, it's status will change to Running and you'll see the Connect to Jupyter link. </li> </ol>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#examples","title":"Examples","text":""},{"location":"events/workshop_materials/intro_to_machine_learning/python/#classification","title":"\ud83c\udf44 Classification","text":"<p>Click here to download Jupyter notebook</p> <p>In this example we will identify fungi species by their images. We will use a subset of the Danish Fungi 2020 dataset.</p> <p></p>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#accessing-the-data","title":"Accessing the data","text":"<p>The local dataset is at <code>/contrib/datasets/workshops/DF20M-4.tar.gz</code>. You can access from an interactive session. It consists of the four species with the most images in the DF20-Mini dataset. Copy it to your working directory (1), and untar it.</p> <ol> <li>under your home directory, or your PI's <code>/groups</code> or <code>/xdisk</code> share</li> </ol> <p>From an interactive session, run the following: <pre><code>cd &lt;working-dir&gt;\ncp /contrib/datasets/workshops/DF20M-4.tar.gz ./\ntar xvf DF20M-4.tar.gz\n</code></pre></p>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#visualize-the-data","title":"Visualize the data","text":"<p>A sample of the local dataset will look different from the image above because it only contains a subset of the species. Visualizing the data will help us to decide which model to use, and how to train it. </p> <pre><code>from pathlib import path\nfrom itertools import islice\n\ndef _ls(p, n=None): return list(islice(p.iterdir(), n)) # (1)!\n\np = Path(\"/xdisk/sohampal/sohampal/fungi/DF20M-4/\") # (2)!\n_ls(p)\n</code></pre> <ol> <li>Helper function to list the contents of a directory. Why do we need <code>n</code>?</li> <li>Replace this with the path to where you downloaded the data.</li> </ol> <pre><code>[PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/train'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/DF20M-4.tar'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/test')]\n</code></pre> <p>There are two directories in the dataset, <code>train</code> and <code>test</code>: </p> <ul> <li><code>train</code> contains images for training your model</li> <li><code>test</code> contains images for testing how well your model has learned</li> </ul> <p>We can use <code>_ls()</code> to probe the contents of <code>train</code>, <code>test</code>, and their subdirectories.</p> <pre><code>_ls(p / \"train\")\n</code></pre> <pre><code>[PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/train/Mycena galericulata'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/train/Boletus edulis'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/train/Amanita muscaria'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/train/Clitocybe nebularis')]\n</code></pre> <pre><code>_ls(p / \"test\")\n</code></pre> <pre><code>[PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/test/Mycena galericulata'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/test/Boletus edulis'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/test/Amanita muscaria'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/test/Clitocybe nebularis')]\n</code></pre> <pre><code>_ls(p / \"test\" / \"Mycena galericulata\", 4) # (1)!\n</code></pre> <ol> <li>Change the number to something else, and see what you get.</li> </ol> <pre><code>[PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/test/Mycena galericulata/2238582384-112591.JPG'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/test/Mycena galericulata/2427873332-344961.JPG'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/test/Mycena galericulata/2238559251-255760.JPG'),\n PosixPath('/xdisk/sohampal/sohampal/fungi/DF20M-4/test/Mycena galericulata/2238471930-167014.JPG')]\n</code></pre> <p>The subdirectories of <code>train</code> and <code>test</code> tell us the name of the species. For example, all the training images corresponding to  Boletus edulis are grouped under the subdirectory <code>Boletus edulis</code>.</p> <pre><code>{d.name: len(_ls(d)) for d in _ls(p / \"train\")} \n</code></pre> <pre><code>{'Mycena galericulata': 1099,\n 'Boletus edulis': 811,\n 'Amanita muscaria': 863,\n 'Clitocybe nebularis': 1003}\n</code></pre> <p>The dataset is fairly balanced:</p> <ul> <li>Mycena galericulata and Clitocybe nebularis have similar number of training images</li> <li>Boletus edulis and Amanita muscaria have similar number of training images (around 80% of the other two)</li> </ul> <p>It is usually much easier to train models with balanced datasets. For unbalanced datasets you might have to adopt smarter training processes to counter the bias in the datasets.</p> <pre><code>from PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_images(p, ncols=8):\n    fig, axes = plt.subplots(1, ncols, figsize=(20, 2))\n    imgs = p.iterdir()\n    for i, ax in enumerate(axes):\n        img = next(imgs)\n        ax.imshow(Image.open(img))\n        ax.axes.get_xaxis().set_ticks([])\n        ax.axes.get_yaxis().set_ticks([])\n    fig.suptitle(p.name)\n\nfor d in _ls((p / \"train\")): show_images(d)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>The images are of different shapes, this will be important when you train a model. </p>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#simpler-problem-binary-classification","title":"Simpler problem: binary classification","text":"<p>In the image samples above, we see that Boletus edulis and Amanita muscaria look drastically different from each other (they also have similar number of images). As a simpler problem, we will train a model to classify these two fungi species.</p> <p>We will create a subdirectory called <code>binary</code> and copy the <code>train</code> and <code>test</code> images for the two species to it. This is not strictly necessary, but it will be easier to use the <code>fastai</code> dataloaders in this way.</p> <p>You can copy the images using either shell tools from a terminal, or by running the following Python code from Jupyter.</p> <pre><code>import shutil\n\ndef copy_images(partition):\n    for fungi in (\"Boletus edulis\", \"Amanita muscaria\"):\n        (p / \"binary\" / partition / fungi).mkdir(parents=True, exist_ok=True)\n        for img in (p / partition / fungi).iterdir():\n            shutil.copy2(img, p / \"binary\" / partition / fungi)\n\ncopy_images(\"train\")\ncopy_images(\"test\")\n</code></pre> <p>Check that all the data has been copied correctly.</p> <pre><code>{d.name: len(_ls(d)) for d in _ls(p / \"binary\" / \"train\")}\n</code></pre> <pre><code>{'Boletus edulis': 811, 'Amanita muscaria': 863}\n</code></pre> <pre><code>{d.name: len(_ls(d)) for d in _ls(p / \"binary\" / \"test\")}\n</code></pre> <p><pre><code>{'Boletus edulis': 90, 'Amanita muscaria': 96}\n</code></pre> The <code>fastai</code> package provides a <code>DataBlock</code> class to easily load the data to the model.</p> <pre><code>from fastai.data.all import *\nfrom fastai.vision.all import *\n\ndblock = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                   get_items = get_image_files,\n                   get_y = parent_label,\n                   splitter = RandomSplitter(),\n                   item_tfms = Resize(224))\n</code></pre> <p>It is useful to understand the different arguments used when instantiating the <code>DataBlock</code> class:</p> <ul> <li><code>blocks = (ImageBlock, CategoryBlock)</code>: Every instance of data consists of an image and a corresponding category (species name)</li> <li><code>get_items = get_image_files</code>: Uses the <code>get_image_files</code> function from <code>fastai</code> to load the image files</li> <li><code>get_y = parent_label</code>: Uses the <code>parent_label</code> function from <code>fastai</code> to get the corresponding categories (in this case the name of the parent directory)</li> <li><code>splitter = RandomSplitter()</code>: Randomly splits the data in to a train set and a validation set</li> <li><code>item_tfms = Resize(224)</code>: Resizes all images to a size of 224 pixels x 224 pixels</li> </ul> <p><code>dblock</code>  does not actually know where the images are. We can create a <code>Datasets</code> object from <code>dblock</code> which knows where the images are. Think of it as applying <code>dblock</code> to each of the images under <code>binary/train</code>.</p> <pre><code>dsets = dblock.datasets(p / \"binary\" / \"train\")\n</code></pre> <p>By default, <code>RandomSplitter</code> does a 80:20 split, i.e. 80% of the data under is used for training, and 20% for validation.</p> <pre><code>len(dsets.train), len(dsets.valid)\n</code></pre> <pre><code>(1340, 334)\n</code></pre> <p>After training the model, we can test it with the images under <code>binary/test</code>. The model has never seen the images under <code>binary/test</code> during the training process, and thus this is a good way to test the generalization of the model. Sometimes, when we don't have a lot of data, we only split the data into a train set and a validation set.</p> <p>It is useful to see how <code>Datasets</code> internally represents the data.</p> <pre><code>dsets.train[0], dsets.valid[0]\n</code></pre> <pre><code>((PILImage mode=RGB size=1126x1000, TensorCategory(0)),\n (PILImage mode=RGB size=3024x4032, TensorCategory(1)))\n</code></pre> <p>It records the type of input, colored images, along with their original sizes. But more importantly, it automatically transforms the categories Boletus edulis and Amanita muscaria to 1 and 0, respectively. (1)</p> <ol> <li>The <code>vocab</code> attribute of the <code>Datasets</code> object will tell you which category is mapped to which number. Try running <code>dsets.vocab</code>, the list index of a species is the number it has been mapped to.</li> </ol> <p>We can use <code>dsets</code> to load the data to a model. However, it is best to load the images to a model in small batches (mini-batch). We will create a <code>DataLoaders</code> object which can load the images in small batches (default size is 64). (1)</p> <ol> <li>You do not need to create a <code>Datasets</code> object to create a <code>DataLoaders</code> object. <code>dsets</code> was just for demonstrating some of the internals of <code>fastai</code>. Think of <code>DataLoaders</code> as <code>Datasets</code> but with a batch size.</li> </ol> <pre><code>dls = dblock.dataloaders(p / \"binary\" / \"train\")\ndls.show_batch(max=9)\n</code></pre> <p></p> <p>Now it is the time to train a model. You can build and train a deep learning model from scratch. An alternative is to fine-tune a pre-trained model. Fine-tuning is essentially taking advantage of past training, usually training done by the community, or some corporate. Instead of starting with a model with random parameters, you now start with a model that knows something. Generally fine-tuning a pre-trained model takes less time and data than training a model from scratch.</p> <p>For an image classification you should a choose a model that has been trained on some large image dataset. One such model is Resnet-34 from Deep Residual Learning for Image Recognition, trained on the ImageNet dataset. See Which image models are best?  for a catalog of such models. <code>fastai</code> provides a <code>vision_learner</code> object for easy training or fine-tuning image models.</p> <pre><code>learn = vision_learner(dls, resnet34, metrics=error_rate)\n</code></pre> <p>The <code>lr_find</code> method helps to choose a suitable learning rate. </p> <pre><code>learn.lr_find()\n</code></pre> <p></p> <p>Any learning around, preferably larger, the valley (orange point) will be suitable for fine-tuning Resnet-34 to the binary dataset. Fine-tune the model for 3 epochs. (1)</p> <ol> <li>A good exercise to change the number of epochs, and see how that affects the model's learning. </li> </ol> <pre><code>learn.fine_tune(3, 2e-3)\n</code></pre> <pre><code>epoch   train_loss  valid_loss  error_rate  time\n    0   0.605542    0.055386    0.017964    01:21\n\nepoch   train_loss  valid_loss  error_rate  time\n    0   0.139088    0.088573    0.032934    01:21\n    1   0.088828    0.051099    0.020958    01:21\n    2   0.061853    0.035076    0.008982    01:22\n</code></pre> <p>We can use the <code>show_results</code> method of the <code>vision_learner</code> class to get a more visual sense of the quality of the learning. A better method is to use an <code>Interpretation</code> object. With an <code>Interpretation</code> object we can plot a confusion matrix, or print a overall performance report.</p> <pre><code>interp = ClassificationInterpretation.from_learner(learn) \ninterp.plot_top_losses(9, figsize=(15, 11))\n</code></pre> <p></p> <pre><code>interp.plot_confusion_matrix()\n</code></pre> <p></p> <p><pre><code>interp.print_classification_report()\n</code></pre> <pre><code>                  precision    recall  f1-score   support\n\nAmanita muscaria       1.00      0.98      0.99        96\n  Boletus edulis       0.98      1.00      0.99        90\n\n        accuracy                           0.99       186\n       macro avg       0.99      0.99      0.99       186\n    weighted avg       0.99      0.99      0.99       186\n</code></pre></p> <p>The confusion matrix and the classification report show how well the trained model performs on the validation set. Both the confusion matrix and the classification report give us the same information, we can explicitly calculate the metrics in the classification report from the confusion matrix. Choose whichever one that conveys the information better.</p> <p>We can also do the same with the test set (<code>binary/test</code>), which the model didn't see during training.</p> <pre><code>test_files = get_image_files(p / \"binary\" / \"test\") \ntest_dl = dls.test_dl(test_files, with_labels=True) # (1)!\n</code></pre> <ol> <li>This creates a test dataloader using the same transformations used to create the dataloader for the validation set.</li> </ol> <pre><code>preds = learn.get_preds(dl=test_dl)\nfor index, item in enumerate(preds[0]): \n    prediction = dls.categorize.decode(torch.argmax(item)) \n    confidence = max(item) \n    percent = float(confidence) \n    print(f\"Prediction: {prediction:18} - Confidence: {percent:.2%} - Image: {test_dl.items[index].name}\")\n</code></pre> <pre><code>...\nPrediction: Boletus edulis     - Confidence: 100.00% - Image: 2238481466-242195.JPG\nPrediction: Boletus edulis     - Confidence: 100.00% - Image: 2864902315-212162.JPG\nPrediction: Boletus edulis     - Confidence: 98.13% - Image: 2860312323-62558.JPG\nPrediction: Boletus edulis     - Confidence: 99.98% - Image: 2413150418-44022.JPG\nPrediction: Boletus edulis     - Confidence: 98.77% - Image: 2860298436-285200.JPG\nPrediction: Boletus edulis     - Confidence: 96.30% - Image: 2856920321-284750.JPG\nPrediction: Amanita muscaria   - Confidence: 100.00% - Image: 2238561790-182394.JPG\nPrediction: Amanita muscaria   - Confidence: 74.62% - Image: 2874318440-215186.JPG\nPrediction: Amanita muscaria   - Confidence: 99.99% - Image: 2898623358-68526.JPG\nPrediction: Amanita muscaria   - Confidence: 99.81% - Image: 2864909415-212510.JPG\nPrediction: Amanita muscaria   - Confidence: 100.00% - Image: 2430667070-196992.JPG\nPrediction: Amanita muscaria   - Confidence: 100.00% - Image: 2238525200-175134.JPG\n...\n</code></pre> <pre><code>interp = ClassificationInterpretation.from_learner(learn, dl=test_dl)\ninterp.plot_confusion_matrix()\n</code></pre> <p></p>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#multi-class-classification","title":"Multi-class classification","text":"<p>The code for the multi-class classification can be exactly the same as for the binary classification. Just for experimentation and demonstration, we will try something little different. Instead of a train/validation/test split, try a train/validation split - all images under <code>train</code> used for training, and all images under <code>test</code> used for validation, with no separate test set. The <code>GrandparentSplitter</code> from <code>fastai</code> is useful for this arrangement.</p> <pre><code>dblock = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                   get_items = get_image_files,\n                   get_y = parent_label,\n                   splitter = GrandparentSplitter(train_name=\"train\", valid_name=\"test\"),\n                   item_tfms = Resize(224))\n\ndls = dblock.dataloaders(p)\ndls.show_batch()\n</code></pre> <p></p> <pre><code>learn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.lr_find()\n</code></pre> <p></p> <p><pre><code>learn.fine_tune(5, 2e-3)\n</code></pre> <pre><code>epoch   train_loss  valid_loss  error_rate  time\n    0   0.660746    0.330275    0.102310    04:38\n\nepoch   train_loss  valid_loss  error_rate  time\n    0   0.239338    0.225458    0.077558    04:31\n    1   0.141983    0.287751    0.080858    04:33\n    2   0.072769    0.190867    0.049505    04:30\n    3   0.029050    0.197074    0.049505    04:28\n    4   0.015718    0.200661    0.049505    04:39\n</code></pre></p> <p><pre><code>interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n</code></pre> </p>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#clustering","title":"Clustering \ud83d\udc27","text":"<p>Click here to download Jupyter notebook</p> <p>In this example we will cluster penguins into groups based on their bill features. We will use the Palmer penguins dataset.</p> <p></p> <p>Artwork by @allison_horst</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#accessing-the-data_1","title":"Accessing the data","text":"<p>There is a Python package <code>palmerpenguins</code> that provides the data, along with some other goodies. However for the purposes of this example you can simply download the penguins.csv from the Github repo of the package.</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#visualizing-the-data","title":"Visualizing the data","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import colormaps\n\npenguins = pd.read_csv(\"penguins.csv\") # (1)!\npenguins.head()\n</code></pre> <ol> <li>Replace this with the full path to the <code>penguins.csv</code> file if the file and the notebook are not in the same directory.</li> </ol> <pre><code>    species        island   bill_length_mm  bill_depth_mm   flipper_length_mm   body_mass_g     sex     year\n0    Adelie     Torgersen             39.1           18.7               181.0        3750.0    male     2007\n1    Adelie     Torgersen             39.5           17.4               186.0        3800.0  female     2007\n2    Adelie     Torgersen             40.3           18.0               195.0        3250.0  female     2007\n3    Adelie     Torgersen              NaN            NaN                 NaN           NaN     NaN     2007\n4    Adelie     Torgersen             36.7           19.3               193.0        3450.0  female     2007\n</code></pre> <p>The dataset list the species of each penguin, which island in Palmer archipelago and which year they were observed in, and some physical features. Complete data for all penguins are not available, missing values are indicated by <code>NaN</code>.</p> <p><pre><code>penguins[\"species\"].unique()\n</code></pre> <pre><code>array(['Adelie', 'Gentoo', 'Chinstrap'], dtype=object)\n</code></pre></p> <p>There are three unique species of penguins in the dataset. It might be tempting to think that there are three clusters in the dataset - one for each species. However, that need not be the case. Even if there are three clusters, they might not necessarily overlap with any species. Whether the clusters correspond to species will depend on the variance in intra-species differences and the variance in inter-species differences.</p> <pre><code>pd.plotting.scatter_matrix(penguins, columns=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"], figsize=(8, 8))\n</code></pre> <p></p> <p>The figure above shows that only when we plot <code>bill_length_mm</code> vs <code>bill_depth_mm</code>, or <code>bill_length_mm</code> vs <code>flipper_length_mm</code> we see more than two clusters. This does not mean that there are not more that two clusters. It just means that in two-dimensions these are the only two cases where we see hints of more two clusters.</p>"},{"location":"events/workshop_materials/intro_to_machine_learning/python/#mean-shift-clustering","title":"Mean shift clustering","text":"<p>Clustering is an unsupervised machine learning process, we do not provide any labels when we train the model. This is unlike the previous case of image classification, where we provided the labels (the species that image belonged to). What this means is that a clustering model will not be able to tell if a particular cluster corresponds to a penguin species (or any other suitable label depending on the problem), but it will show the clusters that are there in the dataset.</p> <p>In mean shift clustering, candidates for centroids are updated to be the mean of the points within a given region. Each observation belongs to the cluster with the nearest mean. See Mean shift: a robust approach toward feature space analysis for more information on mean shift clustering.</p> <p>Here is a naive implementation of mean shift clustering. Mean shift clustering depends on a kernel function, which takes in one hyperparameter called bandwidth. In this implemetation, we use the Gaussian kernel: </p> \\[ g(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right). \\] <p>For the Gaussian kernel, the bandwidth is the same as the standard deviation.</p> <pre><code>import torch\n\ndef gaussian(x, mu, sigma):\n    return torch.exp(-0.5 * ((x - mu) / sigma) ** 2) / (\n        sigma * torch.sqrt(torch.tensor(2 * torch.pi))\n    )\n\ndef meanshift(data, bw, bs=None, epochs=10):\n    X = data.clone()\n    n = len(data)\n    if bs is None or bs &gt; n:\n        bs = n\n    for _ in range(epochs):\n        for i in range(0, n, bs):\n            dist = torch.cdist(X[i : min(i + bs, n)], X)\n            weights = gaussian(dist, 0, bw)\n            X[i : min(i + bs, n)] = (weights @ X) / weights.sum(1, keepdim=True)\n    return X\n</code></pre> <p>In this workshop we will look at a two-dimensional case, as it is easier to visualize. We will pick two physical features of the penguins, and see if we can identify the clusters in that two-dimensional data. (1)</p> <ol> <li>A good exercise will be to try a multidimensional case, by considering more than two physical features of the penguins.</li> </ol> <pre><code>data = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ndata = data.dropna() # (1)\n</code></pre> <ol> <li>We need to remove the missing values.</li> </ol> <p>Since the mean shift implementation above is with PyTorch, we have to convert the data to PyTorch tensors. This also gives us the opportunity to use GPUs, if available.</p> <pre><code>data = torch.tensor(data.values)\nif torch.cuda.is_available(): data = data.cuda()\n</code></pre> <pre><code>centroids = meanshift(data, 1.5, 10).cpu() # (1)!\n</code></pre> <ol> <li>Try a different value, than 1.5, for the bandwidth, and see how that affects the results.</li> </ol> <pre><code>fig, ax = plt.subplots()\nax.scatter(data = penguins, x = \"bill_length_mm\", y = \"bill_depth_mm\")\nax.set_xlabel(\"bill_length_mm\")\nax.set_ylabel(\"bill_depth_mm\")\nax.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", color=\"red\")\n</code></pre> <p></p> <p>The algorithm gives the centroids for three clusters, but not necessarily in the positions that you would expect them. A good exercise will be to check the scikit-learn implementation of mean shift clustering, and see if the naive implementation above can be improved.</p>"},{"location":"events/workshop_materials/intro_to_parallel_computing/","title":"Index","text":""},{"location":"events/workshop_materials/intro_to_parallel_computing/#introduction-to-parallel-computing","title":"Introduction to Parallel Computing","text":"<p> Click here to download PDF </p>"},{"location":"events/workshop_materials/intro_to_parallel_computing/#video-presentation","title":"Video Presentation","text":""},{"location":"events/workshop_materials/intro_to_statistics/","title":"Index","text":""},{"location":"events/workshop_materials/intro_to_statistics/#intro-to-statistics-on-hpc-using-matlab","title":"Intro to Statistics on HPC Using Matlab","text":""},{"location":"events/workshop_materials/intro_to_statistics/#workshop-files","title":"Workshop Files","text":"<p> Click here to download PDF Version Click here to download PowerPoint Version Click here to download examples <p></p>"},{"location":"events/workshop_materials/intro_to_visualization/","title":"Index","text":""},{"location":"events/workshop_materials/intro_to_visualization/#introduction-to-visualization-on-hpc","title":"Introduction to Visualization on HPC","text":"<p>This workshop provides an introduction to some concepts of visualization. It is done in the context of HPC so you will be able to follow along with the practical examples section using your HPC Account.</p> <p> Click here to download PDF Click here to download Jupyter notebook </p>"},{"location":"events/workshop_materials/optimization/","title":"Index","text":""},{"location":"events/workshop_materials/optimization/#managing-and-optimizing-jobs-on-hpc","title":"Managing and Optimizing Jobs on HPC","text":"<p> <p>Download Slides </p>"},{"location":"policies/acceptable_use/","title":"Acceptable Use","text":"<p>High Performance Computing (HPC) facility users are responsible for complying with all University policies.</p> <p>The supercomputers represent a unique resource for the campus community. These computers have special characteristics that are not found, or are of limited availability, on other central computers, including parallel processing, large memory, and a Linux operating system. The UArizona HPC resources require close supervision by those charged with management of these resources.  </p>"},{"location":"policies/acceptable_use/#controlled-data","title":"Controlled Data","text":"<p>UArizona HPC does not provide support for any type of controlled data. No controlled data (HIPAA, EAR, FERPA, PII, CUI, ITAR, etc.) can be analyzed or stored on any HPC storage.</p> <p>For HIPAA data we maintain a separate cluster called Soteria. See Secure Services.</p>"},{"location":"policies/acceptable_use/#federal-regulations","title":"Federal Regulations","text":"<p>Equipment purchased exclusively for research purposes is exempt from Arizona State Sales Tax. See UA FSO statement of Research Equipment Tax Exemption. However, there are exceptions to this exemption that impact central, shared use research facilities. These exceptions include \"research in social sciences or psychology\"\u2014see AZ ARS 42-5061 subsection B.15.</p> <p>Machinery or equipment used in research and development. For the purposes of this paragraph, \"research and development\" means basic and applied research in the sciences and engineering, and designing, developing or testing prototypes, processes or new products, including research and development of computer software that is embedded in or an integral part of the prototype or new product or that is required for machinery or equipment otherwise exempt under this section to function effectively. Research and development do not include manufacturing quality control, routine consumer product testing, market research, sales promotion, sales service, research in social sciences or psychology, computer software research that is not included in the definition of research and development, or other nontechnological activities or technical services. (emphasis added)</p> <p>In order to provide research computing resources to researchers from these exception areas, some of the research computing resources have been purchased with Arizona taxes included. The result is that there are resources available to all campus researchers, with the caveat that researchers in the social sciences, psychology and instructional projects areas are restricted to using resources that are purchased with taxes paid.</p> <p>Please contact our HPC consultants to learn about the resources that are available for social sciences, psychology and instructional purposes.</p>"},{"location":"policies/acceptable_use/#access-for-research-and-limited-access-for-instruction","title":"Access for Research and Limited Access for Instruction","text":"<p>As described in the 'Sales Tax Exemption' section above, most of the HPC systems are limited to research applications as defined in Section B-14 of ARS Statute 42-5061 by the Arizona Legislature. All users are expected to use these resources accordingly and to use other computing systems for non-research purposes.</p>"},{"location":"policies/access/","title":"Access","text":""},{"location":"policies/access/#eligibility","title":"Eligibility","text":"<p>The University of Arizona offers high performance computing resources to all active faculty, staff, students, and affiliates free of cost. </p> <p>Users are required to have active university credentials to gain access to all HPC services. If you are collaborating with university members but are not actively affiliated with the university, you may apply for Designated Campus Colleague (DCC) status through HR. DCC status grants non-affiliated individuals with active university credentials required for accessing university services.</p>"},{"location":"policies/access/#email-list-registration","title":"Email List Registration","text":"<p>All HPC account owners and sponsors are required to be members of the HPC-Announce email list and will automatically be added as members during the account creation process. This email list is used to send HPC system-related updates and notifications. Deleting your HPC account will automatically remove you from the listserv. </p>"},{"location":"policies/access/#registration-process","title":"Registration Process","text":"<ul> <li> <p>Principal Investigators (PIs)</p> <p>PIs (typically faculty members) can sponsor themselves for access by following the instructions on our Account Creation page. Once a PI has successfully set up their group, they may sponsor other university members for access.</p> </li> <li> <p>Non-PIs</p> <p>Non-PI university members may request sponsorship from PIs with active HPC accounts to gain access by following the instructions on our Account Creation page.</p> </li> </ul>"},{"location":"policies/access/#loss-of-university-affiliation","title":"Loss of University Affiliation","text":"<p>Users who lose affiliation with the university (e.g., graduating or leaving their position) will lose access to HPC. They may apply for DCC status to reinstate their credentials if HPC access is needed. For more information, see our policy page on losing university affiliation.</p>"},{"location":"policies/acknowledgements/","title":"Acknowledgements","text":"<p>PIs should notify our HPC consultants about posters or other publications (published, accepted, submitted, or in preparation) that benefited from the use of UArizona High Performance Computing, Statistical Consulting, and/or Data &amp; Visualization Consulting. These will be listed in the Collection of Published Results.</p> <p>Results</p>  Acknowledging the UArizona HPC Resources  <p>The suggested format to acknowledge University of Arizona High-Performance Computing in a paper, poster, or presentation is:</p> <p>This material is based upon High Performance Computing (HPC) resources supported by the University of Arizona TRIF, UITS, and Research, Innovation, and Impact (RII) and maintained by the UArizona Research Technologies department.</p>  Acknowledging Contributions from a UArizona Research Technologies Staff Member or Consultant  <p>If you wish to additionally acknowledge an individual who assisted you from University of Arizona High-Performance Computing, the suggested format is:</p> <p>We thank [consultant's name(s)] for [his/her/their] assistance with [describe tasks accomplished], which was made possible through University of Arizona Research Technologies Collaborative Support program. </p>"},{"location":"policies/buy_in/","title":"Buy-in","text":""},{"location":"policies/buy_in/#overview","title":"Overview","text":"<p>The University of Arizona's High Performance Computing (HPC) clusters are servers (computing nodes) and associated high performance storage. There are additional nodes to meet specific needs like high amounts of memory or GPUs. All UArizona research faculty can sign up for free monthly allocation following these directions. For researchers who need compute resource beyond the free standard allocation, and who have funding available, we encourage 'buy-in' of additional compute nodes.</p>"},{"location":"policies/buy_in/#benefits-of-buy-in","title":"Benefits of Buy-in","text":"Dedicated Research Compute Research groups can 'Buy-In' (add resources such as processors, memory, etc.) to the base HPC systems as funding becomes available. Researchers receive 100% of the CPU*hour time their purchases create as a monthly high-priority allocation. This time receives the highest priority queue on the HPC systems. Quality Environment The Buy-In option allows research groups to take advantage of the central machine room space that is designed for maintaining high performance computing resources. The UITS Research Technologies group physically maintains the purchased nodes, applies updates and patches, monitors the systems for performance and security, and manages software. Additionally, Research Technologies staff is available for research support. In short, essentially all costs associated with maintaining compute resources are covered by UITS rather than individual researchers. Flexible Capacity Buy-in research group members also benefit from their resources being integrated into a larger computing resource. This means the buy-in resources can be used in conjunction with the free allocation and resources provided to address computational projects that would be beyond the capacity of a group running an independent system alone. Shared Resource The University research computing community as a whole benefits from buy-in expansions to the HPC systems. As mentioned above, researchers who buy-in receive 100% of the allocation of time for their purchase. However if the buy-in resources are not fully utilized, they are made available as windfall resources. This helps to ensure full use of all HPC resources and can be used to justify future purchases of computing resources. Cost Competitiveness Lower costs included in the grant proposals (i.e. hardware only, no operations costs) and evidence of campus cost\u2010sharing give a positive advantage during funding agency review. Pricing For the year following the award the UArizona HPC request for proposal (RFP) pricing is locked in and is often considerably less than the \"market price.\""},{"location":"policies/buy_in/#buy-in-policies","title":"Buy-in Policies","text":"<ul> <li>For Puma, the University of Arizona could only purchase whole chassis units from Penguin Computing. That is 4 CPU nodes (option 1D), 1 GPU node with 4 GPUs (option 2D), or 1 high memory nodes (option 3). Research Computing worked to match partial node buy-in requests to make full nodes.</li> <li>Monthly high priority time is calculated as: \\(\\frac{{\\text{{Number of CPUs}} \\times 24 \\text{{ hours/day}} \\times 365 \\text{{ days/year}}}}{{12 \\text{{ months/year}}}}\\)</li> <li>Purchasing GPUs expands the limit the PI has on number of GPUs that can be used at any time.</li> <li>Buy-in high priority allocations will last the lifetime of the system. Puma was purchased in August 2020 and will be officially end-of-life August 2025.</li> <li>The HPC Buy-in program is not designed to replace or compete with the very large\u2010scale resources at national NSF and DOE facilities, e.g. ACCESS, the Open Science Grid, etc. National resources are available at no financial cost to most US-based researchers through competitive proposal processes. Please contact our consulting team if you are interested in applying for these resources.</li> <li>The HPC Buy-in program is designed to meet the needs of researchers with medium\u2010scale HPC requirements who want guaranteed, consistent access to compute resources.</li> </ul>"},{"location":"policies/buy_in/#high-priority-allocation-policies","title":"High-priority Allocation Policies","text":"<ul> <li>Standard and high priority jobs will preempt windfall jobs when necessary. </li> <li>Standard jobs do not run on high priority nodes since standard jobs can not be preempted</li> <li>High priority jobs are run on both the buy-in nodes and the centrally-funded nodes. This is advantageous if there is a short-term project deadline.</li> </ul>"},{"location":"policies/buy_in/#compute-buy-in-details-puma-2020","title":"Compute Buy-in Details (Puma 2020)","text":""},{"location":"policies/buy_in/#hardware","title":"Hardware","text":"<p>The buy-in process for Puma has ended. The community will be informed when the next purchase cycle is announced.</p> Buy-in Option Technical Specs CPU-Only NodePenguin Computing Altus XE2242 There are 4 CPU nodes in an Altus XE2242 chassisTechnical specs for 1 node of 4 in an Altus XE2242 chassis - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.4 GHz, 225 W) - 512 GB RAM, DDR4-3200MHz REG, ECC, 2Rx4 (16 x 32 GB) - 2 TB SSD local hard drive, 2.5\u201d, NVMe, 4 Lane, 1 DWPD, 3D TLC GPU Node Penguin Computing Altus XE2214GT GPU chassis have 4 GPUs in themTechnical specs for the full XE2214GT chassis - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.3 GHz, 225 W) - 4 NVIDIA Tesla V100S-PCIe, 32 GB video memory, 5120 CUDA, 640 Tensor, 250 W   - 512 GB RAM, DDR4-3200 MHz REG, ECC, 2Rx4 (16 x 32 GB) - 2 TB SSD local hard drive, NVMe, 4 Lane, 1 DWPD, 3D TLC High Memory NodePenguin Computing Altus XE1212 - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.4 GHz, 225 W) - 3072 GB RAM, DDR4-2933 MHz LR, ECC, 4R (24 x 128 GB)  - 2 TB SSD local hard drive, NVMe, 4 Lane, 1 DWPD, 3D TLC"},{"location":"policies/buy_in/#cost-and-allocations","title":"Cost and Allocations","text":"Notice <ul> <li>With V100S GPU's no longer available the pricing will be different.</li> <li>The locked in pricing expired February 28, 2022.  </li> </ul> Option Number CPU Cores V100s GPU RAM (GB) Monthly High-priority Allocation Cost CPU-only Options 1A - One CPU node 96 512 70,080 $8,037.50  (expired) 1B - Two CPU nodes 192 512 140,160 $16,075.00  (expired) 1C - Three CPU nodes 288 512 210,240 $24,112.00  (expired) 1D - Full Altus XE2242 384 512 280,320 $32,150.00  (expired) GPU Node Options 2A - 1/4 Altus XE2214GT 24 1 512 17,520 $8,523.75  (expired) 2B - 2/4 Altus XE2214GT 48 2 512 35,040 $17,047.50  (expired) 2C - 3/4 Altus XE2214GT 72 3 512 52,560 $25,571.25  (expired) 2D - Full Altus XE2214GT 96 4 512 70,080 $34,095.00  (expired) High Memory Node 3 - Full Altus XE1212 96 3072 70,080 $42,230.00  (expired)"},{"location":"policies/committees/","title":"Committees","text":"<p>The Research Computing Guidance Committee (RCGC) is a cross-departmental group of researchers and IT professionals at the University of Arizona with oversight of central research computing resources.</p> <p>The charge to the committee is to design and implement the policies and procedures; the oversight of the operations; and promotion and recommendations for the centrally funded and administered research computing resources. The policies and procedures will be monitored and updated by this steering committee and related task forces created by this committee. The resources will be administered, maintained, and supported by UITS Research Computing, Systems, and Operations.</p>"},{"location":"policies/loss_of_university_affiliation/","title":"Loss of University Affiliation","text":""},{"location":"policies/loss_of_university_affiliation/#affiliation-loss-policy","title":"Affiliation Loss Policy","text":"<p>When you lose affiliation with the university, such as through graduation or leaving a work position, access to HPC resources will be automatically terminated on the effective date listed in the University of Arizona Records Database.</p> <p>Unfortunately, we are unable to extend HPC access for those losing affiliation. However, you may regain access by registering as a Designated Campus Colleague (DCC) through Human Resources. Once approved, DCCs can request sponsorship from a university faculty member.</p>"},{"location":"policies/loss_of_university_affiliation/#retirement-and-emeritus-faculty","title":"Retirement and Emeritus Faculty","text":"<p>HPC users who retire retain their university affiliation and continue to have access to HPC resources. However, emeritus faculty are not eligible for Principal Investigator (PI) status. To maintain access to compute and storage allocations, emeritus faculty must be sponsored by an active university PI.</p>"},{"location":"policies/loss_of_university_affiliation/#data-after-affiliation-loss","title":"Data After Affiliation Loss","text":""},{"location":"policies/loss_of_university_affiliation/#non-pi-users","title":"Non-PI Users","text":"<p>Home directory data are retained for 90 days following loss of affiliation. If university affiliation is reestablished and HPC access is restored within those 90 days, files will be accessible upon login.</p> <p>Data stored in shared locations such as <code>/groups</code> or <code>/xdisk</code> persist for as long as the storage location is available (see below) and are not deleted after affiliation loss. However, it should be noted that without access to HPC, data will not be directly downloadable by the user. Users will want to plan ahead of time and retrieve any data they need off the system before they lose access. Otherwise, users will need to coordinate with active group members who may retrieve their data on their behalf. If file permission changes are needed, contact our consultants and they can help.  </p>"},{"location":"policies/loss_of_university_affiliation/#pis","title":"PIs","text":"<p>Home directory data are retained for 90 days following loss of affiliation. If university affiliation is reestablished and HPC access is restored within those 90 days, files will be accessible upon login.</p> <p>A PI's communal <code>/groups</code> directory will be removed on the first day of denial of access. PI's should coordinate with their group members to ensure data are moved off the system prior to this to prevent data loss. The exception is if the faculty member is retiring with emeritus status or transitioning to DCC status, provided there is no gap in affiliation between their faculty status and the new role. In this case, they may retain their <code>/groups</code> directory. </p> <p>Similar to above, a PI's communal directory <code>/xdisk</code> will be removed on the first day of denial of access. The exception is if the faculty member is retiring with emeritus status or transitioning to DCC status, provided there is no gap in affiliation between their faculty status and the new role, then the xdisk will remain until its expiration date. It should be noted that because emeritus faculty and most DCC affiliates are not classified as PIs, once the xdisk expires, a new one cannot be requested. </p> <p>If the a communal directory has been deleted and data retrieval is needed, contact us immediately and we may be able to restore your files.</p>"},{"location":"policies/maintenance/","title":"Maintenance","text":""},{"location":"policies/maintenance/#maintenance","title":"Maintenance","text":""},{"location":"policies/maintenance/#planned-maintenance","title":"Planned Maintenance","text":"<p>Most maintenance is performed during regular hours with no interruption to service.  System wide maintenance is usually planned ahead of time and is scheduled for Wednesdays from 8:00 AM to 5:00 PM with at least 10 days notice.  These will be planned to occur four times per year.</p> <p>Maintenance windows represent periods when UITS may choose to drain the queues of running jobs and suspend access to the cluster operation for HPC maintenance purposes.</p> <p>The notification will describe the nature and extent (partial or full) of the interruptions of HPC services. </p>"},{"location":"policies/maintenance/#system-wide-maintenance","title":"System-wide Maintenance","text":"<p>Impacts to job queues</p> <p>During system-wide maintenance cycles, jobs queues are impacted before and during maintenance. Jobs submitted whose runtimes would overlap with maintenance are held until maintenance is concluded.</p> <p>Some maintenance cycles require the entire system to be taken offline. In preparation, batch queues will be modified prior to scheduled downtimes to hold jobs which request more wallclock time than remains before the shutdown. Held jobs will be released to run once maintenance concludes.</p>"},{"location":"policies/maintenance/#rolling-maintenance","title":"Rolling maintenance","text":"<p>Impacts to job queues</p> <p>During rolling maintenance cycles, job queues are impacted during and after maintenance. All nodes are drained, meaning they cannot accept new jobs and must allow running jobs to complete before they can be updated, rebooted, and put back online. The system may be slower to accept new jobs for 10 days following these maintenance cycles.</p> <p>Rolling maintenance cycles are implemented to facilitate updates or maintenance tasks without necessitating a complete system shutdown. Throughout rolling maintenance, nodes will stop accepting new jobs, allowing currently running tasks to finish uninterrupted. As nodes gradually become vacant, they are taken offline, updated, rebooted, and then restored to service. This iterative process ensures minimal disruption to ongoing computational tasks while maintenance is underway. It's important to note that during rolling maintenance cycles, job queues may experience a temporary slowdown as nodes await reboot.</p>"},{"location":"policies/maintenance/#emergency-maintenance","title":"Emergency Maintenance","text":"<p>Unavoidable (emergency) downtime may occur as a result of any of the above reasons at almost any time. Such events are rare and great effort is made to avoid these situations. However, when emergency maintenance is needed, the UITS unit responsible for the item affected will provide as much notice to users as possible and work to resolve the fault as quickly as possible.</p> <p>Any emergency outages will be announced via email through the hpc-announce@list.arizona.edu mailing list. </p>"},{"location":"policies/maintenance/#maintenance-history","title":"Maintenance History","text":"January 29, 2025 <p> Type: Rolling Maintenance   <ul> <li>Routine patching of all nodes.</li> <li>Completion of OS migration. All remaining CentOS 7 Puma nodes migrated to Rocky Linux 9. Puma9 made default cluster.</li> </ul> October 30, 2024 <p> Type: Rolling Maintenance     <ul> <li>Routine patching of all nodes and storage array.</li> <li>OS upgrades continue. A block of nodes were migrated from CentOS 7 to Rocky 9, accessible from the login nodes using <code>puma9</code>. More details can be found on our OS Updates page.</li> </ul> </p> July 31, 2024 <p> Type: Rolling Maintenance     <ul> <li>OnDemand graphical jobs limited to four days to support general resource availability.</li> <li>User portal upgraded to support mobile clients. </li> <li>New GPU partitions introduced to improve GPU resource availability.</li> </ul> </p> April 24, 2024 <p> Type: Rolling Maintenance     <ul> <li>OnDemand Upgrade.</li> <li>Gatekeeper moved to EL8 operating system.</li> <li>Enabled job script storage in slurm accounting.</li> </ul> </p> January 31, 2024 <p> Type: Rolling Maintenance     <ul> <li>General operating system patches.</li> <li>Qumulo storage array update.</li> <li>Slurm configuration improvements.</li> <li>Metrics (XDMOD) OS upgrade.</li> <li>RStudio Server support for R version 4.3.2.</li> <li>OnDemand OS upgrades.</li> </ul> </p>"},{"location":"policies/special_projects/","title":"Special Projects","text":""},{"location":"policies/special_projects/#overview","title":"Overview","text":"<p>The Research Computing Governance Committee (RCGC) has approved support for \"Special Projects\" that use more than the standard allocation of hours. When a special project request is granted, an additional allocation of standard hours is provided for a limited period of time.   </p> <p>There is not a specific definition of what comprises a project, but it is often support for publication or grant deadlines, or graduation dates. Each request is considered on a case by case basis.</p>"},{"location":"policies/special_projects/#authorized-requestors","title":"Authorized requestors","text":"<p>Project requests must be submitted by a Principal Investigator (PI); partly because the additional time granted will go to the allocation of the PI.</p>"},{"location":"policies/special_projects/#guidelines","title":"Guidelines","text":"<ul> <li>All special project allocations are temporary.</li> <li>Special project allocations cannot be granted if they will impact the community of users with a standard allocation.</li> <li>PIs can only be granted a special project once within a 12 month period (starting from the ending period of the last special project).<ul> <li>PI groups that routinely need more standard computing hours should supplement their needs in other ways (e.g. buy-in, accessing national/international computing resources, etc).</li> <li>Our research facilitators can help UArizona PIs access national-scale computing centers (e.g. ACCESS, TACC, Open Science Grid, etc).</li> </ul> </li> </ul>"},{"location":"policies/special_projects/#categories","title":"Categories","text":"Size Requirements 10,000 hours per month or less The majority of requests fall into this category. These requests provide a general statement of the need and are one month or less in duration. These requests are occasional and they can be granted automatically by UITS staff with minimal impact to standard queue usage. This will be done at the discretion of the Research Technologies staff. 10,000 to 100,000 hours per month These requests must provide a defined number of hours and not to exceed 3 months. A defined statement of need (e.g. publication deadline) will be provided. These requests are occasional, and can be granted by Research Technologies staff after considering the researcher\u2019s need, alternative ways to solve those needs, and assessing that there is no overall impact to the system. Greater than 100,000 hours per month These requests must provide a defined number of hours, defined duration not to exceed 6 months, and a defined statement of need (e.g. publication deadline). These requests required PIs to report back the results of their calculations (publications, conference presentations, etc) for potential inclusion in our online documentation. Benchmarking, profiling, or assessment of analyses run should also be provided to help the UArizona HPC understand how the additional computing time was used and ways that need can be met in the future without a special project. These requests will be forwarded to the HPC policies subcommittee of RCGC and require committee approval before being granted."},{"location":"policies/special_projects/#submitting-a-request","title":"Submitting a Request","text":"<p>Requests are submitted via a web form in the HPC user portal under Support Requests. </p> <p></p> <p>In the form that opens,  Please include:</p> <ul> <li>Number of additional standard hours needed for the special project.</li> <li>Duration of the Special Project in months.</li> <li>Reason for the temporary increase.</li> </ul> <p></p>"},{"location":"quick_start/accessing_compute/","title":"Accessing Compute Nodes","text":""},{"location":"quick_start/accessing_compute/#accessing-compute-nodes","title":"Accessing Compute Nodes","text":""},{"location":"quick_start/accessing_compute/#the-compute-nodes","title":"The Compute Nodes","text":"<p>Unlike the bastion host and login nodes, there are many compute nodes and each has, as the name suggests, a large amount of computational resources available to run your work. For example, Puma standard nodes have 94 available CPUs and a whopping 470 GB of RAM. </p> <p></p> <p>To get a sense of what the cluster looks like, try running the command <code>nodes-busy</code>. The output should look something like this:</p> <pre><code>\u271a    Buy-in nodes. Only accept high_priority and windfall jobs\n(puma) [netid@wentletrap ~]$ nodes-busy \n==============================================================\n\n                      \u2592 System Status \u2592\n              Wed Feb 14, 03:42:09 PM (MST) 2024\n\nStandard Nodes\n==============================================================\nr1u16n2  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u17n2  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u18n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u25n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u26n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u26n2  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u27n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \n</code></pre> <p>Each line shows one compute node on the cluster you're connected to and how busy it is running jobs. By default, when you first log in you're connected to the Puma cluster. This is the largest, newest, and generally provides the most in terms of computational resources. However, we have two other clusters available: Ocelote and El Gato, each with a good number of computational resources available and shorter wait times to access them. </p> <p>When you first connected to a login node in the previous section, your terminal should have displayed:</p> <pre><code> ***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nPuma:\n$ puma\n(puma) $\nOcelote:\n$ ocelote\n(ocelote) $\nElGato:\n$ elgato\n(elgato) $\n-----------------------------------------\n</code></pre> <p>This shows you the various shortcuts you can use to connect to the different clusters. Try running the command <code>elgato</code> now. You should see a change in your terminal prompt to indicate that your cluster has changed. </p> <pre><code>(puma) [netid@wentletrap ~]$ elgato\n(elgato) [netid@wentletrap ~]$ \n</code></pre>"},{"location":"quick_start/accessing_compute/#job-charging","title":"Job Charging","text":"<p>Before we connect to a compute node, let's quickly cover how access is charged. </p> <p>Every HPC group gets a free allocation of CPU hours that they can spend every month to access compute resources. You can think of a CPU hour as a token to buy one CPU for one hour, so if you want to reserve 5 CPUs for 10 hours, this will charge 50 CPU hours to your account. You can see more detailed information on job queues, allocations, and job charging on our Time Allocations page which has a comprehensive breakdown. </p> <p>For this tutorial, we'll focus on the standard partition. This is a job queue and is one that consumes your standard allocation. To use this job queue, you'll need to know your account name. To check, use the command <code>va</code> which stands for \"view allocation\". The output will look something like:</p> <pre><code>(elgato) [netid@wentletrap ~]$ va\nWindfall: Unlimited\n\nPI: parent_974 Total time: 7000:00:00\n    Total used*: 1306:39:00\n    Total encumbered: 92:49:00\n    Total remaining: 5600:32:00\n    Group: group_name Time used: 862:08:00 Time encumbered: 92:49:00\n\n*Usage includes all subgroups, some of which may not be displayed here\n</code></pre> <p>You should see a name next to the <code>Group</code> field (in the example above, this is <code>group_name</code>). If you see multiple groups, then you are sponsored in multiple groups and can choose any one of your group names. Note the name of your account and hang onto it for the upcoming sections.</p>"},{"location":"quick_start/accessing_compute/#interactive-jobs","title":"Interactive Jobs","text":"<p>Now, let's actually access a compute node. When you're connected to a login node, you can initiate a Slurm job to interactively connect to a compute node by using the command <code>interactive</code>. By default, this will give you one CPU for one hour. You can adjust this using the different flags available which are documented on our Interactive Jobs page. For now, we'll stick with the default resources. </p> <p>To access a session, run the following, substituting your own group name (that you found with <code>va</code> in the section above) for <code>&lt;group_name&gt;</code>: <pre><code>interactive -a &lt;group_name&gt;\n</code></pre></p> <p>For example, in my case: <pre><code>(elgato) [netid@wentletrap ~]$ interactive -a hpcteam\nRun \"interactive -h for help customizing interactive use\"\nSubmitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1    --ntasks=1 --time=01:00:00 --account=hpcteam --partition=standard\nsalloc: Granted job allocation 1800857\nsalloc: Nodes cpu39 are ready for job\n[netid@cpu39 ~]$ hostname\ncpu39.elgato.hpc.arizona.edu\n</code></pre></p> <p>You'll notice once your job starts that your command line prompt changes to display the name of the compute node. If you run <code>hostname</code>, this should match your command line prompt and show you the name of the compute node you're connected to. In my case, I'm connected to the El Gato compute node <code>cpu39</code>.</p> <p>You'll also notice that your session has been assigned a job number (in the above, you can see this as <code>Granted job allocation 1800857</code>). A job number is assigned to every job on the system and is used to keep track of job statistics and metrics. </p> <p>Since you're in an interactive session, you now have exclusive access to the resources you've reserved which means you can do things like use software to develop, test, run, and debug your code. Interactive sessions are optimal for these sorts of actions. However, you might run into problems executing your analyses if:</p> <ul> <li>Your session times out due to inactivity</li> <li>Your internet connection gets disrupted</li> <li>Your computer gets closed or turned off</li> <li>You want to run more than one job at a time</li> </ul> <p>That's where batch jobs come in. </p>"},{"location":"quick_start/accessing_compute/#batch-jobs","title":"Batch Jobs","text":"<p>Detailed Intro to Batch</p> <p>Below is a brief summary of batch scripts and how to run them. For a more detailed walkthrough, see our Batch Jobs documentation.</p> <p>Batch jobs are the real workhorses of HPC. In contrast to interactive jobs, batch jobs are a way of submitting work to run on a compute node without the need for an active connection. This allows you to execute large jobs that may need a long time to run, or many (hundreds or even thousands) of jobs without the need to be present. </p> <p>Batch jobs are run using batch scripts. Batch scripts are just text files that act as blueprints that the scheduler uses to allocate resources and execute the terminal commands needed to run your analysis. Batch scripts have three sections: </p> <ol> <li>The \"shebang\" will always be the line <code>#!/bin/bash</code>. This tells the system to interpret your file as a bash script. Our HPC systems use bash for all our environments, so it should be used in your scripts to get the most consistent, predictable results.</li> <li>The directives section will have multiple lines, all of which start with <code>#SBATCH</code>. These lines are interpreted as Slurm directives by the scheduler and are how you request resources on the compute nodes, set your output filenames, set your job name, request emails, etc. </li> <li>The code section in your script is a set of bash commands that tells the system how to run your analyses.</li> </ol> <p>An example batch script might look like the following:</p> <pre><code>#!/bin/bash# --------------------\n### Directives Section\n# --------------------\n#SBATCH --job-name=hello_world\n#SBATCH --account=&lt;your_group&gt;\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:01:00# --------------------\n### Code Section\n# --------------------\necho \"Hello world, I am running on compute node $HOSTNAME\"\n# sleep only used for demonstration purposes\nsleep 30</code></pre> <p>In the script above, we're requesting one minute of runtime (<code>--time=00:01:00</code>), one CPU (<code>--ntasks=1</code>), one physical computer (<code>--nodes=1</code>), and are using the standard partition. More detailed information on what each of these directives mean plus many others can be found in our Batch Directives documentation.</p> <p>Try creating a file on HPC called <code>hello_world.slurm</code> and add the contents above, replacing <code>&lt;your_group&gt;</code> with your own group's name.</p> <p>Not sure how to create a text file from the command line?</p> <p>Try <code>nano hello_world.slurm</code>, then simply enter your text. To save and exit, use Ctrl + X, select Y to save, and Enter to complete the process.</p> <p>Once you have your text file, you can submit your job by using the command <code>sbatch</code> followed by your script's name. This will return a job ID (just like your interactive job). For example:</p> <pre><code>[netid@cpu39 ~]$ sbatch hello_world.slurm \nSubmitted batch job 1940917\n</code></pre> <p>This sends your script to the scheduler, which puts your job in queue. Once the resources you have requested become available, your job automatically begins running. The time your job spends in queue depends on many factors, including the scale of your resource request and the overall system usage. To check on jobs you have submitted, use the command <code>squeue --job=&lt;your_jobid&gt;</code>. For example:</p> <pre><code>[netid@cpu39 ~]$ squeue --job 1940917\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           1940917  standard hello_wo    netid  R       0:08      1 cpu37\n</code></pre> <p>After the scheduler has received your request, the rest happens automatically. This means you can close your SSH connection, or even turn off your personal computer and walk away without interrupting your jobs. </p> <p>Submitting from login and compute nodes</p> <p>You can submit your jobs either from the login nodes or from compute nodes. If you submit from an interactive session, you can terminate your interactive job and it will not affect the batch job you've submitted.</p> <p>Once your job starts, you should see an output text file with the name <code>slurm-&lt;your_job_id&gt;.out</code>. This will contain any text that would have been printed to the terminal if you had run your work interactively. This text file is updated in real time allowing you to monitor your job's progress as it runs. Checking the output of the script we just ran, you should see something that looks like the following: </p> <pre><code>[netid@cpu39 ~]$ cat slurm-1940917.out \nHello world, I am running on compute node cpu37\n</code></pre> <p>That's it! You just ran your first batch job. As a next step, we'll cover how you can access software to run your analyses, allowing you to do more in your jobs than just run bash commands. </p>"},{"location":"quick_start/common_misconceptions/","title":"Common Misconceptions","text":""},{"location":"quick_start/common_misconceptions/#common-misconceptions","title":"Common Misconceptions","text":"<p>Both experienced and novice users may benefit from reading through these common misconceptions.</p> Do not run computations on the login nodes. <p>See Running Jobs for detailed instructions on the proper way to run computationally intensive tasks. </p> <p>If I move my code to HPC, it will automatically run faster</p> <p>You might be surprised to learn that if you move code from a local computer to a supercomputer, it will not automatically run faster and may even run slower. This is because the power of a supercomputer comes from the volume of resources available (compute nodes, CPUs, GPUs, etc.) and not the clockspeed of the processors themselves. Performance boosts come from optimizing your code to make use of the additional processors available on HPC, a practice known as parallelization.</p> <p>Parallelization enables jobs to \"divide-and-conquer\" independent tasks within a process when multiple threads are available. In practice, this typically means running a job with multiple CPUs on the HPC. On your local machine, running apps like your web browser is natively parallelized, meaning you don't have to worry about having so many tabs open. However, on the HPC, parallelization must almost always be explicitly configured and called from your job. This process is highly software-dependent, so you'll want to research the proper method for running your program of choice in parallel. </p> <p>If I allocate more CPUs to my job, my software will use them</p> <p>Running a job with a large number of CPUs when the software has not been configured to use them is a waste of your allocation, your time, and community resources. Software needs to be designed to use multiple CPUs as part of its execution. You will need to ensure your software has the capability to make use of multiple CPUs for it to be able to take advantage of additional hardware. The job scheduler only provides the resources, the code itself is what needs to know how to make use of them.</p> <p>All nodes on a supercomputer are the same</p> <p>Navigating the HPC means being aware of the different types of nodes you can land on. For example, the login node is available to all users by default upon login, and is designed for managing and editing files. However, it is not designed to run production computations. Running jobs that are too computationally intensive on the login node can severely impact performance for other users. Such jobs will be noticed and stopped by the HPC systems team.</p> <p>Types of nodes on the UArizona HPC system include the Bastion Host, the Login Node, the Compute Nodes, and the Data Transfer Node. See Compute Resources for information on the compute hardware available.</p> <p>As a user I (am)(am not) allowed to install my own software</p> <p>Well, it depends. Users can create custom environments and install packages for languages like Python and R by using their built-in package managers. Users are even encouraged to download software from GitHub or other repositories and compile it themselves (provided it is done on a compute node). However, system-wide modules are generally taken care of by the HPC team. If you would like something to be installed as software available to all HPC users, you can make a request through ServiceNow. But, if you would like something to be installed for personal use, or use between members of your group, you are encouraged to install it in one of your shares on the HPC filesystem. </p>"},{"location":"quick_start/logging_in/","title":"Logging In","text":""},{"location":"quick_start/logging_in/#logging-in","title":"Logging In","text":""},{"location":"quick_start/logging_in/#system-access","title":"System Access","text":"<p>Account creation is necessary to log in</p> <p>If you have not yet done so, you will need to register for an account to log in. See our registration documentation for steps. </p> <p>Login issues</p> <p>If you experience any issues during the login process, see our FAQs for common problems.</p> Do not run computations on the login nodes. <p>See Running Jobs for detailed instructions on the proper way to run computationally intensive tasks. </p> <p>Once you've successfully registered for an HPC account, you're ready to log in. There are two main methods to access the HPC system</p> <ol> <li> <p>Open OnDemand</p> <p>This is a browser-based application that provides users with a graphical interface to access the HPC filesystem, run software that requires a graphical component, or access an interactive desktop environment. The login portal for Open OnDemand uses the familiar UArizona WebAuth login screen. HPC accounts are tied to university accounts, so use your standard NetID and password (i.e. the one used for your email).</p> </li> <li> <p>Terminal</p> <p>The terminal is a text-based command interpreter provided by the operating system on your local machine. Mac and Linux users can access the \"Secure SHell\" (SSH) command by default, and Windows users will either have to use the Linux subsystem for Windows, or a program called PuTTY. Using these tools, users can access a command-line environment on the HPC, which can be used to manage files, write code, install software, and submit jobs. See our Bash Cheat Sheet for an overview of common commands.</p> </li> </ol>"},{"location":"quick_start/logging_in/#system-layout","title":"System Layout","text":"<p>The inner workings of HPC systems may be somewhat obscured to new users. In this section, we'll give you an idea of how the system is laid out so you understand exactly where you are at each stage of the login process and what activities are performed where. </p>"},{"location":"quick_start/logging_in/#the-bastion-host","title":"The bastion host","text":"<p>In another browser window, open our instructions on logging in from the command line. Start by following the first step shown that's specific to your operating system. Stop when your terminal displays </p> <p><pre><code>Success. Logging you in...\nLast login:\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n</code></pre> If all has gone well, you are now connected to what is known as the bastion host. </p> <p></p> <p>The bastion host is the first computer you land on when you log in using the hostname <code>hpc.arizona.edu</code>. This machine is only used to validate your credentials and provide a gateway to the rest of the HPC environment. It is not used for storing files and has no software installed so no computational work is done at this stage. As a test, try running the command <code>hostname</code>:</p> <pre><code>[user@gatekeeper 14:50:49 ~]$ hostname\ngatekeeper.hpc.arizona.edu\n</code></pre> <p>The output shows <code>gatekeeper</code> which is the name of this node and is how you can tell you're connected to the bastion. </p> <p>Next, to advance from the bastion host, type the command <code>shell</code>.</p>"},{"location":"quick_start/logging_in/#the-login-nodes","title":"The login nodes","text":"<p>After you type <code>shell</code> on the bastion host, you're connected to a computer called a login node. </p> <p></p> <p>We have two of these available and you will be assigned one at random. If you run the <code>hostname</code> command as you did on the bastion host, you should see either <code>wentletrap</code> or <code>junonia</code>. </p> <p>A login node is a shared workspace with minimal computational capabilities and very little software installed. This is not the place where computational work is done so users should not run their analyses, compile their software, or perform computationally intensive work in this location. Instead, the login nodes are meant for activities such as managing files, writing scripts, submitting and monitoring jobs, and viewing system resources.</p>"},{"location":"quick_start/overview/","title":"HPC Quick Start","text":""},{"location":"quick_start/overview/#overview","title":"Overview","text":"<p>Welcome to the University of Arizona's (UArizona) High Performance Computing (HPC)! </p> <p>This Quick Start Tutorial is the place to be if you are new to HPC, or simply wish to take a refresher of the basics. We will cover some concepts that are general to HPC, as well as those that are specific to the system here at UArizona.</p>"},{"location":"quick_start/overview/#getting-started-checklist","title":"Getting Started Checklist","text":"<p>Before beginning this tutorial, you'll want to ensure that you're familiarized with our system policies and have registered for an account. If you haven't done so yet, please take a moment to review the following pages before proceeding with the rest of the tutorial.</p> <ul> <li> Policies: Read up on our HPC guidelines. In particular:<ul> <li> Acceptable use</li> <li> Acknowledgements</li> <li> What happens if you lose university affiliation</li> </ul> </li> <li> Register for an account: To log into HPC, you'll need to have registered for an account. If you have not yet done so, see our registration documentation for steps. Note that the process varies based on your university affiliation. </li> </ul>"},{"location":"quick_start/overview/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this quick start, you should know:</p> <ul> <li> What HPC is</li> <li> How the UArizona HPC is structured</li> <li> How to log in</li> <li> What storage you have access to</li> <li> What time allocations are</li> <li> How to access computational resources</li> <li> How to access software</li> </ul> <p></p>"},{"location":"quick_start/software/","title":"Software","text":""},{"location":"quick_start/software/#software","title":"Software","text":"<p>As much fun as echoing \"Hello World\" is, it's hard to perform analyses without software. We've seen that software packages are not available on the login nodes, but now that we're interactively connected to a compute node, we can view and use what's available. </p> <p>Since the HPC is a shared system, we need to organize our installed packages in a way that's helpful for everyone. There are three main categories of software on HPC:</p>"},{"location":"quick_start/software/#system-software","title":"System Software","text":"<p>System software represents executables pre-installed on the system, and are available everywhere. This includes basic bash commands, as well as tools developed by our team, like <code>va</code> and <code>nodes-busy</code>. </p>"},{"location":"quick_start/software/#modules","title":"Modules","text":"<p>Modules are software packages that are only accessible on compute nodes. Modules make it easy to load and unload software from your environment and allow hundreds of packages to be available on the same system without dependency or versioning conflicts. </p> <p>Specifying module versions</p> <p>It's always good practice to specify which software version you need when loading a module to ensure a stable environment.</p> <p>You can view and load software modules using the commands <code>module avail</code> and <code>module load</code>, respectively. For example:</p> <pre><code>[netid@gpu66 ~]$ module avail python\n\n------------------------------------------------- /opt/ohpc/pub/modulefiles --------------------------------------------------\n   python/3.6/3.6.5    python/3.8/3.8.2 (D)    python/3.8/3.8.12    python/3.9/3.9.10\n\n[netid@gpu66 ~]$ module load python/3.9\n[netid@gpu66 ~]$ python3 --version\nPython 3.9.10\n</code></pre> <p>Try running <code>module avail</code> without specifying any arguments. You'll notice we have a lot available. These software packages can be used both interactively for shorter jobs and testing, or in your batch scripts.</p> <p>If you have software that you would like installed as a module, please refer to our software policies to see if it's a good candidate. </p>"},{"location":"quick_start/software/#personal-software","title":"Personal Software","text":"<p>Installing software packages locally into your own directories is encouraged. This means you can download, install, manage, and customize software from the internet (e.g. Pip, CRAN, GitHub) without waiting for someone else to install it on HPC for you. Software should be compiled on a compute node so that the load doesn't strain the login nodes and to ensure you have access to the newest system software and compilers.</p> <p>If you would like to install your own software, please refer to our guidelines in User Installations which also has some handy tips and tricks if you've never installed software locally (i.e., not in a root-owned location) before. </p>"},{"location":"quick_start/storage_and_transfers/","title":"Storage and Transfers","text":""},{"location":"quick_start/storage_and_transfers/#storage-and-transfers","title":"Storage and Transfers","text":"<p>When you first access a login node, you're located in your home directory. This is a space with a 50 GB limit and is accessible to only you. The files you store here are housed on a large storage array and are accessible anywhere you are on the system except the bastion host. </p> <p>Do not overfill your home directory</p> <p>Once your data reaches the 50 GB limit you will experience problems, like the inability to log into Open OnDemand because the session file cannot be created. And your jobs may fail with unexpected errors depending on how your software reacts to no more space for writing output. </p> <p></p> <p>To store your files in your home, you will need to transfer them to the system. Small files can most easily be transferred to/from HPC using our web interface Open OnDemand. In the upper-left you'll see a dropdown called Files where you can select Home Directory. </p> <p></p> <p>On the following page, select \"Upload\" to open a window where you can drag/drop files.</p> <p></p> <p>For larger files, we have a designated Data Transfer Node (DTN). Comprehensive instructions for alternative methods for file transfers can be found on our data transfer page. </p> <p>With larger files comes the need for more storage. If you find your home is insufficient to store your data, group allocations are available. See our storage documentation for details on options that are available. </p> <p>Now that we're on the login nodes and know where our files are, it's time to access a compute node. </p>"},{"location":"quick_start/summary/","title":"Summary","text":""},{"location":"quick_start/summary/#summary","title":"Summary","text":"<p>That's it! You now should have a sense of how UArizona's HPC systems are laid out, how to log in, and how to access compute nodes and software.</p> <p>To continue learning about HPC, our online documentation has a lot more information that can help get you started. For example FAQs, information on HPC storage, and file transfers. You may also want to check out our workshops as well as other community events which offer great training materials on a variety of topics. </p> <p>Other great resources include: virtual office hours every Wednesday from 2:00-4:00pm, consultation services offered through ServiceNow, and a YouTube channel with training videos. </p>"},{"location":"quick_start/summary/#external-resources","title":"External Resources","text":"<p>While this documentation covers some concepts and techniques that are widely applicable, it's beyond our scope to cover everything you may need or want to know while using the HPC. We highly encourage users to supplement their learning with resources from around the web! Below are some recommended resources to get started. </p> Bash &amp; Linux reference External Documentation Misc HPC Tutorials Bash Manual Slurm Intro to HPC Bash scripting beginner tutorial HPC Wiki Lawrence Livermore HPC Tutorials Intro to Bash scripting e-book Software Carpentry General purpose linux tutorials Cornell HPC Roadmaps <p>Please note that information related to other HPC clusters may not necessarily apply to the UArizona HPC cluster. Each system is configured differently, and what may be supported on another system may not be supported here, or vice versa. Please refer to this documentation site for information specific to the UArizona HPC, or ask HPC Consult if you have any questions.</p> <p></p>"},{"location":"quick_start/what_is_hpc/","title":"What Is HPC?","text":""},{"location":"quick_start/what_is_hpc/#what-is-hpc","title":"What is HPC?","text":"<p>Keywords in bold</p> <p>Important concepts are emphasized in bold in the text below. We will explore these keywords in depth in the upcoming HPC Quick Start sections. </p> <p>If you've never used an HPC system before, you may be wondering what one is and why you'd want to use it. This section is designed to give you a big picture understanding of what these systems offer and how they may benefit your research.</p>"},{"location":"quick_start/what_is_hpc/#introduction-to-hpc","title":"Introduction to HPC","text":""},{"location":"quick_start/what_is_hpc/#what-is-hpc_1","title":"What is HPC?","text":"<p>HPC stands for High Performance Computing and is synonymous with the more colloquial term Supercomputer. A supercomputer is a collection, or cluster, of a large number of regular computers (referred to as compute nodes) connected over a network. Each of the computers is like a local workstation though typically much more capable. For example, a standard laptop might have 4 CPUs and 8 GB of RAM. Compare this with a standard compute node on one of our clusters, Puma, which has a whopping 94 CPUs and 470 GB of RAM. </p>"},{"location":"quick_start/what_is_hpc/#shared-resource-model","title":"Shared Resource Model","text":"<p>Another main difference between a supercomputer and a personal workstation is that the supercomputer is a shared resource. This means there may be hundreds or even thousands of simultaneous users. Each of these users connects to HPC from their own local workstation and can run work on one or more of HPC's compute nodes. You can imagine with this shared resource model, however, that without some sort of coordination, managing which users get what resources turns into a major logistical challenge. That's why supercomputers use job schedulers. </p>"},{"location":"quick_start/what_is_hpc/#role-of-job-schedulers","title":"Role of Job Schedulers","text":"<p>Batch Jobs Documentation</p> <p>More comprehensive information on using job schedulers and running batch jobs can be found in the Running Jobs section.</p> <p>A job scheduler is software used to coordinate user jobs. In our case, we use a scheduler called Slurm. You can use it by writing a batch script that requests compute resources (e.g., CPUs, RAM, GPUs) and includes instructions for running your code. You submit this script to the job scheduler which then goes and finds available resources on the supercomputer for your job. When the resources become available, it initiates the commands included in your batch script, and outputs the results to a text file. </p>"},{"location":"quick_start/what_is_hpc/#benefits-of-hpc","title":"Benefits of HPC","text":""},{"location":"quick_start/what_is_hpc/#scaling-up-and-scaling-out","title":"Scaling Up and Scaling Out","text":"<p>Supercomputers provide opportunities for data storage and parallel processing that far surpass what is capable in a standard workstation. These systems provide researchers with the ability to scale up or scale out their work.</p> <p>Increasing the data throughput of a single job is known as scaling up. This may mean moving from a 500 GB database on a workstation to a 5 TB database on the HPC, or raising the resolution of your simulation by a factor of 10 or 100. </p> <p>Other types of analyses may benefit from an increased number of jobs, such performing parameter sweeps, running Monte Carlo simulations, or performing molecular dynamics simulations. Local machines are limited by the number of cores accessible to them, decreasing the number of simultaneous computations as compared to an HPC. An increase in the number of CPUs used during analysis is known as scaling out your work.</p>"},{"location":"quick_start/what_is_hpc/#workflow-automation","title":"Workflow Automation","text":"<p>Automation is another feature of HPC systems that allows users to schedule jobs ahead of time, and for those jobs to be run without supervision. Managing a workstation or keeping an SSH terminal active while scripts are running can lead to major complications when running extended analyses. Batch scripts allow a prewritten set of instructions to be executed when the scheduler determines that sufficient resources will be available. This allows for jobs with extended completion times to be run for up to 10 days (a limit imposed by the scheduler). Real-time output is saved to a text file, allowing you to check the progress of the job. Checkpointing is recommended for jobs that require longer than 10 days.</p>"},{"location":"registration_and_access/account_creation/","title":"Account Creation","text":""},{"location":"registration_and_access/account_creation/#overview","title":"Overview","text":"<p>All UArizona Faculty, Staff, Students, and Affiliates are eligible for HPC accounts free of cost.</p> <p>If you are considered to be a PI in the University's employee system (EDS) you may establish a PI account. PI Accounts are self-sponsored, manage their own HPC group(s), and receive their own time and storage allocations to be shared among group members. Staff, students and affiliates must be sponsored by a PI at the PI's discretion to obtain HPC access. Users may be a member of more than one HPC group.</p>"},{"location":"registration_and_access/account_creation/#how-to-register","title":"How to Register","text":"<p>The process of registering for an HPC account varies depending on your affiliation with the university. Take a look at the list below to determine how you should register:</p> I'm a faculty member/principal investigator (PI) <p>If you are a research faculty member or principal investigator, you can sponsor yourself for access through our user portal.       Step 1: Visit https://portal.hpc.arizona.edu/. This will automatically create an HPC account for you.       Step 2: Go to https://portal.hpc.arizona.edu/portal/sendlink.php and click the link on the left-hand side as shown below             This will automatically redirect you back to the user portal, create a research group for you, and add you as a member. You can check your group and membership in the portal by going to the Groups tab and clicking your group's dropdown menu to view its members.        </p> <p>For more information on how you can manage your HPC group, see our Group Management documentation.</p> I'm a student, postdoc, staff member, or Designated Campus Colleague <p>      If you are affiliated with the University of Arizona but are not faculty, you will need to request sponsorship from a faculty member. This can be done through our web portal.       Step 1: Create an HPC account by navigating to https://portal.hpc.arizona.edu/.      Step 2: Request sponsorship from a UArizona faculty member. Note: Your faculty sponsor will need their own HPC account before they are able to sponsor others.           To request sponsorship, navigate to https://portal.hpc.arizona.edu/portal/sendlink.php. On the right-hand side, enter your sponsor's email address and click send.             Your sponsor will then receive an email with a link used to authorize your account. Once they confirm your request, you will receive an email with instructions for accessing the HPC systems.          Note: it may take up to 15 minutes after approval to receive a confirmation email and for your account to officially be activated.          If you do not receive an email verification, you should contact your sponsor and confirm receipt and approval of the HPC account request. If your account has been approved but you have not received verification, you should contact HPC consulting and provide your NetID, your name, and the email address of your sponsor.      </p> I'm not affiliated with the university <p>     HPC systems are restricted to those with valid university credentials and are not available for general public use. However, if you are not officially affiliated with the university but are actively collaborating with university members, you may register for Designated Campus Colleague (DCC) status. This is done through human resources and provides collaborators with active UArizona credentials. Once your DCC status is approved, you may request sponsorship from a university faculty member (see the section above).     </p>"},{"location":"registration_and_access/account_deletion/","title":"Account Deletion","text":""},{"location":"registration_and_access/account_deletion/#manual-deletion","title":"Manual Deletion","text":"<p>If you wish to delete your HPC account, you may do so through the User Portal. Navigate to the Support tab and click the Close Your HPC Account link. </p> <p></p> <p>In a new window, you will be prompted to manually confirm by entering confirm at the prompt. Click Close Account to complete the process.</p> <p></p>"},{"location":"registration_and_access/account_deletion/#loss-of-university-affiliation","title":"Loss of University Affiliation","text":"<p>Losing affiliation with the university will result in the denial of access to HPC resources. This will happen automatically on the day of termination according to the University of Arizona Records Database. Data may be retrievable if a student or employee is reinstated, or by the PI. More details are found under Policies. Please contact us for support in this case. </p> <p>If you are losing affiliation and require continued access to HPC services, you may register as a Designated Campus Colleague (DCC) through Human Resources.</p>"},{"location":"registration_and_access/group_management/","title":"Group Management","text":""},{"location":"registration_and_access/group_management/#overview","title":"Overview","text":"<p>HPC groups allow faculty members to manage file permissions, job allocations, and group members. When a PI creates a new HPC account, a group is created with an allocation of space and time. Additional storage is available for free upon request, called xdisk</p> <p>There are two types of Groups: </p> <ul> <li> <p>Research Groups</p> <p>Research groups include any faculty, postdocs, graduate students, DCCs, staff, or student workers actively affiliated with your group's research. </p> </li> <li> <p>Class Groups</p> <p>These are for educational purposes only and will include students enrolled in a semester-long course.</p> </li> </ul>"},{"location":"registration_and_access/group_management/#research-groups","title":"Research Groups","text":"<p>If you are a faculty member who has registered for an HPC account, a research group named after your UArizona NetID has been automatically created for you. This group has an allocation of CPU hours associated with it as well as communal storage for your data.</p> <p>Membership and Allocation</p> <p>Members of research groups have full access to the PI's allocation. PIs are able to create multiple research groups that pull from the same allocation. Users are able to be members of multiple research groups. Creating multiple research groups does not change the total CPU-time allocation.</p> <p>Permissions</p> <p>Research groups can also be used to manage access permissions to files and folders on HPC. The PI can use the <code>chgrp</code> command to change which group has access rights corresponding to the 'group' setting on a particular item. See our Linux File Permissions cheat sheet for more information.</p>"},{"location":"registration_and_access/group_management/#adding-members","title":"Adding Members","text":"<p>To add members to your research group, go to https://portal.hpc.arizona.edu/ and click the Groups tab at the top of the screen. </p> <p></p> <p>Click your group's dropdown tab and navigate to where it says Add a new group member. In the field below, enter the new group member's NetID and click \" ADD MEMBER\". </p> <p></p> <p>To add members in bulk, you may also select Upload Member List and upload a CSV file of UArizona NetIDs.</p> <p>The process of adding new members may take a few seconds to complete. Once the changes have taken place, you will see the user's NetID in your group.</p>"},{"location":"registration_and_access/group_management/#creating-a-new-group","title":"Creating a New Group","text":"<p>A new group can be created at any time through the user portal. New groups will share their time and storage allocations with your primary group. Alternate research groups can be a good solution for managing file permissions. For example, if a particular directory and its contents needs restricted access, you could do this by creating a new research group, adding the group members who need access to those files, and then changing the group ownership of the files/directories.</p> <p>To create a new group, log into the user portal, navigate to the Groups tab and select the Add New Group dropdown menu. In the Group Name field, enter the name of the group you'd like to create. Under Group Type, select researchGroup, then select the  to confirm.</p> <p></p> <p>Once your group has been created, you will see it when running <code>va</code> (short for View Allocation) in the same block as your primary group:</p> <pre><code>(puma) [faculty-netid@junonia ~]$ va\nPI: parent_1206 Total time: 7000:00:00\n    Group: faculty-netid Time used: 0:00:00 Time encumbered: 0:00:00\n    Group: your-new-group Time used: 0:00:00 Time encumbered: 0:00:00\n    Total used: 0:00:00\n    Total encumbered: 0:00:00\n    Total remaining: 7000:00:00\n</code></pre>"},{"location":"registration_and_access/group_management/#class-groups","title":"Class Groups","text":"<p>Tip</p> <p>If you are interested in having an HPC staff member come to your class to do an Intro to HPC presentation, reach out to our consultants.</p> <p>If you are a faculty member and are teaching a course that makes use of HPC resources, you can create a class group that will grant your students system access. Class groups are designed to be created and used for one semester only.</p>"},{"location":"registration_and_access/group_management/#class-group-restrictions","title":"Class Group Restrictions","text":"<p>Due to Arizona sales tax restrictions class groups are restricted to the Ocelote cluster and cannot use Puma or El Gato. To submit standard jobs on Ocelote, students will use the class group's name for the <code>--account</code> Slurm directive. For example:</p> <p><pre><code>#SBATCH --account=hpc101\n#SBATCH --partition=standard\n</code></pre> Class group members may also use the Windfall partition on Ocelote. </p>"},{"location":"registration_and_access/group_management/#creating-a-class-group","title":"Creating a Class Group","text":"<p>Log into your user portal, navigate to the Groups tab, and select the Add New Group dropdown option at the top of the page.There will be an option to specify your Group Type. Choose classGroup from the dropdown menu. Then under Group Name enter the name of your group. Finally, click  to complete the process. </p> <p></p> <p>Once this process is complete, you can find your group's dropdown under the Groups tab. There you can add students either individually or in batch by uploading a CSV file with your student's NetIDs. You may also remove students from the group by clicking the \"REMOVE \" button. You can also delete the group itself by selecting \" REMOVE GROUP\".</p> <p></p>"},{"location":"registration_and_access/group_management/#file-permissions-and-storage","title":"File Permissions and Storage","text":"<p>Students in your class group will only be able to access files and directories owned by the class group. This means they will not be able to access files and directories owned by your standard research group. </p>"},{"location":"registration_and_access/group_management/#running-jobs-and-allocations","title":"Running Jobs and Allocations","text":"<p>Due to Arizona sales tax restrictions class groups may only use the Ocelote cluster. To submit standard jobs on Ocelote, students will use the class group's name for the <code>--account</code> Slurm directive. For example:</p> <p><pre><code>#SBATCH --account=hpc101\n#SBATCH --partition=standard\n</code></pre> Standard hours used on Ocelote are pulled from the same pool as your research group so make sure to plan accordingly. If a student runs the command <code>va</code>, they will see the class group as being nested under the total time allocated to your primary research group as well as any others you may have created. Students will not see the names of your other research groups if they run <code>va</code> unless they are members. </p> <pre><code>(ocelote) [faculty_netid@wentletrap ~]$ va\nWindfall: Unlimited\n\nPI: parent_000 Total time: 100000:00:00\n    Group: hpc101 Time used: 0:00:00 Time encumbered: 0:00:00\n    Group: faculty_netid Time used: 0:00:00 Time encumbered: 0:00:00\n    Total used: 0:00:00\n    Total encumbered: 0:00:00\n    Total remaining: 100000:00:00\n</code></pre>"},{"location":"registration_and_access/group_management/#delegating-group-management-rights","title":"Delegating Group Management Rights","text":""},{"location":"registration_and_access/group_management/#adding-a-delegate","title":"Adding a Delegate","text":"<p>PI's can delegate management rights to trusted group members. Delegates may create research and class groups, sponsor users, remove users, and request and manage storage offerings on behalf of their faculty sponsor. To add a group member as a delegate, the PI can click the Manage Delegates link on the home page of the user portal.</p> <p></p> <p>In the Manage Delegates window that appears, select Add a delegate, enter your group member's NetID, and click .</p> <p></p>"},{"location":"registration_and_access/group_management/#instructions-for-delegates","title":"Instructions for Delegates","text":"<p>Once a group member has been added as a delegate, they can log into the user portal, then select Switch User from the Home tab.</p> <p></p> <p>In the next window, they can enter their PI's NetID under Switch user form --&gt; UA NetID, and click . They should see Current effective user change from their own NetID to their PI's NetID. This will allow them to perform functions on their PI's behalf. They may switch back to their own account at any time by following the same process, entering their own NetID in the form instead of their PI's.</p> <p></p>"},{"location":"registration_and_access/system_access/","title":"System Access","text":""},{"location":"registration_and_access/system_access/#system-access","title":"System Access","text":""},{"location":"registration_and_access/system_access/#overview","title":"Overview","text":"<p>Logging into the HPC supercomputers starts with your UArizona NetID and password with two-factor authentication enabled. This section is intended to provide you with instructions on getting terminal access to the system from your specific OS, how to log into the system from our web interface (Open OnDemand), how to set up X11 (image) forwarding, and how to configure your account to allow for a password-less login with SSH keys.</p> <p>If you experience any problems, refer to our FAQ page which provides some solutions to common problems.</p>"},{"location":"registration_and_access/system_access/#web-access","title":"Web Access","text":"<p>Open OnDemand</p> Browser TerminalVirtual Desktop <p>Users can gain command line access to HPC through our OOD web interface as an alternative to using a local SSH Client. To use this interface:</p> <ol> <li>Log into Open OnDemand</li> <li>Go to the dropdown menu at the top of the screen and select <code>Clusters</code></li> <li> <p>Click <code>&gt;_Shell Access</code></p> <p></p> </li> <li> <p>This will put you on the command line on one of the login nodes where you may perform regular housekeeping work, submit jobs, or request an interactive session. By default, you will automatically be connected to Puma. To navigate to a different cluster, use the displayed shortcuts. </p> </li> </ol> <p>Users may also interact with a cluster using a virtual desktop interface. To do this:</p> <ol> <li> <p>Log into Open OnDemand and, under My Interactive Sessions, select Interactive Desktop under Desktops on the left-hand side of the page.</p> </li> <li> <p>A form will appear where you will select the target cluster, enter the amount of time you'd like to be allotted (in hours), the number of cores you need, your PI Group (if you are unsure what your group name is, you can check in https://portal.hpc.arizona.edu/portal/), and the queue. Once you've filled in your request, click Launch.</p> <p></p> </li> <li> <p>A window will appear with the status of your request. It will start in a Pending state and will switch to Running when your desktop session is ready. Click Launch Interactive Desktop to access your session.</p> <p></p> </li> <li> <p>That's it! You can now use the cluster with a Desktop interface</p> <p></p> </li> </ol>"},{"location":"registration_and_access/system_access/#command-line-access","title":"Command Line Access","text":"Tip <ul> <li>Credentials: To log into HPC, you will need NetID+ enabled, an HPC account, and internet access. Because we require Duo-authentication to access the system, no VPN is required. </li> <li>Password Visibility: When entering your password in the terminal at the prompt, you will not see any characters appear on the screen while typing during this step. This is normal and everything is working as it should.</li> </ul> Linux/MacWindows Tip <p>Mac systems provide a built-in SSH client, so there is no need to install any additional software. You will find the terminal application under Applications \u2192 Utilities \u2192 Terminal.</p> <p>Open the terminal and enter: <pre><code>ssh &lt;netid&gt;@hpc.arizona.edu\n</code></pre> where <code>&lt;netid&gt;</code> is your UArizona NetID. When you press enter, you will be prompted for your university password. After successfully entering your password, you will be prompted to Duo Authenticate. If everything is successful, you will be connected to the bastion host.</p> <p>Windows systems do not have any built-in support for using SSH, so you will have to download a software package to do so. There are several available for Windows workstations.  Free SSH clients are available for download from the University of Arizona's Site License website.  </p> PuTTYMobaXterm <p>PuTTY is the most popular open source SSH Windows client. To use it: download, install, and open the Putty client. Next, open a connection and enter <code>hpc.arizona.edu</code> under Host Name and press Open</p> <p></p> <p>This will open a terminal. At the prompt, enter the following, replacing <code>&lt;netid&gt;</code> with your own NetID:</p> <pre><code>Login as: &lt;netid&gt;\n</code></pre> <p>You will then be prompted to Duo-Authenticate. If the process is successful, you will be connected to the bastion host.</p> <p>MobaXterm is another available SSH Windows client. To connect to HPC, download and install MobaXterm, open the software, select Session </p> <p></p> <p>From there, select SSH and enter <code>hpc.arizona.edu</code> under Remote host. Next, select the box next to Specify username and enter your UArizona NetID. To connect, click OK at the bottom of the screen:</p> <p></p> <p>This will open a terminal and will prompt you for your UArizona password. You will then need to Duo-authenticate. If everything is successful, you will be connected to the bastion host.</p> <p>Once you reach the bastion host, regardless of method, you should see the following: <pre><code>Success. Logging you in...\nLast login:\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n</code></pre> From there, type <code>shell</code> to connect to the login nodes that will provide access to our three clusters. On the login nodes, you should see: <pre><code>***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nPuma:\n$ puma\n(puma) $\nOcelote:\n$ ocelote\n(ocelote) $\nElGato:\n$ elgato\n(elgato) $\n-----------------------------------------\n</code></pre></p>"},{"location":"registration_and_access/system_access/#x11-forwarding","title":"X11 Forwarding","text":"<p>X11 forwarding is a mechanism that allows a user to start up a remote application (e.g. VisIt or Matlab) and forward the application display to their local machine. The key to make forwarding work successfully is to include the <code>-X</code> flag at each login step. To check whether X11 forwarding is active, you may run the command:</p> <p><pre><code>echo $DISPLAY\n</code></pre> If it comes back blank, X11 forwarding is not enabled.</p> Mac/LinuxWindows Tips <ul> <li> <p>Mac users will want to install the additional software package XQuartz onto their machines to use X11 forwarding with HPC. </p> </li> <li> <p>On a Mac, if you get a blank response to <code>echo $DISPLAY</code>, you might need this line in your <code>~/.ssh/config</code> file: <code>ForwardX11Trusted yes</code></p> </li> <li> <p>Be aware forwarding X traffic does not work with the DEPRECATED menu interface enabled.  You should disable the menu option and use the hostname shortcuts instead.</p> </li> </ul> <p>Start a terminal session and connect as you typically would with an additional flag <code>-X</code> in your ssh command. Once you're connected to the bastion host, enter the name of the cluster you want to access, including the additional <code>-X</code> flag again. An example of this process is provided below: <pre><code>$ ssh -X netid@hpc.arizona.edu\nPassword:\nDuo two-factor login for netid\nEnter a passcode or select one of the following options:\n\n1. Duo Push to XXX-XXX-8969\n2. Phone call to XXX-XXX-8969\n3. Phone call to XXX-XXX-0502\n4. SMS passcodes to XXX-XXX-8969\n\nPasscode or option (1-4): 1\nSuccess. Logging you in...\nLast login:\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n-----------------------------------------            \n[netid@gatekeeper ~]$ echo $DISPLAY\nlocalhost:13.0\n\n[netid@gatekeeper ~]$ shell -X\n***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nOcelote:\n$ ocelote\n(ocelote) $\nPuma:\n$ puma\n(puma) $\n\n(puma)[netid@junonia ~]$ echo $DISPLAY\nlocalhost:18.0\n</code></pre></p> <p>To use X11 forwarding on a Windows system, you will need to download an X11 display server such as Xming. </p> PuTTYMobaXterm <p>To enable X11 forwarding in PuTTY, go to SSH \u2192 X11 and select the box next to Enable X11 forwarding.</p> <p></p> <p>Once you've connected to the bastion host, connect to the login nodes with the an additional flag <code>-X</code>:</p> <pre><code>shell -X\n</code></pre> <p>To enable X11 forwarding in MobaXterm, open a new session, select SSH, and open Advanced SSH settings. Select the option below called X11-Forwarding.</p> <p></p> <p>Once you've connected to the bastion host, connect to the login nodes with the an additional flag <code>-X</code>: <pre><code>shell -X\n</code></pre></p> <p>Once you're connected to the login nodes, you'll need to include an X11 forwarding flag when you start an interactive session. When using the <code>interactive</code> command, use the flag <code>-x</code>. When using <code>salloc</code> directly, use <code>--x11</code>.</p>"},{"location":"registration_and_access/system_access/#port-forwarding","title":"Port Forwarding","text":"<p>Port forwarding is a technique used to redirect network traffic from one network address and port number to another. In the context of HPC systems, port forwarding allows users to access remote resources or services that are not directly accessible due to network configurations.</p> <p>Common use cases for port forwarding include accessing remote desktops, running graphical applications, or accessing web-based interfaces of applications. Some examples of this might include using Jupyter Lab or a containerized RStudio instance. </p> <p>The steps to set up port forwarding are the following:</p> <ol> <li> <p>Start a job</p> <p>This can either be done using an interactive session or an Open OnDemand job (e.g. an interactive desktop session). Once your job starts, make note of the node name. For example, in an interactive session, you can use the command <code>hostname</code></p> <pre><code>(elgato) [user@wentletrap ~]$ interactive -a hpcteam -t 5:00:00\n[user@cpu37 ~]$ hostname\ncpu37.elgato.hpc.arizona.edu\n</code></pre> </li> <li> <p>Connect to the HPC VPN</p> <p>The HPC VPN can be used with Cisco AnyConnect using <code>vpn.hpc.arizona.edu</code>. This is different from the standard university VPN and will allow you to connect directly to a compute node, bypassing the bastion and login nodes. </p> <p>Use the HPC VPN</p> <p>Note that it's always safer and more efficient to connect directly to your compute node rather than tunneling through the bastion and login nodes. If you tunnel through the bastion/login nodes, you may inadvertently use the same port as another user causing unwanted interference. Additionally, tunneling will result in reduced performance.</p> </li> <li> <p>SSH to your compute node</p> <p>Once you're connected to the HPC VPN, ssh into your compute node with the additional arguments <code>-L &lt;port&gt;:localhost:&lt;port&gt;</code> where <code>&lt;port&gt;</code> is the port you want to use. For example:</p> <p><pre><code>ssh -L 1234:localhost:1234 user@cpu37.elgato.hpc.arizona.edu\n</code></pre> 4. Start your application and launch it in a web browser</p> <p>In your new terminal window, start your application, specifying the port number. For example:</p> <pre><code>[user@cpu37 ~]$ jupyter lab --port 1234\n</code></pre> <p>Your application should give you a URL that you can then use to access your session in a local web browser. </p> <p></p> </li> </ol>"},{"location":"registration_and_access/system_access/#ssh-keys","title":"SSH Keys","text":""},{"location":"registration_and_access/system_access/#why-use-ssh-keys","title":"Why Use SSH Keys?","text":"<p>The Bastion Host uses two-factor authentication and will, by default, prompt you for a password and 2nd factor when you attempt to log in. As an alternative, you can use PKI (Public Key Authentication). This means you will not have to provide a password or Duo-authenticate for any future sessions. In brief, you will need to create an SSH Key on your local workstation and copy the public key to the <code>~/.ssh/authorized_keys</code> file in your HPC account on the bastion host. More detailed explanation &amp; instructions below.</p>"},{"location":"registration_and_access/system_access/#setting-up-ssh-keys-on-linuxmac","title":"Setting Up SSH Keys On Linux/Mac","text":"<p>The proper use of SSH keys involves creating a public/private keypair, and configuring a couple of files on each system. Both Source (e.g. your laptop) and Destination (e.g. the HPC Bastion Host) systems need a directory in your <code>home</code> called <code>.ssh</code>. This is a hidden folder that will store the keypair and configuration files. Note that you will need to enter your password to access the system until this process is complete and all files are set up correctly.</p> <p>Important Note on Account Security</p> <p>Do not store a backup of these keys on any other system! If you lose the keys, you will still be access the HPC using your UA password. If a third party obtains your SSH key, they will gain access to your account.</p> 1. Setting up on Source <p>The following files need to be present in <code>~/.ssh</code> on Source</p> <ul> <li>private key: <code>id_rsa</code> -- Do not share this with anybody! It is analogous to your password; anybody who has this file will gain access to your account.</li> <li>public key: <code>id_rsa.pub</code> -- Upload this onto any servers that you wish to automatically login to. It is recommended to use different keys for different servers.</li> <li>configuration file: <code>config</code></li> </ul> <p>The keypair is generated on Source with the command <code>ssh-keygen -t rsa</code>. You will be prompted to enter a passphrase. This is optional but recommended. </p> <p>You may need to enter a name other than the default <code>id_rsa</code> if you already have a keypair with that name on your system, or if you wish to use mutliple SSH keys to access different servers.</p> 2. Setting up on Destination <p>In this case, we will be treating the Bastion Host as the Destination. This is necessary because it serves as the authentication host, meaning users are required to pass through it before accessing the rest of the HPC environment. Note that the Bastion Host has very limited storage space and a different file system than the main HPC. Do not put any files on the Bastion Host other than what is necessary to set up SSH Keys. Files that you place on the Bastion Host will not be present on the main HPC. You can read more about the system layout here. </p> <p>A file containing a list of accepted public keys called <code>authorized_keys</code> needs to be present in <code>~/.ssh</code> on Destination. You will then need to copy the contents of <code>id_rsa.pub</code> from Source into this file. This can be done with the command <code>ssh-copy-id netid@hpc.arizona.edu</code>. If your computer does not support the this command, or if this process does not yield desired results, you will need to copy it manually:</p> <pre><code>scp ~/.ssh/id_rsa.pub netid@hpc.arizona.edu:\nssh netid@hpc.arizona.edu\nmkdir -p ~/.ssh &amp;&amp; cat ~/id_rsa.pub &gt;&gt; .ssh/authorized_keys &amp;&amp; rm ~/id_rsa.pub \n</code></pre> 3. Configuring the SSH Agent <p>Sometimes the ssh agent does not associate the right key with the right server, and you may still have to enter your password. If this occurs, setting up a config file can correct the error. </p> <p>On Source, run <code>touch ~/.ssh/config</code>. Then, copy the following code block into the new file, making sure to replace <code>&lt;netid&gt;</code> with your correct UA net id. You can change the contents of <code>Host</code> to any name you like. Do not change <code>HostName</code>. Make sure <code>IdentityFile</code> matches the key you generated in step one, in particular if you gave it a different name.</p> <pre><code>Host uahpcbastion\n    HostName hpc.arizona.edu\n    User &lt;netid&gt;\n    IdentityFile ~/.ssh/id_rsa\n</code></pre> <p>This will associate the identity file with the HPC server, and will also allow you to replace <code>netid@hpc.arizona.edu</code> with <code>uahpcbastion</code> in the command line, e.g. <code>ssh uahpcbastion</code></p> <p>Now, logout and attempt to login to the server again. You should not be prompted for a password!</p> 4. Direct Access to Login Nodes <p>You can set up a proxy jump in order to access the login nodes without having to type \"shell\" from the Bastion Host.</p> <p>First, you will need to copy the contents of <code>id_rsa.pub</code> from Source into the <code>~/authorized_keys</code> file on the main HPC filesystem in a similar manner to step 2.  </p> <p>Then, put an empty line after the last entry in <code>~/.ssh/config</code> and add the following contents, again making sure to replace <code>&lt;netid&gt;</code> with you correct UA net id. You may change the <code>Host</code> entry as you prefer, and make sure the name after <code>ProxyJump</code> matches the name you gave to the Bastion Host in the previous entry. </p> <pre><code>Host uahpclogin\n    HostName shell.hpc.arizona.edu\n    User &lt;netid&gt;\n    IdentityFile ~/.ssh/id_rsa\n    ProxyJump uahpcbastion\n</code></pre> <p>Now you should be able to run <code>ssh uahpclogin</code> from Source to directly access the login node.</p> 5. Accessing the File Transfer Node <p>SSH Keys can also be used to avoid entering a password and two-factor authentication when transferring files to or from the cluster via <code>filexfer.hpc.arizona.edu</code>.</p> <p>Put an empty line after the last entry in <code>~/.ssh/config</code> on Source and add the following contents, again making sure to replace <code>&lt;netid&gt;</code> with you correct UA net id.</p> <pre><code>Host uahpcfxfr\n    HostName filexfer.hpc.arizona.edu\n    User &lt;netid&gt;\n    IdentityFile ~/.ssh/id_rsa\n</code></pre> <p>You should now be able to use <code>scp</code>, <code>sftp</code>, and the like from your local computer without entering your password. </p> <p>You may also wish to access the file transfer node from the login node without entering your password, for example to copy data from <code>/rental</code>. In this case, you will need to perform steps 1-3 but treating Source as the login node and Destination as the file transfer node. It may be helpful to name this new key something to indicate its association with the file transfer node, for example <code>fxfr</code> and <code>fxfr.pup</code>. </p> <p>Since both Source and Destination share access to your home folder, your public and private keys will both be in the <code>~/.ssh</code> folder on the HPC, as will the <code>authorized_keys</code> and <code>config</code> files. Make sure to create these and put the contents of <code>fxfr.pub</code> into <code>authorized_keys</code>. If multiple keys are being added to <code>authorized_keys</code>, make sure they are on separate lines.</p> <p>Then, add the following code block to <code>~/.ssh/config</code>:</p> <pre><code>Host uahpcfxfr\n    HostName filexfer.hpc.arizona.edu\n    User &lt;netid&gt;\n    IdentityFile ~/.ssh/fxfr\n</code></pre> <p>Now, you should be able to perform <code>ssh</code>, <code>scp</code>, <code>sftp</code>, and the like from the HPC login node to/from the HPC file transfer node without having to enter your password. For example:</p> <pre><code>ssh uahpcfxfr\nscp -r /rental/netid/data /xdisk/netid/project\n</code></pre>"},{"location":"registration_and_access/system_access/#setting-up-ssh-keys-on-windows","title":"Setting Up SSH Keys On Windows","text":"<p>To set up SSH keys on Windows with the PuTTy client, refer to the official PuTTy documentation.</p> <p>To set up SSH keys on Windows for file transfers using WinSCP, refer to the official WinSCP documentation.</p> <p>If you are a Windows user and would like to set up SSH keys to access the file transfer node from a login node without entering your password, please read through the above section on setting up SSH Keys on Linux, since the HPC is a Linux system. Some information in steps 1-4 may be relevant, but you should not perform those actions on your local computer. Then, refer to step 5 for specific directions on setting this up. Do so from an active SSH session on an HPC login node.</p>"},{"location":"registration_and_access/system_access/#learn-more","title":"Learn More","text":"<p>If you would like to learn more about SSH keys and more, please refer to this in-depth guide created by our friends at Digital Ocean.</p>"},{"location":"registration_and_access/system_overview/","title":"System Layout","text":"<p>The HPC system has a structure of interconnected nodes and storage systems. It is important to be aware of this structure while using the HPC to help with navigation and ensure proper usage of each type of node. To see more about the compute resources available, please see our compute resources page. </p> <p>Below is a graphic depiction of the layout of the HPC nodes:</p> <p></p> Do not run computations on the login nodes. <p>See Running Jobs for detailed instructions on the proper way to run computationally intensive tasks. </p> <p>Bastion Host</p> <p>This is the first node that users access when using an SSH connection. Open OnDemand users do not directly access this node. </p> <p>The Bastion Host, also called \"gatekeeper\", is a security feature, and only serves to provide access to the main system. As you can see in the diagram above, it does not connect to any part of the HPC other than the login nodes. </p> <p>There is a very limited amount of storage on the Bastion Host, and you will not be able to access any of your normal files from here. After SSH-ing to hpc.arizona.edu, you will see this message:</p> <pre><code>[user@local-machine ~]$ ssh user@hpc.arizona.edu\nLast login: Thu Apr  4 15:14:47 2024 from ip72-201-152-35.ph.ph.cox.net\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n-----------------------------------------\n\n[user@gatekeeper ~]$\n</code></pre> <p>Please type <code>shell</code> to jump onto a login node, where you can access your file, and the compute/data-transfer nodes. SSH Keys can be used to set up password-free access.</p> <p>Login Nodes</p> <p>After accessing a login node, you will see this message:</p> <pre><code>Last login: Mon Apr  8 09:40:58 2024 from junonia.hpc.arizona.edu\n***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nPuma:\n$ puma\n(puma) $\nOcelote:\n$ ocelote\n(ocelote) $\nElGato:\n$ elgato\n(elgato) $\n-----------------------------------------\n\n(puma) [user@wentletrap ~]$\n</code></pre> <p>There are two login nodes: <code>wentletrap</code> and <code>junonia</code>. If you need to switch between them, you can type <code>ssh &lt;node-name&gt;</code>. </p> <p>The login nodes are for managing files and launching jobs. </p> <p>No Jobs on Login Nodes</p> <p>Do not run computationally intensive work, including running scripts or compiling software, on the login nodes. </p> <p>Compute Nodes</p> <p>These are the workhorses of HPC. They are designed to handle large computational loads, and time on them is managed by our job scheduler, Slurm. </p> <p>To access the compute nodes, please refer to our section on running jobs. To view technical specs on our compute nodes, including quantity, see our resources page.</p> <p>Data Transfer Nodes</p> <p>These are used to facilitate data transfers to and from the HPC filesystem. Copying files from a local machine to the HPC should use syntax like <code>rsync localfiles user@filexfer.hpc.arizona.edu</code> to transfer files onto the HPC, or in the reverse order to transfer out of HPC. The crucial element is the <code>filexfer</code> in the HPC domain name. </p> <p>They are also used to access your <code>/rental</code> share if you have one. From a login node, you can run <code>ssh filexfer.hpc.arizona.edu</code> to access this. Once you have done so, you can access your rental storage by typing <code>cd /rental/&lt;pi-netid&gt;</code>. If you are not on a DTN, you will not be able to see this share. This means data will have to be copied over to the main HPC filesystem in order to be accessible to compute nodes. </p> <p>Note that only preinstalled system executables are able to be run from the DTNs. If you have custom software to grab data from somewhere, it may not work from here.</p>"},{"location":"registration_and_access/vpn/","title":"VPN","text":""},{"location":"registration_and_access/vpn/#overview","title":"Overview","text":"<p>A virtual private network (VPN) is a mechanism for creating a secure connection between a computer and a computing network using an insecure communication medium like the Internet. You can access the resources available within the network from your computer using a VPN.</p> <p>You will find the following VPN services useful for accessing some of the resources offered by the Research and Discovery Technologies:</p> <ul> <li>UArizona SSL VPN (<code>vpn.arizona.edu</code>): If you are not connected to the UArizona campus network you will need to connect to this VPN to access R-DAS.</li> <li>UArizona HPC VPN (<code>vpn.hpc.arizona.edu</code>): You will need to connect to this VPN to use graphical applications that need X11 forwarding with the HPC clusters.</li> </ul>"},{"location":"registration_and_access/vpn/#instructions-for-connecting","title":"Instructions for Connecting","text":"GUICLI <p>You can connect to the UArizona VPN services with the software Cisco Secure Client. It is available for Windows, Mac, and Linux distributions. On Linux distributions you might have a better experience with OpenConnect VPN (see CLI). Follow the UITS Knowledge Base guide for Windows, Mac, or Linux, to install Cisco Secure Client on your computer. The guide also shows how you can connect to the UArizona SSL VPN.</p> <p>Follow the steps below to connect to UArizona HPC VPN (the screenshots are from a Mac, but the experience is similar across OSs):</p> <ol> <li>Open Cisco Secure Client</li> <li>Enter <code>vpn.hpc.arizona.edu</code> in the address bar and click Connect.</li> <li>In the window that launches, enter your UArizona NetID as your Username and click OK.</li> <li>In the next window, enter your UArizona NetID password and click OK.</li> <li>In the window that launches, enter the NetID+ method you selected when you enrolled.</li> <li>Lastly, review the notice box and click Accept.</li> </ol> <p>Use of sudo</p> <p>Do not run any <code>sudo</code> commands on the HPC clusters when following the instructions below. These are strictly meant for your personal machines.</p> <p>You can connect to UArizona VPN services from the command line with OpenConnect VPN. To do this, you will need <code>sudo</code> privileges.</p> <p>OpenConnect VPN is available for Windows, Mac and Linux distributions, however installation on Windows can be difficult. On Windows, you might have a better experience with Cisco Secure Client (see GUI). You can find more information on platforms supported by OpenConnect from the project website. Select your operating system from the list below to view installation instructions:</p> MacLinux <p>Install with the Homebrew package manager: <code>brew install openconnect</code></p> <p>Follow the instructions from Open Build Service for your distribution.</p> <p>Once you have OpenConnect installed, you can connect to UArizona VPNs using the following:</p> UA SSL VPNUA HPC VPN <ol> <li>Open your terminal</li> <li>Enter <code>sudo openconnect vpn.arizona.edu</code></li> <li>A prompt will appear asking you to choose a VPN <code>GROUP</code>. Enter <code>1</code>.</li> <li>A prompt will appear asking you for your Username. Enter your UArizona NetID.</li> <li>A prompt will appear asking for your Password. Enter your UArizona NetID password. </li> <li>A second prompt will appear asking for your Password. Enter the NetID+ method you selected when you enrolled. <ol> <li>If you selected the Push method, then enter <code>push</code>.</li> <li>If you selected the SMS method, enter <code>sms</code>. If you do this, it will show that the login has failed and will ask you to reenter your Username, Password, and NetID+ method. For Username and Password do the same as before. For NetID+ method, enter the SMS passcode you received.</li> <li>If you selected the Passcode method, then enter your passcode. </li> </ol> </li> </ol> <ol> <li>Open your terminal</li> <li>Enter <code>sudo openconnect vpn.hpc.arizona.edu</code></li> <li>A prompt will appear asking you for your Username. Enter your UArizona NetID.</li> <li>A prompt will appear asking for your Password. Enter your UArizona NetID password. </li> <li>A second prompt will appear asking for your Password. Enter the NetID+ method you selected when you enrolled. <ol> <li>If you selected the Push method, then enter <code>push</code>.</li> <li>If you selected the SMS method, enter <code>sms</code>. If you do this, the prompt will appear again. Enter the SMS passcode that you received. </li> <li>If you selected the Passcode method, then enter your passcode.         </li> </ol> </li> </ol>"},{"location":"resources/allocations/","title":"Time Allocations","text":"Do not run computations on the login nodes. <p>CPU time allocations do not apply to login nodes. See Running Jobs for detailed instructions on the proper way to run computationally intensive tasks. </p>"},{"location":"resources/allocations/#group-allocations","title":"Group Allocations","text":"<p>All University of Arizona Principal Investigators (PIs; typically faculty) that register for access to UArizona High Performance Computing (HPC) services receive free standard allocations on the HPC clusters which are shared among all members of their team and refreshed on a monthly basis. All PIs receive a standard allocation in addition to the windfall partition. A breakdown of the allocations available on the system and their usage is shown below. </p> <p>High Priority</p> <p>Please note that the High Priority partition is only available to PI groups who participated in the buy-in process for Puma. PIs will be notified when another buy-in session is available</p> <p>Qualified Hours</p> <p>Qualified Hours are only available to groups which have been awarded a special project. See Policies for information on how to apply. </p> StandardWindfallHigh PriorityQualified <p>Every group receives a free allocation of standard hours that refreshes on the first day of each month. </p> Puma Ocelote El Gato Standard CPU Hours 150,000 100,000 7,000 <p>In batch jobs, standard hours can be used to request resources on CPU-only nodes with the directives</p> <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=standard\n</code></pre> <p>To request GPU resources using standard hours: <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=gpu_standard\n#SBATCH --gres=gpu:&lt;options&gt;\n</code></pre></p> <p>Windfall is a partition available to jobs that enables them to run without consuming your allocation, but it also reduces their priority. This means windfall jobs are slower to start than other partitions. In addition to lower priority, windfall jobs are preemptible, meaning standard and high-priority jobs can interrupt a running windfall job, effectively placing it back in the queue. The purpose of windfall is to ensure that the clusters are busy at all times, and to allow researchers additional compute while increasing the efficiency of the system.</p> <p>The <code>--account</code> flag should be omitted when using the Windfall partition.</p> <p>In batch jobs, the windfall partition can be used to request resources on CPU-only nodes with the directive:</p> <pre><code>#SBATCH --partition=windfall\n</code></pre> <p>To request GPU resources: <pre><code>#SBATCH --partition=gpu_windfall \n#SBATCH --gres=gpu:&lt;options&gt;\n</code></pre></p> <p>High priority allocations provide access to an additional pool of purchased compute nodes and increase the priority of jobs such that they start faster than standard jobs. Please check with your PI to ensure that your group has access before including these directives in your jobs.</p> <p>In batch jobs, high priority hours can be used to request resources on CPU-only nodes with the directives:</p> <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=high_priority\n#SBATCH --qos=user_qos_&lt;PI GROUP&gt;\n</code></pre> <p>To request GPU resources with high priority hours:</p> <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=gpu_high_priority\n#SBATCH --qos=user_qos_&lt;PI GROUP&gt;\n#SBATCH --gres=gpu:&lt;options&gt;\n</code></pre> <p>Groups with an upcoming deadline (e.g., conference, paper submission, graduation) are eligible to apply for a Special Project allocation once per year. Special projects provide an additional pool of standard hours, known as \"qualified hours\" to the group for a limited amount of time. </p> <p>In batch jobs, qualified hours can be used to request resources on CPU-only nodes with the directive: <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=standard\n#SBATCH --qos=qual_qos_&lt;PI GROUP&gt;\n</code></pre> To request GPU resources with qualified hours: <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=gpu_standard\n#SBATCH --qos=qual_qos_&lt;PI GROUP&gt;\n#SBATCH --gres=gpu:&lt;options&gt;\n</code></pre></p> <p>For more information on batch directives and <code>--gres</code> options, see our batch directives documentation.</p> <p>See: interactive jobs, batch jobs, or Open OnDemand for more information on the specific syntax for using hours in different jobs.</p>"},{"location":"resources/allocations/#how-allocations-are-charged","title":"How Allocations are Charged","text":"<p>The number of CPU hours a job consumes is determined by the number of CPUs it is allocated multiplied by its requested walltime. When a job is submitted, the CPU hours it requires are automatically deducted from the account. If the job ends early, the unused hours are automatically refunded.</p> <p>For example, a job requesting 50 CPUs for 10 hours will be charged 500 CPU hours. When the job is submitted, all 500 CPU hours are deducted from the user's account. However, if the job only runs for 8 hours before completing, the unused 100 CPU hours would be refunded.</p> <pre><code>graph LR\n  A[Request 50 CPUs&lt;br&gt;for 10 hours] --&gt; B[500 CPU hours&lt;br&gt;charged];\n  B --&gt; C[Job starts];\n  C --&gt; D[Job completes&lt;br&gt;after 8 hours];\n  D --&gt; E[100 CPU hours&lt;br&gt;refunded];</code></pre> <p>This accounting is the same regardless of which type of node you request. Standard, GPU, and high memory nodes are all charged using the same model and use the same allocation pool. </p> <p>Charging discrepancies</p> <p>If you find you are being charged for more CPUs that you are specifying in your submission script, it may be an issue with your job's memory request.</p> <p>Allocations are refreshed on the first day of each month. Unused hours from the previous month do not roll over.</p>"},{"location":"resources/allocations/#how-to-find-your-remaining-allocation","title":"How to Find Your Remaining Allocation","text":"<p>To view your allocation's used, unused, and encumbered hours, use the command <code>va</code> (short for \"view allocation\") in a terminal. For example: <pre><code>(elgato) [user@gpu5 ~]$ va\nWindfall: Unlimited\n\nPI: parent_974 Total time: 7000:00:00\n    Total used*: 1306:39:00\n    Total encumbered: 92:49:00\n    Total remaining: 5600:32:00\n    Group: group1 Time used: 862:08:00 Time encumbered: 92:49:00\n    Group: group2 Time used: 0:00:00 Time encumbered: 0:00:00\n\n*Usage includes all subgroups, some of which may not be displayed here\n</code></pre></p> <p>Note that if your PI has created multiple groups, each of these groups consumes CPU hours from the same allocation. You can see the total allocation pool and the usage for each group you are a member of in the output of <code>va</code>. </p>"},{"location":"resources/compute_resources/","title":"Compute Resources","text":"Do not run computations on the login nodes. <p>The compute resources described on this page are designed for handling computationally intensive tasks. See Running Jobs for detailed instructions. </p>"},{"location":"resources/compute_resources/#compute-resources-available-by-cluster","title":"Compute Resources Available by Cluster","text":"<p>Below is a list of the node types and physical hardware that are available on each of our three clusters. These can be used as a reference when submitting jobs to the system to ensure you are targeting the correct machines and getting the computational resources you need.</p> <p>Requesting resources in jobs</p> <p>For information on the specifics of requesting the compute resources detailed below, see our Batch Jobs, Interactive Jobs, and/or Open OnDemand Jobs guides. </p> <p>Node Types</p> Node Type Description Standard CPU Node This is the general purpose node, designed to be used by the majority of jobs. High Memory CPU Node Similar to the standard nodes, but with significantly more RAM. There a only a few of them and they should only be requested for jobs that are known to require more RAM than is provided by standard CPU nodes. GPU Node Similar to the standard node, but with one or more GPUs available. The number of GPUs available per node is cluster-dependent. Buy-in Node Nodes that have been purchased by research groups as part of our buy-in process. Buy-in nodes are only accessible to high priority and windfall jobs. <p>Available Hardware by Cluster and Node Type</p> <p>CPUs and Memory</p> <p>For information on memory to CPU ratios, shown as RAM/CPU in the tables below, see CPUs and Memory</p> PumaOceloteEl Gato <p>Resources Available</p> Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node GPUs/Node RAM/GPU GPU RAM/Node Total GPUs Standard 192 standard108 buy-in 94 5 GB 470 GB - - - - High Memory 3 standard2 buy-in 94 32 GB 3008 GB - - - - GPU 9 standard6 buy-in 94 5 GB 470 GB 4 32 GB (v100s)20 GB (MIGs) 128 GB 36 standard24 buy-in Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node GPUs/Node RAM/GPU GPU RAM/Node Total GPUs Standard 360 28 6 GB 168 GB - - - - High Memory 1 48 41 GB 1968 GB - - - - Single GPU Nodes 25 28 8 GB 224 GB 1 16 GB 16 GB 25 Dual GPU Nodes 35 28 8 GB 224 GB 2 16 GB 32 GB 70 Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node Standard 118 16 4 GB 64 GB"},{"location":"resources/compute_resources/#gpu-nodes","title":"GPU Nodes","text":"PumaOceloteEl Gato <p> Puma has a different arrangement for GPU nodes than Ocelote. Whereas Ocelote has one GPU per node, Puma has four. This has a financial advantage for providing GPU's with lower overall cost, and a technical advantage of allowing jobs that can use multiple GPU's to run faster than spanning multiple nodes.</p> <p>Puma's GPU nodes have four Nvidia V100S model GPUs. They are provisioned with 32 GB memory compared to 16 GB on the P100's.  </p> <p>In addition to the V100 nodes, one node has four A100s, each subdivided into three smaller virtual GPUs. See the MIG (Multi-instance GPU) Resources section below for details. </p> <p>Multi-Instance GPU (MIG) Resources </p> <p>MIG resources are only available on Puma</p> <p>The Four A100 GPUs on Puma Node r5u13n1 are each subdivided into three smaller virtual GPUs using the Nvidia MIG (Multi-Instance GPU) method.  Each of these MIG slices allows the use of 20 GB of GPU memory. The vast majority of jobs run on Puma in 2023 used less than this amount of GPU memory. The 12 MIG GPUs increase overall GPU availability on Puma by freeing the 32 GB V100 GPUs for users requiring larger amounts of GPU memory.</p> <p>Jobs requesting MIG resources will ideally be scheduled more quickly than those requesting the standard V100 GPUs, so MIG resources should be preferred when sufficient.</p> <p>A limitation is that only one MIG slice can be addressed by a single application, so MIG slices are not appropriate for jobs utilizing multiple GPUs.</p> <p>The addition of the MIG devices to the Slurm queues will have a number of impacts, and some users may need to make changes to submissions to ensure proper functioning of analyses. </p> <p></p> <p>Ocelote has 25 compute nodes with one Nvidia P100 and 35 compute nodes with two Nvidia P100 GPUs that are available to researchers on campus. Research groups are limited to using a maximum of 10 GPUs simultaneously. </p> <p>Previously, one node with a V100 was available, but it has since been replaced with a P100. </p> <p>El Gato has no GPU nodes. During the quarterly maintenance cycle on April 27, 2022 the El Gato K20s and Ocelote K80s were removed after support was ended by Nvidia.</p>"},{"location":"resources/compute_resources/#system-technical-specifications","title":"System Technical Specifications","text":"El Gato Ocelote Puma Model IBM System X iDataPlex dx360 M4 Lenovo NeXtScale nx360 M5 Penguin Altus XE2242 Year Purchased 2013 2016 (2018 P100 nodes) 2020 Node Count 118 360 CPU-only60 GPU1 High Memory 300 CPU-only15 GPU5 High Memory Total System Memory 23.5 TB 83.3 TB 169.7 TB Processors 2x Xeon E5-2650v2 8-core (Ivy Bridge) 2x Xeon E5-2695v3 14-core (Haswell)2x Xeon E5-2695v4 14-core (Broadwell)4x Xeon E7-4850v2 12-core (Ivy Bridge) 2x AMD EPYC 7642 48-core (Rome) Cores/Node (Schedulable) 16 28 (48 - High-memory node) 94 Total Cores 1888 11724<sup>1</sup> 30720<sup>1</sup> Processor Speed 2.66 GHz 2.3 GHz (2.4GHz - Broadwell CPUs) 2.4 GHz Memory/Node 64 GB 192 GB(2 TB - High-memory node) 512 GB(3 TB - High-memory nodes) Accelerators 60 NVIDIA P100 (16GB) 56 NVIDIA V100S12 A100 20 GB MIG slices /tmp<sup>2</sup> ~840 GB spinning ~840 GB spinning ~1440 TB NVMe HPL Rmax (TFlop/s) 46 382 OS CentOS 7 CentOS 7 Rocky Linux 9 Interconnect FDR Inifinband FDR Infiniband for node-node10 Gb Ethernet node-storage 1x 25 Gb/s Ethernet RDMA (RoCEv2)1x 25 Gb/s Ethernet to storage <ol> <li> <p>Includes high-memory and GPU node CPUs\u00a0\u21a9\u21a9</p> </li> <li> <p>/tmp is scratch space and is part of the root filesystem\u00a0\u21a9</p> </li> </ol>"},{"location":"resources/data_center/","title":"Research Data Center","text":""},{"location":"resources/data_center/#central-computing-facilities","title":"Central Computing Facilities","text":"<p>The University of Arizona (UArizona) has two data center facilities available to assist researchers on campus:</p> <ul> <li> <p>Research Data Center (RDC): 1200 ft<sup>2</sup> raised floor data center designed for water-cooled racks dedicated to centrally managed research computing systems</p> </li> <li> <p>Co-location Data Center:  1900 ft<sup>2</sup> of raised floor data center space for air-cooled research co-located equipment</p> </li> </ul> <p>These campus data centers are managed by the UArizona\u2019s central computing organization, University Information Technology Services (UITS). Other than installation costs no bandwidth or other recurring charges will be levied for co-location of research systems in these facilities.</p>"},{"location":"resources/data_center/#power-and-cooling","title":"Power and Cooling","text":"<p>The UITS data centers are both located in the Computer Center with 1192 kW of battery backup and a 1750 kW generator for backup power.</p> <p>Cooling in the RDC is both in-rack cooling with chilled water heat exchangers and Computer Room Air Conditioning (CRAC) units. The co-location Data Center is cooled with chilled water CRAC units and dual cool CRACs. Both data centers are equipped with 18\u201d raised floors that allow for full coverage of cooling to all the equipment, and leak detection systems in the subfloor.</p>"},{"location":"resources/data_center/#fire-suppression","title":"Fire Suppression","text":"<p>The fire suppression system is a multi-tiered defense with clean agent compressed gas, dry pipe pre-action sprinkler and EPO (Emergency Power Off) systems zoned to deploy in affected areas.  For prevention storage of combustible materials such as cardboard, flammable liquids and other hazardous materials is prohibited within the data centers.</p>"},{"location":"resources/data_center/#security","title":"Security","text":"<p>UITS data centers have badge swipe access with two-factor authentication and video surveillance in data center and surrounding building. The data centers are monitored by a co-located 24/7 Operations and dedicated infrastructure team. With automated environmental and system monitoring to assist with issue triaging and escalation. All personnel with swipe access to the data centers have undergone background checks and are required to be US Citizens.</p>"},{"location":"resources/data_center/#network-and-connectivity","title":"Network and Connectivity","text":"<p>In addition to direct connections to commodity Internet carriers, the UArizona connection to Internet2 is through the Sun Corridor Network \u2013 an Arizona regional network established through a collaborative effort sponsored by the Arizona Board of Regents\u2019 (ABOR) three state universities \u2013 Arizona State University (ASU), Northern Arizona University (NAU), and the University of Arizona (UArizona). The Sun Corridor Network provides advanced networking services beyond those available from the individual Arizona Universities and builds an environment essential to leading-edge education, research, and the sharing of digital communications resources, network services, and applications among eligible members.</p> <p>The UArizona manages and operates the Sun Corridor Network. The current connection from UArizona to Sun Corridor is dual 10 Gb, while Sun Corridor is connected to Internet2 via dual 100 Gb connections in Tucson and Phoenix. Network traffic to Internet2 is automatically routed via the Internet2 infrastructure; no action or configuration by the user is required to take advantage of Internet2 connectivity.</p> <p>The UArizona\u2019s Research Data Center has 40 Gb/s connections to the UArizona core with all the servers connected by 1 Gb/s or 10 Gb/s connections.  In-rack switching is enabled with Cisco FEX switches used in a top of rack configuration in both data centers with servers connected to two different switches for (N + 1) redundancy.</p> <p>In addition to direct connectivity to the campus network at the building level, researchers have an opportunity to use a Science DMZ for fast and high volume data transfers to outside collaborating institutions. The Science DMZ is deployed at the University of Arizona network perimeter, outside border firewalls, and is directly connected to Sun Corridor via 10 Gb link. It is secured via static access lists deployed at the Sun Corridor router without impact to performance. There are two high-performance Data Transfer Nodes (DTNs) deployed in the Science DMZ. DTN\u2019s are dedicated servers with hardware and operating system optimized for high speed transfer. We recommend using these DTNs for large transfers</p>"},{"location":"resources/secure_hpc/","title":"Secure Services","text":"<p>Research Technologies in partnership with the Data Science Institute is providing a secure research enclave that is HIPAA compliant called Soteria. In Greek mythology, Soteria (Greek: \u03a3\u03c9\u03c4\u03b7\u03c1\u03af\u03b1) was the goddess or spirit (daimon) of safety and salvation, deliverance, and preservation from harm.</p> <p>Soteria uses the same Slurm scheduling system and software module interface as our main HPC computing clusters. For general information on HPC usage, the rest of our documentation site can be used. The information below covers everything specific to Soteria. Additional information can also be found at https://soteria.arizona.edu</p>"},{"location":"resources/secure_hpc/#prerequisites-and-registration","title":"Prerequisites and Registration","text":"<p>To gain access, you will need to submit a Soteria request form. Once your form has been reviewed and approved, you will receive an email with the subject UA Soteria Access Request Approved. This email will contain the next steps to take which are detailed below:</p> <ul> <li> <p> Complete Required Training in Edge Learning</p> <p>The CRRSP (regulated research) team will register you for three required trainings listed below </p> <ol> <li>HIPAA Essentials</li> <li>Information Security: Insider Threat Awareness</li> <li>Information Security Awareness Certification</li> </ol> <p>You will receive instructions on accessing these courses in your confirmation email. Courses will also be findable here: https://uaccess.arizona.edu.</p> </li> <li> <p> Assignment to the Soteria VPN</p> <p>Once you have completed your required training, the CRRSP team will notify you via email when you have been assigned access to the Soteria VPN. This VPN is an important part of our HIPAA compliance and differentiates Soteria usage from the standard HPC clusters. Soteria access cannot be established when not connected to the VPN. For VPN access, use: <code>vpn.arizona.edu/soteria</code>.</p> <p></p> </li> <li> <p> Additional Requirements</p> <p>The computer you will use to access Soteria services must meet the following requirements:</p> <ol> <li>The Operating System and applications must be updated with the latest patches.</li> <li>You must have a strong password to log into the computer (at least 8 characters and a mix of character types). </li> <li>This must not be a shared computer with other users.</li> <li>Up to date anti-virus software.</li> </ol> </li> </ul>"},{"location":"resources/secure_hpc/#access","title":"Access","text":"<p>VPN Required</p> <p>You must be connected to the Soteria VPN to access the system.</p>"},{"location":"resources/secure_hpc/#command-line-access","title":"Command Line Access","text":"<p>Soteria command line access is available with SSH using the hostname <code>shell.cougar.hpc.arizona.edu</code> as shown below (replacing <code>&lt;your_netid&gt;</code> with your own NetID):</p> <pre><code>$ ssh &lt;your_netid&gt;@shell.cougar.hpc.arizona.edu\n\nAuthorized uses only. All activity may be monitored and reported.\nLast login: Tue Nov 29 06:18:33 2022 from ans-02.hpc.arizona.edu\nAuthorized uses only. All activity may be monitored and reported.\nnetid@taub:~ $\n</code></pre> <p>Taub is a login node and will provide the same functionality and have the same policies as the other HPC clusters. Modules are available on Soteria's compute nodes but not on the login node. The command <code>interactive</code> is available to request a session on a compute node and jobs may be submitted using the standard <code>sbatch</code>. More details on Slurm commands can be found in Running Jobs.</p>"},{"location":"resources/secure_hpc/#graphical-interface","title":"Graphical Interface","text":"<p>Similar to the other HPC clusters, we offer the service Open OnDemand to provide web browser access to Soteria. This can be used to navigate, view, and edit files as well as gain access to graphical applications.</p> <p>To access the Soteria-specific OOD service, open the following link in your favorite browser: https://ondemand-hipaa.hpc.arizona.edu</p> <p>The applications currently available are  RStudio, Matlab and Python 3.9 (Jupyter). </p>"},{"location":"resources/secure_hpc/#available-compute-resources","title":"Available Compute Resources","text":"<p>This small cluster has four standard compute nodes. Each has 94 cores and 470 GB memory available. The two GPU nodes have the same resources but there are also four V100 GPU's in each. You can use the Running Jobs documentation to learn how to use Slurm with these nodes.</p> Node Type Node Names Standard Nodes r1u26n1,r1u27n1,r1u28n1,r1u29n1 GPU Nodes r1u30n1,r1u32n1"},{"location":"resources/secure_hpc/#compute-time-allocations","title":"Compute Time Allocations","text":"<p>All PI groups will receive a time allocation of 100,000 CPU hours per month. For general information on time allocations and charging, see our Allocations documentation</p>"},{"location":"resources/secure_hpc/#storage","title":"Storage","text":"<p>All users are granted a home directory. Additional communal space can be found in a <code>/groups</code> directory allocated to each PI. Your files can be accessed both on the HPC filexfer nodes(1) as well as when connected to Soteria. A summary of Soteria's storage is listed below: </p> <ol> <li>hostname: <code>filexfer.hpc.arizona.edu</code></li> </ol> Storage Allocation Availability Quota Location on Soteria Location on <code>filexfer</code> Home Allocated to each user 50 GB <code>/home/u&lt;xx&gt;/&lt;your_netid&gt;</code> <code>/hipaa/home/u&lt;xx&gt;/&lt;your_netid&gt;</code> Groups Allocated to each PI group Unlimited <code>/groups/&lt;pi_netid&gt;</code> <code>/hipaa/groups/&lt;pi_netid&gt;</code>"},{"location":"resources/secure_hpc/#transferring-data","title":"Transferring Data","text":"<p>Globus can be used for moving data in and out of the Soteria environment. For more information on using Globus, see our Globus documentation</p> <p>Soteria's endpoint is: UA HPC HIPAA Filesystems</p>"},{"location":"resources/updates/","title":"Updates","text":"<p>The Puma cluster is currently being updated from the outdated CentOS 7 operating system to Rocky Linux 9. This will involve updating both the operating system and much of the installed software.</p> <ul> <li>A small number of nodes were previously made available in a testing environment.</li> <li>At the October 30th maintanance, an initial subset of compute nodes will be available for full production runs.</li> <li>During November and December 2024 more nodes will be updated.</li> <li>In January all nodes will be updated, by which point all users must transition to continue using Puma resources.</li> </ul> <p>See the below for information on using the new system.</p> <p>This update will have a number of impacts, and we are taking steps to minimize disruptions to HPC users.</p> <p>Things to not worry about</p> <ul> <li>All user files will remain unchanged. </li> <li>The Slurm scheduler and procedure for submitting jobs will remain unchanged. </li> <li>HPC systems will continue to be available for use during the OS update. </li> </ul> <p>Things to note</p> <ul> <li>Compute hour allocations on the test system will be independent of allocations on the existing Puma system. </li> <li>Significant changes to system software have occurred, most importantly: <ul> <li>updating the default compiler from GCC 8 (module <code>gnu8</code>) to GCC 13 (module <code>gnu13</code>).</li> <li>updating the default MPI distribution from OpenMPI 3 (module <code>openmpi3</code>) to OpenMPI 5 (module <code>openmpi5</code>).</li> </ul> </li> <li>Some previously compiled software will not run on the new system, and will need to be recompiled.</li> <li>HPC staff have recompiled public software modules when necessary.</li> <li>The majority of software modules previously available will remain available, sometimes with version changes.</li> </ul>"},{"location":"resources/updates/#submitting-jobs-to-the-updated-cluster","title":"Submitting jobs to the updated cluster","text":"<ul> <li>Subsets of nodes will be converted to the new OS, effectively creating a new cluster.</li> <li>The updated cluster can be selected by entering <code>puma9</code>, just as the target cluster is currently specified by entering <code>puma</code>, <code>ocelote</code> or <code>elgato</code>. </li> <li>The resources available on the Puma9 cluster will remain functionally identical to those on the existing Puma, including the number of CPUs per node and memory per CPU. Thus, Slurm batch scripts that work properly on Puma should work on the new Puma without modifications.</li> </ul>"},{"location":"resources/updates/#adapting-existing-analyses","title":"Adapting existing analyses.","text":"<p>Support</p> <p>Please open a support ticket if you have difficulty using the new updated Puma9 system.</p> <p>Researchers may currently use cluster resources in a number of ways. Here are general guidelines for adapting to the new environment:</p> Users of Open OnDemand Open OnDemand users should generally be able to continue usage unchanged. However, users of RStudio should see the R section below.  Users of software modules Users who load analysis software with the <code>module load</code> command will generally be able to continue their analyses unchanged.  All commonly used software modules have been transferred to Puma9, sometimes with version updates or changes. Users of Conda environments We recommend switching to Mamba going forward. However, many Conda users should be able to continue using their existing environments, although in some cases may need to recreate them. Users of Python Users who run pure Python code, with or without the use of virtual environments will likely be able to continue their analyses unchanged. The same versions of Python are available on Puma9. <p>Users of R</p> <ul> <li>The primary challenge for R users will be to maintain separate R package libraries for the old and new operating systems. R packages installed under the old CentOS 7 operating system may not function under the new Rocky 9 systems, and vice versa. </li> <li>R users should maintain separate R libraries and switch between them as necessary. See the Creating a Custom Library and Switching Between Custom Libraries sections on our R documentation page for details.  We suggest that your Puma9 R library be named something like <code>~/R/library_4.4-puma9</code>. </li> </ul> Users who compile code themselves In many cases user-compiled software will need to be recompiled to run on Puma9."},{"location":"resources/updates/#important-software-changes","title":"Important software changes","text":"<ul> <li>The procedure for using the Conda package manager is expected to change, and existing environments will likely need to be rebuilt. More news on this will be forthcoming.</li> <li>In addition to the currently available Cuda 11.8, Cuda 12.4 and 12.5 will also be available. </li> </ul>"},{"location":"resources/updates/#intel-compiled-software","title":"Intel compiled software","text":"<ul> <li>On the previous system, a number of software modules were provided for use in compiling software using the Intel compiler, e.g., modules <code>hdf5-intel</code>, <code>netcdf-intel</code>, etc. </li> <li>On the new system, you should first load the Intel software module with <code>module swap gnu13 intel</code>. After loading the Intel module, you can load the Intel specific modules without the <code>-intel</code> specifier, i.e., as <code>hdf5</code>, <code>netcdf</code>, etc.</li> </ul> old module name use now hdf5-intel hdf5 netcdf-intel netcdf netcdf-cxx-intel netcdf-cxx netcdf-fortran-intel netcdf-fortran petsc-complex/intel petsc petsc-real/intel petsc gsl-intel gsl phdf5-intel phdf5"},{"location":"resources/updates/#anaconda-conda-mamba","title":"Anaconda / Conda / Mamba","text":"<p>Mamba Guide</p> <p>See Mamba for our guide on using Mamba in place of Anaconda.</p> <p>While not part of the OS updates itself, we want to take this opportunity to bring to your attention upcoming changes to our use of Anaconda. Anaconda.org has started enforcing license restrictions which has necessitated these changes. You can read more about the license restrictions here.</p> <p>In short:</p> <ol> <li>The license restrictions only come into effect when a user installs packages from The Anaconda Repository.</li> <li>The license restrictions do not apply if a user uses the <code>conda</code> tool to install packages from community repositories like <code>conda-forge</code>.</li> </ol> <p>The existing Anaconda modules on the HPC automatically load The Anaconda Repository, and thus come under the purview of the license restrictions. If you use a local installation of <code>conda</code>(not the Anaconda modules), and you only install packages from community repositories, you need not read any further. However, you might still find the following useful.</p> <p>We are considering the following changes to help users transition to a new setup that will not be affected by the license restrictions:</p> <ol> <li>Deprecating the existing Anaconda modules. We will not install any new version of Anaconda. The existing modules will stay for a while, likely not beyond the end of the year, to give users time to transition to the new setup.</li> <li>Creating a new module based on the Miniforge distribution. Miniforge provides access to two tools \u2014 <code>conda</code> and <code>mamba</code> \u2014 and the <code>conda-forge</code> repository.</li> </ol> <p>With the Miniforge module you will be able to do the following:</p> <ol> <li>Install software from <code>conda-forge</code>, which is an extensive community repository. Whatever software you want, you will likely find it there. Of course, if required you will be able to use other repositories, except The Anaconda Repository.</li> <li>You can use <code>mambda</code> instead of <code>conda</code>. While access to <code>conda</code> will not go away, <code>mamba</code> is a much, much faster drop-in replacement for <code>conda</code>. Almost anything <code>conda</code> can do, <code>mamba</code> can do faster. Sometimes even succeeding where <code>conda</code> fails. We think you will have a much pleasant experience with <code>mamba</code> than with <code>conda</code>.</li> </ol> <p>If you are on the fence about this change, we highly recommend that you use the transition period to recreate and test your workflows and scripts with the <code>mamba</code> + <code>conda-forge</code> setup. Please do not wait till the last moment to make the necessary changes. While <code>mamba</code> is a drop-in replacement for <code>conda</code>, it is not a guarantee that your existing scripts will not break. Going forward, <code>mamba</code> + <code>conda-forge</code> will be the main setup that we support. We will share more details about how to use <code>mamba</code> shortly.</p>"},{"location":"results/","title":"Publications","text":""},{"location":"results/#user-publications","title":"User Publications","text":"<p>Have you used our HPC for your published research? Let us know and we'll add you to the list!</p> <p>Our users do some really cool stuff! Check out the list below for publications that were made possible by UArizona's HPC systems.</p> <p>"},{"location":"results/#steward-observatory-department-of-astronomy","title":"Steward Observatory, Department of Astronomy","text":"Tamadoni Saray, M., Yurkiv, V., &amp; Shahbazian-Yassar, R. (2023). Role of Kinetics and Thermodynamics in Controlling the Crystal Structure of Nickel Nanoparticles Formed on Reduced Graphene Oxide: Implications for Energy Storage and Conversion Applications. ACS Applied Nano Materials. Boruah, S. S., Eifler, T., Miranda, V., &amp; Krishanth, P. S. (2023). Accelerating cosmological inference with Gaussian processes and neural networks--an application to LSST Y1 weak lensing and galaxy clustering. Monthly Notices of the Royal Astronomical Society, 518(4)), 4818-4831. Xu, J., Eifler, T., Huff, E., Huang, H. J., Everett, S., &amp; Krause, E. (2022). Kinematic Lensing with the Roman Space Telescope. arXiv preprint arXiv:2201.00739. Kong, S., Whitworth, D. J., Smith, R. J., &amp; Hamden, E. T. (2022). Filament formation via collision-induced magnetic reconnection\u2013formation of a star cluster. Monthly Notices of the Royal Astronomical Society, 517(4), 4679-4695. Woodrum, C., Williams, C. C., Rieke, M., Leja, J., Johnson, B. D., Bezanson, R., ... &amp; Tacchella, S. (2022). Molecular Gas Reservoirs in Massive Quiescent Galaxies at $\\mathrm {z\\sim0. 7} $ Linked to Late Time Star Formation. arXiv preprint arXiv:2210.03832. Yung, L. Y., Somerville, R. S., Finkelstein, S. L., Behroozi, P., Dav\u00e9, R., Ferguson, H. C., ... &amp; Koekemoer, A. M. (2022). Semi-analytic forecasts for Roman--the beginning of a new era of deep-wide galaxy surveys. arXiv preprint arXiv:2210.04902. Long, J. D., Males, J. R., Haffert, S. Y., Close, L. M., Morzinski, K. M., Van Gorkom, K., ... &amp; Otten, G. P. (2022). XPipeline: Starlight subtraction at scale for MagAO-X. arXiv preprint arXiv:2208.07354. Wenzl, L., Doux, C., Heinrich, C., Bean, R., Jain, B., Dor\u00e9, O., ... &amp; Fang, X. (2022). Cosmology with the Roman Space Telescope\u2013Synergies with CMB lensing. Monthly Notices of the Royal Astronomical Society, 512(4), 5311-5328. Fang, X., Eifler, T., Schaan, E., Huang, H. J., Krause, E., &amp; Ferraro, S. (2022). Cosmology from clustering, cosmic shear, CMB lensing, and cross correlations: combining Rubin observatory and Simons Observatory. Monthly Notices of the Royal Astronomical Society, 509(4), 5721-5736. Eifler, T., Simet, M., Krause, E., Hirata, C., Huang, H. J., Fang, X., ... &amp; Wu, H. Y. (2021). Cosmology with the Roman Space Telescope: synergies with the Rubin Observatory Legacy Survey of Space and Time. Monthly Notices of the Royal Astronomical Society, 507(1), 1514-1527. Eifler, T., Miyatake, H., Krause, E., Heinrich, C., Miranda, V., Hirata, C., ... &amp; Wu, H. Y. (2021). Cosmology with the Roman Space Telescope\u2013multiprobe strategies. Monthly Notices of the Royal Astronomical Society, 507(2), 1746-1761. Fang, X., Eifler, T., &amp; Krause, E. (2020). 2D-FFTLog: efficient computation of real-space covariance matrices for galaxy clustering and weak lensing. Monthly Notices of the Royal Astronomical Society, 497(3), 2699-2714. Eifler, T., Simet, M., Krause, E., Hirata, C., Huang, H. J., Fang, X., ... &amp; Wu, H. Y. (2020). Cosmology with the Wide-Field Infrared Survey Telescope\u2013Synergies with the Rubin Observatory Legacy Survey of Space and Time. arXiv e-prints, art. arXiv preprint arXiv:2004.04702. Besla, G., Peter, A. H. G., &amp; Garavito-Camargo, N. (2019). The highest-speed local dark matter particles come from the Large Magellanic Cloud. Journal of Cosmology and Astroparticle Physics, 2019(11), 013. Garavito-Camargo, N., Besla, G., Laporte, C. F., Johnston, K. V., G\u00f3mez, F. A., &amp; Watkins, L. L. (2019). Hunting for the dark matter wake induced by the large magellanic cloud. The Astrophysical Journal, 884(1), 51. Behroozi, P., Wechsler, R. H., Hearin, A. P., &amp; Conroy, C. (2019). UniverseMachine: The correlation between galaxy growth and dark matter halo assembly from z= 0\u2212 10. Monthly Notices of the Royal Astronomical Society, 488(3), 3143-3194. Bozzola, G., Espino, P. L., Lewin, C. D., &amp; Paschalidis, V. (2019). Maximum mass and universal relations of rotating relativistic hybrid hadron-quark stars. The European Physical Journal A, 55(9), 1-18. Bozzola, G., &amp; Paschalidis, V. (2019). Initial data for general relativistic simulations of multiple electrically charged black holes with linear and angular momenta. Physical Review D, 99(10), 104044. Chan, C. K., Medeiros, L., \u00d6zel, F., &amp; Psaltis, D. (2018). GRay2: a general purpose geodesic integrator for kerr spacetimes. The Astrophysical Journal, 867(1), 59. Jeon, M., Besla, G., &amp; Bromm, V. (2017). Connecting the first galaxies with ultrafaint dwarfs in the Local Group: chemical signatures of Population III stars. The Astrophysical Journal, 848(2), 85. Raithel, C. A., \u00d6zel, F., &amp; Psaltis, D. (2017). From Neutron Star Observables to the Equation of State. II. Bayesian Inference of Equation of State Pressures. The Astrophysical Journal, 844(2), 156. Schneider, E. E., &amp; Robertson, B. E. (2017). Hydrodynamical coupling of mass and momentum in multiphase galactic winds. The Astrophysical Journal, 834(2), 144. Ball, D., \u00d6zel, F., Psaltis, D., &amp; Chan, C. K. (2016). Particle Acceleration and the Origin of X-Ray Flares in GRMHD Simulations of SGR A. The Astrophysical Journal, 826(1), 77. Smullen, R. A., Kratter, K. M., &amp; Shannon, A. (2016). Planet scattering around binaries: ejections, not collisions. Monthly Notices of the Royal Astronomical Society, 461(2), 1288-1301. Raithel, C. A., \u00d6zel, F., &amp; Psaltis, D. (2016). Model-independent inference of neutron star radii from moment of inertia measurements. Physical Review C, 93(3), 032801. Schneider, E. E., &amp; Robertson, B. E. (2015). CHOLLA: a new massively parallel hydrodynamics code for astrophysical simulation. The Astrophysical Journal Supplement Series, 217(2), 24. <p>"},{"location":"results/#schwartz-research-group","title":"Schwartz Research Group","text":"Luft, C. M., Munusamy, E., Pemberton, J. E., &amp; Schwartz, S. D. (2018). Molecular dynamics simulation of the oil sequestration properties of a nonionic rhamnolipid. The Journal of Physical Chemistry B, 122(14), 3944-3952. McConnell, M., Tal Grinspan, L., Williams, M. R., Lynn, M. L., Schwartz, B. A., Fass, O. Z., ... &amp; Tardiff, J. C. (2017). Clinically divergent mutation effects on the structure and function of the human cardiac tropomyosin overlap. Biochemistry, 56(26), 3403-3413. Munusamy, E., Luft, C. M., Pemberton, J. E., &amp; Schwartz, S. D. (2017). Structural properties of nonionic monorhamnolipid aggregates in water studied by classical molecular dynamics simulations. The Journal of Physical Chemistry B, 121(23), 5781-5793. Harijan, R. K., Zoi, I., Antoniou, D., Schwartz, S. D., &amp; Schramm, V. L. (2017). Catalytic-site design for inverse heavy-enzyme isotope effects in human purine nucleoside phosphorylase. Proceedings of the National Academy of Sciences, 114(25), 6456-6461. Pan, X., &amp; Schwartz, S. D. (2016). Conformational heterogeneity in the michaelis complex of lactate dehydrogenase: An analysis of vibrational spectroscopy using markov and hidden markov models. The Journal of Physical Chemistry B, 120(27), 6612-6620. Dzierlenga, M. W., Varga, M. J., &amp; Schwartz, S. D. (2016). Path Sampling Methods for Enzymatic Quantum Particle Transfer Reactions. Methods in enzymology, 578, 21-43. Zoi, I., Suarez, J., Antoniou, D., Cameron, S. A., Schramm, V. L., &amp; Schwartz, S. D. (2016). Modulating enzyme catalysis through mutations designed to alter rapid protein dynamics. Journal of the American Chemical Society, 138(10), 3403-3409. Williams, M. R., Lehman, S. J., Tardiff, J. C., &amp; Schwartz, S. D. (2016). Atomic resolution probe for allostery in the regulatory thin filament. Proceedings of the National Academy of Sciences, 113(12), 3257-3262. Pan, X., &amp; Schwartz, S. D. (2015). Free energy surface of the Michaelis complex of lactate dehydrogenase: a network analysis of microsecond simulations. The Journal of Physical Chemistry B, 119(17), 5430-5436. Dzierlenga, M. W., Antoniou, D., &amp; Schwartz, S. D. (2015). Another look at the mechanisms of hydride transfer enzymes with quantum and classical transition path sampling. The journal of physical chemistry letters, 6(7), 1177-1181. <p>"},{"location":"results/#others","title":"Others","text":"Wu, J., Zhu, X., AlYahyaei, K., Peyghambarian, N., &amp; Norwood, R. A. (2024). Toward 10-watt-level single-frequency fiber laser oscillators. Journal of the Optical Society of America B, 41(12), 2675-2683. Wang, Y.-H., &amp; Gupta, H. V. (2024). Towards interpretable physical-conceptual catchment-scale hydrological modeling using the mass-conserving-perceptron. Water Resources Research, 60, e2024WR037224. Burrell, D. J., Spencer, M. F., &amp; Driggers, R. G. (2024). Closed-loop adaptive optics in the presence of speckle and weak scintillation. Journal of Optics.     Burrell, D. J., Spencer, M. F., &amp; Driggers, R. G. (2024). Open-loop wavefront sensing in the presence of speckle and weak scintillation. Optics Communications, 130960. Sundman, M. H., Green, J. M., Fuglevand, A. J., &amp; Chou, Y.-H. (2024). TMS-derived short afferent inhibition discriminates cognitive status in older adults without dementia. Aging Brain, 6:100123. Sundman, M. H., De Vault, B. E. A., Chen, Y. A., Madhavan, L., Fuglevand, A. J., &amp; Chou, Y.-H. (2023). The (hyper)excitable brain: What can a ubiquitous TMS measure reveal about cognitive aging? Neurobiology of Aging, 132, 250-252. Das Goswami, B. R., Jabbari, V., Shahbazian-Yassar, R., Mashayek, F., &amp; Yurkiv, V. (2023). Unraveling Ion Diffusion Pathways and Energetics in Polycrystalline SEI of Lithium-Based Batteries: Combined Cryo-HRTEM and DFT Study. The Journal of Physical Chemistry C. Kuo, P. H., Chen, Y. A., Rodriguez, R. Stuehm, C., Chalasani, P., Chen, N.-K., &amp; Chou, Y.-H. (2023). Transcranial magnetic stimulation for the treatment of chemo brain. Sensors, 23(19):8017.     Gonzalez, E., Zarei, A., Hendler, N., Simmons, T., Zarei, A., Demieville, J., ... &amp; Pauli, D. (2023). PhytoOracle: Scalable, modular phenomics data processing pipelines. Frontiers in Plant Science, 14. Liu, Y., Lim, K., Sundman, M., Ugonna, C., Ton That, V., Cowen, S., &amp; Chou, Y.-H. (2023). Association between responsiveness to transcranial magnetic stimulation and interhemispheric functional connectivity of sensorimotor cortex. Brain Connectivity, 13(1). 39-50. Gong, Z., Rodriguez, N., Gagatsos, C. N., Guha, S., &amp; Bash, B. A. (2022) Quantum-Enhanced Transmittance Sensing. IEEE Journal of Selected Topics in Signal Processing, 17(2), 473-490. Benowitz, K. M., Allan, C. W., Jaworski, C. C., Sanderson, M. J., Diaz, F., Chen, X., &amp; Matzkin, L. M. (2022). Chromosome-length genome assemblies of cactophilic Drosophila illuminate links between structural and sequence evolution. bioRxiv. Manga, V. R., Muralidharan, K., &amp; Zega, T. J. (2022). The interplay between twinning and cation inversion in MgAl2O4-spinel: Implications for a nebular thermochronometer. American Mineralogist, 107(8), 1470-1476. Zanetta, P. M. (2022). The Unrestrained Use of Python Libraries for Bridging the Gap Between Planetary and Material Sciences. Microscopy and Microanalysis, 28(S1), 2686-2688. Song, J. P., Clay, R. T., &amp; Mazumdar, S. (2022). Valence Transition Theory of the Pressure-Induced Dimensionality Crossover in Superconducting Sr $ _ {14-x} $ Ca $ _x $ Cu $ _ {24} $ O $ _ {41} $. arXiv preprint arXiv:2207.00628. Chen, A. Y., Ton That, V., Ugonna, C., Liu, Y., Nadal, L., &amp; Chou, Y.-H. (2022). Diffusion MRI-guided theta burst stimulation enhances memory and functional connectivity along inferior longitudinal fasciculus in mild cognitive impairment. Proc Natl Acad Sci U S A, 119(21), e2113778119. Dunham, A. M., Kiser, E., Kargel, J. S., Haritashya, U. K., Watson, C. S., Shugar, D. H., ... &amp; DeCelles, P. G. (2022). Topographic control on ground motions and landslides from the 2015 Gorkha earthquake. Geophysical Research Letters, e2022GL098582. Hviding, R. E., Hainline, K. N., Rieke, M., Juneau, S., Lyu, J., &amp; Pucha, R. (2022). A New Infrared Criterion for Selecting Active Galactic Nuclei to Lower Luminosities. The Astronomical Journal, 163(5), 224. Thakur, A. K., Muralidharan, K., Zega, T. J., &amp; Ziurys, L. M. (2022). A nanometric window on fullerene formation in the interstellar medium: Insights from molecular dynamics studies. The Journal of Chemical Physics, 156(15), 154704. Bamberger, N. D., Dyer, D., Parida, K. N., El-Assaad, T. H., Pursell, D., McGrath, D. V., ... &amp; Monti, O. L. (2022). Beyond Simple Structure\u2013Function Relationships: The Interplay of Geometry, Electronic Structure, and Molecule/Electrode Coupling in Single-Molecule Junctions. The Journal of Physical Chemistry C, 126(15), 6653-6661. Rom\u00e1n\u2010Palacios, C., Moraga\u2010L\u00f3pez, D., &amp; Wiens, J. J. (2022). The origins of global biodiversity on land, sea and freshwater. Ecology letters. Lin, A. S., Monson, A., Mahadevan, S., Ninan, J. P., Halverson, S., Nitroy, C., ... &amp; Stefansson, G. (2022). Observing the Sun as a star: Design and early results from the NEID solar feed. The Astronomical Journal, 163(4), 184. Gorelik, R., Asaduzzaman, A., Manga, V. R., Thakur, A., &amp; Muralidharan, K. (2022). A First-Principles Investigation of Lithium and Sodium Ion Diffusion in C60 Molecular Solids. The Journal of Physical Chemistry C, 126(9), 4259-4266. Yang, B., Balazs, K. R., Butterfield, B. J., Laushman, K. M., Munson, S. M., Gornish, E. S., &amp; Barber\u00e1n, A. (2022). Does restoration of plant diversity trigger concomitant soil microbiome changes in dryland ecosystems?. Journal of Applied Ecology, 59(2), 560-573. Kinney, A. C., Current, S., &amp; Lega, J. (2021). Aedes-AI: Neural network models of mosquito abundance. PLoS computational biology, 17(11), e1009467. Spotnitz, M., Kwong, N. H., &amp; Binder, R. (2021)Terahertz spectroscopy of semiconductor microcavity lasers: Photon lasers. Physical Review B, 104(11), 115305. Song, J. P., Mazumdar, S., &amp; Clay, R. T. (2021). Absence of Luther-Emery superconducting phase in the three-band model for cuprate ladders. Physical Review B, 104(10), 104504. Hosseinverdi, S., &amp; Fasel, H. F. (2022). Towards understanding of natural boundary-layer transition for low-speed flows via random excitation. In IUTAM Laminar-Turbulent Transition (pp. 391-405). Springer, Cham. Chen, Y., Martinez, A., Cleavenger, S., Rudolph, J., &amp; Barber\u00e1n, A. (2021). Changes in soil microbial communities across an urbanization gradient: A local-scale temporal study in the arid southwestern usa. Microorganisms, 9(7), 1470. Gillan, J. K., Ponce\u2010Campos, G. E., Swetnam, T. L., Gorlier, A., Heilman, P., &amp; McClaran, M. P. (2021). Innovations to expand drone data collection and analysis for rangeland monitoring. Ecosphere, 12(7), e03649. He, W., Guha, S., Shapiro, J. H., &amp; Bash, B. A. (2021). Performance analysis of free-space quantum key distribution using multiple spatial modes. Optics Express, 29(13), 19305-19318. Li, H., &amp; Bredas, J. L. (2021). Impact of structural defects on the elastic properties of two-dimensional covalent organic frameworks (2D COFs) under tensile stress. Chemistry of Materials, 33(12), 4529-4540. Zhou, Z., Daly, A. M., &amp; Kukolich, S. G. (2021). Microwave measurements of rotational transitions and nitrogen quadrupole coupling for 2-aminopyridine. Journal of Molecular Spectroscopy, 378, 111457. Li, H., Evans, A. M., Dichtel, W. R., &amp; Bredas, J. L. (2021). Quantitative Description of the Lateral Growth of Two-Dimensional Covalent Organic Frameworks Reveals Self-Templation Effects. ACS Materials Letters, 3(4), 398-405. Klein, K. G., Verniero, J. L., Alterman, B., Bale, S., Case, A., Kasper, J. C., ... &amp; Whittlesey, P. (2021). Inferred Linear Stability of Parker Solar Probe Observations Using One-and Two-component Proton Distributions. The Astrophysical Journal, 909(1), 7. Chen, Y., Neilson, J. W., Kushwaha, P., Maier, R. M., &amp; Barber\u00e1n, A. (2021). Life-history strategies of soil microbial communities in an arid ecosystem. The ISME journal, 15(3), 649-657. Xiong, Z., Potter, C. J., &amp; McLeod, E. (2021). High-speed lens-free holographic sensing of protein molecules using quantitative agglutination assays. ACS sensors, 6(3), 1208-1217. Zhang, Y., Weeks, R. R., Lecaplain, C., Harilal, S. S., Yeak, J., Phillips, M. C., &amp; Jones, R. J. (2021). Burst-mode dual-comb spectroscopy. Optics letters, 46(4), 860-863. Luk, S. M., Vergnet, H., Lafont, O., Lewandowski, P., Kwong, N. H., Galopin, E., ... &amp; Baudin, E. (2021). All-optical beam steering using the polariton lighthouse effect. ACS Photonics, 8(2), 449-454. Ivie, J. A., Bamberger, N. D., Parida, K. N., Shepard, S., Dyer, D., Saraiva-Souza, A., ... &amp; Monti, O. L. (2021). Correlated energy-level alignment effects determine substituent-tuned single-molecule conductance. ACS Applied Materials &amp; Interfaces, 13(3), 4267-4277. Lemoine, D. (2021). The climate risk premium: how uncertainty affects the social cost of carbon. Journal of the Association of Environmental and Resource Economists, 8(1), 27-57. Tanjaroon, C., Mills, D. D., Hoyos, C. A. J., &amp; Kukolich, S. G. (2021). Calculations and analysis of 55Mn nuclear quadrupole coupling for asymmetric top acyl methyl manganese pentacarbonyl. Chemical Physics Letters, 762, 138151. Sundman. M., Lim, K., Ton That, V., Mizell, J.-M., Ugonna, C., Rodriguez, R., Chen, N.-K., Fuglevand, A. J., Liu, Y., Wilson, R. C., Fellous, J.-M., Rapcsak, S., &amp; Chou, Y.-H. (2020). Transcranial magnetic stimulation reveals diminished homeostatic metaplasticity in cognitively impaired adults. Brain Communications, 2, fcaa203. Kuo, H. P., Zhang, X., Stuehm, C. Chou, Y.-H., Chen, N.-K. (2020). Functional magnetic resonance imaging reveals Taiji effects on neuronal networks of the brain. Journal of Chinese Health Practices. Jaworski, C. C., Allan, C. W., &amp; Matzkin, L. M. (2020). Chromosome\u2010level hybrid de novo genome assemblies as an attainable option for nonmodel insects. Molecular ecology resources, 20(5), 1277-1293. Tanjaroon, C., Zhou, Z., Mills, D., Keck, K., &amp; Kukolich, S. G. (2020). Microwave Spectra and Theoretical Calculations for Two Structural Isomers of Methylmanganese Pentacarbonyl. Inorganic chemistry, 59(9), 6432-6438. Pyarelal, A., &amp; Su, S. (2020). Higgs assisted razor search for Higgsinos at a 100 TeV pp collider. SCIENCE CHINA Physics, Mechanics &amp; Astronomy, 63(10), 1-9. Klein, K. G., Martinovi\u0107, M., Stansby, D., &amp; Horbury, T. S. (2019). Linear stability in the inner heliosphere: Helios re-evaluated. The Astrophysical Journal, 887(2), 234. Yazdi, E. T., Limaye, A., Akoglu, A., Adegbija, T., &amp; Buntzman, A. (2019, December). Bit-wise and Multi-GPU Implementations of the DNA Recombination Algorithm. In 2019 IEEE 26th International Conference on High Performance Computing, Data, and Analytics (HiPC) (pp. 131-140). IEEE. Sarid, G., Volk, K., Steckloff, J. K., Harris, W., Womack, M., &amp; Woodney, L. M. (2019). 29P/Schwassmann\u2013Wachmann 1, a centaur in the gateway to the Jupiter-family comets. The Astrophysical Journal Letters, 883(1), L25. Grant, A. D., Vail, P., Padi, M., Witkiewicz, A. K., &amp; Knudsen, E. S. (2019). Interrogating mutant allele expression via customized reference genomes to define influential cancer mutations. Scientific reports, 9(1), 1-15. Volk, K., &amp; Malhotra, R. (2019). Not a simple relationship between Neptune\u2019s migration speed and Kuiper belt inclination excitation. The Astronomical Journal, 158(2), 64. Kling, F., Li, H., Pyarelal, A., Song, H., &amp; Su, S. (2019). Exotic Higgs decays in Type-II 2HDMs at the LHC and future 100 TeV hadron colliders. Journal of High Energy Physics, 2019(6), 1-33. Esmaili, E., Akoglu, A., Hariri, S., &amp; Moukabary, T. (2019). Implementation of scalable bidomain-based 3d cardiac simulations on a graphics processing unit cluster. The Journal of Supercomputing, 75(8), 5475-5506. Schissler, A. G., Piegorsch, W. W., &amp; Lussier, Y. A. (2018). Testing for differentially expressed genetic pathways with single-subject N-of-1 data in the presence of inter-gene correlation. Statistical methods in medical research, 27(12), 3797-3813. Chen, N.-K., Chou, Y.-H., Sundman, M., Hickey, P., Kasoff, W. S., Bernstein, A., Lin, T., Rapcsak, S. Z., Sherman, S. J., Weingarten, C. P. (2018). Alteration of Diffusion-Tensor Magnetic Resonance Imaging Measures in Brain Regions Involved in Early Stages of Parkinson's Disease. Brain Connectivity, 8(6), 343-349. Alsayoud, A. Q., Manga, V. R., Muralidharan, K., Vita, J., Bringuier, S., Runge, K., &amp; Deymier, P. (2018). Atomistic insights into the effect of polymerization on the thermophysical properties of 2-D C-60 molecular solids. Carbon, 133, 267-274. Gladstein, A. L., Quinto-Cort\u00e9s, C. D., Pistorius, J. L., Christy, D., Gantner, L., &amp; Joyce, B. L. (2018). SimPrily: A Python framework to simplify high-throughput genomic simulations. SoftwareX, 7, 335-340. Lucas, P., Coleman, G. J., Venkateswara Rao, M., Edwards, A. N., Devaadithya, C., Wei, S., ... &amp; Deymier, P. A. (2017). Structure of ZnCl2 melt. Part II: Fragile-to-strong transition in a tetrahedral liquid. The Journal of Physical Chemistry B, 121(49), 11210-11218. Vasquez, M. M., Hu, C., Roe, D. J., Chen, Z., Halonen, M., &amp; Guerra, S. (2016). Least absolute shrinkage and selection operator type methods for the identification of serum biomarkers of overweight and obesity: simulation and application. BMC medical research methodology, 16(1), 1-19. Hendler, N. P., Mulders, G. D., Pascucci, I., Greenwood, A., Kamp, I., Henning, T., ... &amp; Evans II, N. J. (2017). Hints for Small Disks around Very Low Mass Stars and Brown Dwarfs\u2217. The Astrophysical Journal, 841(2), 116. Li, Q., Schissler, A. G., Gardeux, V., Achour, I., Kenost, C., Berghout, J., ... &amp; Lussier, Y. A. (2017). N-of-1-pathways MixEnrich: advancing precision medicine via single-subject analysis in discovering dynamic changes of transcriptomes. BMC Medical Genomics, 10(1), 5-16. Lorenzo, A. T., Morzfeld, M., Holmgren, W. F., &amp; Cronin, A. D. (2017). Optimal interpolation of satellite and ground data for irradiance nowcasting at city scales. Solar Energy, 144, 466-474. Li, Q., Schissler, A. G., Gardeux, V., Berghout, J., Achour, I., Kenost, C., ... &amp; Lussier, Y. A. (2017). kMEn: Analyzing noisy and bidirectional transcriptional pathway responses in single subjects. Journal of biomedical informatics, 66, 32-41. Schlawin, E., Rieke, M., Leisenring, J., Walker, L. M., Fraine, J., Kelly, D., ... &amp; Stansberry, J. (2016). Two NIRCam channels are better than one: how JWST can do more science with NIRCam\u2019s short-wavelength dispersed Hartmann sensor. Publications of the Astronomical Society of the Pacific, 129(971), 015001. Danford, F., Welch, E., C\u00e1rdenas-R\u00f3driguez, J., &amp; Strout, M. M. (2016, September). Analyzing parallel programming models for magnetic resonance imaging. In International Workshop on Languages and Compilers for Parallel Computing (pp. 188-202). Springer, Cham. Schissler, A. G., Li, Q., Chen, J. L., Kenost, C., Achour, I., Billheimer, D. D., ... &amp; Lussier, Y. A. (2016). Analysis of aggregated cell\u2013cell statistical distances within pathways unveils therapeutic-resistance mechanisms in circulating tumor cells. Bioinformatics, 32(12), i80-i89."},{"location":"running_jobs/batch_jobs/array_jobs/","title":"Array Jobs","text":""},{"location":"running_jobs/batch_jobs/array_jobs/#what-are-job-arrays","title":"What Are Job Arrays?","text":"<p>Slurm job arrays are a powerful feature for submitting and managing multiple similar jobs efficiently using a single script. Instead of submitting individual jobs, users define an array with shared characteristics, treating them as a unified entity. Each subjob within the array is assigned a unique environment variable, enabling easy differentiation. Array jobs are submitted in the same manner as regular batch jobs: <code>sbatch myscript.slurm</code>. Slurm then schedules and executes these tasks based on resource availability, streamlining the process.</p>"},{"location":"running_jobs/batch_jobs/array_jobs/#why-use-job-arrays","title":"Why Use Job Arrays?","text":"<ol> <li> <p>Workflow Efficiency </p> <p>Job arrays streamline the process of managing multiple jobs with similar configurations, reducing manual effort and potential errors in job submission.</p> </li> <li> <p>Resource Utilization</p> <p>By grouping similar tasks into a single job array, you can optimize resource utilization on the cluster. Slurm can efficiently allocate resources to individual tasks within the array based on availability. </p> </li> <li> <p>Scheduler Efficiency</p> <p>Submitting multiple individual jobs in loops can significantly slow down the scheduler for all users. Job arrays help alleviate this issue by reducing the number of job submissions, leading to improved scheduler performance and responsiveness.</p> </li> <li> <p>Scalability</p> <p>Job arrays are particularly useful for parallel and repetitive tasks, such as parameter sweeps, Monte Carlo simulations, or running the same code with different inputs. They provide a scalable approach to handling large numbers of tasks without the need to write and manage hundreds or thousands of related submission scripts.</p> </li> <li> <p>Simplified Management</p> <p>With job arrays, you only need to manage a single submission script for a group of tasks, making it easier to track, monitor, and troubleshoot your jobs.</p> </li> </ol>"},{"location":"running_jobs/batch_jobs/array_jobs/#how-to-use-job-arrays","title":"How to Use Job Arrays","text":"<p>Array job resources</p> <p>The resource requirements defined in an array job script are applied to each job in the array.</p> <p>To utilize job arrays, a specific range of task IDs needs to be specified using the <code>--array</code> batch directive in your submission script. This directive is set to a range of integer values and determines the number of individual jobs submitted. It also controls the index associated with each subjob in the array.</p> <p>Since each task within an array can have its own parameters, input files, or commands, a unique identifier is needed to differentiate jobs. These are controlled by the Slurm environment variable <code>$SLURM_ARRAY_TASK_ID</code> which is set to a unique integer value for each subjob. The integers associated with each job match the values specified with your <code>--array</code> directive.</p> <p>For example, the following directive would tell the scheduler to submit three jobs, each with index 1, 2, and 3, respectively.</p> <p><pre><code>#SBATCH --array=1-3\n</code></pre> Arrays can begin at any value, so the following option would also submit three jobs, each with index 8, 9, and 10, respectively.</p> <pre><code>#SBATCH --array=8-10\n</code></pre> <p>Non-sequential array indices can also be used. For example:</p> <pre><code>#SBATCH --array=1-3,10,15-20\n</code></pre>"},{"location":"running_jobs/batch_jobs/array_jobs/#monitoring-array-jobs","title":"Monitoring Array Jobs","text":"<p>When you submit a job array, Slurm will assign a single job ID to the array. When you are tracking your jobs, if you run a standard <code>squeue</code> you will only see the parent job ID. If you use the command <code>squeue -r --job &lt;jobid&gt;</code>, that will return a list of all subjobs in the array where each job ID is formatted as <code>&lt;parent_job_id&gt;_&lt;job_array_index&gt;</code>. </p>"},{"location":"running_jobs/batch_jobs/array_jobs/#output-files","title":"Output Files","text":"<p>Each subjob in an array produces its own output file. By default, these are formatted as <code>slurm-&lt;parent_job_id&gt;_&lt;job_array_index&gt;.out</code>. Be careful if you set your own custom output filenames.  If the output filenames are not distinguished from one another using an array task ID, your output files will overwrite one another. </p>"},{"location":"running_jobs/batch_jobs/array_jobs/#example-jobs","title":"Example Jobs","text":"Basic Array JobArray with Text Filenames <p>In this example, we'll use Python to analyze a series of input files called <code>1.inp</code>, <code>2.inp</code>, and <code>3.inp</code>. This general methodology can be adapted to your own analyses using similar techniques. The most important takeaway is the use of the <code>--array</code> flag and the environment variable <code>$SLURM_ARRAY_TASK_ID</code> which is used to control each job's input.</p> <p>Let's say each of our input files has a series of space-delimited integers that we'd like to sum using a Python script. Rather than using a for loop to process each in a single job, or writing multiple submission scripts to explicitly use different input filenames, we can simply use an array job to define a different input file for each subjob. The structure of our working directory is shown below.</p> Directory structure<pre><code>[netid@junonia ~]$ tree\n.\n\u251c\u2500\u2500 input\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1.inp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2.inp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 3.inp\n\u251c\u2500\u2500 output\n\u251c\u2500\u2500 sum_inputs.py\n\u2514\u2500\u2500 sum_inputs.slurm\n\n2 directories, 5 files\n</code></pre> Click to view Python script sum_inputs.py<pre><code>#!/usr/bin/env python3\n\nimport sys, os\n\n\ndef sum_file(input_file):\n    with open(input_file, \"r\") as f:\n        data = f.read()\n        numbers = [int(num) for num in data.split()]\n    return sum(numbers)\n\n\nif __name__ == \"__main__\":\n    array_index = int(sys.argv[1])\n    input_file  = os.path.join(\"input\",f\"{array_index}.inp\")\n    output_file = os.path.join(\"output\",f\"{array_index}.out\")\n\n    result = sum_file(input_file)\n\n    with open(output_file, \"w\") as f:\n        f.write(f\"Sum of numbers in {input_file}: {result}\\n\")\n</code></pre> <p>Below is the Slurm script we can use to execute this workflow. The specific lines that include the relevant batch directive and environment variable that make this an array job are highlighted below.  sum_inputs.slurm<pre><code>#!/bin/bash\n\n#SBATCH --job-name=sum_inputs\n#SBATCH --output=slurm_logs/%x_%A_%a.out\n#SBATCH --account=hpcteam\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:01:00\n#SBATCH --array=1-3\n\nmodule load python/3.11\npython3 sum_inputs.py ${SLURM_ARRAY_TASK_ID}\n</code></pre></p> <p>In this case, we're passing <code>$SLURM_ARRAY_TASK_ID</code> to the Python script which uses its value to find the relevant input file. </p> <p>Submitting this script with <code>sbatch</code> will return a single job ID that we can use to track all three of our jobs.  Job submission and tracking<pre><code>[netid@junonia ~]$ sbatch sum_inputs.slurm \nSubmitted batch job 3186754 \n\n[netid@junonia ~]$ squeue --job=3186754 -r\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n        3186754_1  standard sum_inpu    netid PD       0:00      1 (Priority)\n        3186754_2  standard sum_inpu    netid PD       0:00      1 (Priority)\n        3186754_3  standard sum_inpu    netid PD       0:00      1 (Priority)\n</code></pre></p> <p>Once our job completes, we can check our output: <pre><code>[netid@junonia ~]$ cat output/*\nSum of numbers in input/1.inp: 25\nSum of numbers in input/2.inp: 30\nSum of numbers in input/3.inp: 15\n</code></pre></p> <p>Sometimes, using <code>$SLURM_ARRAY_TASK_ID</code> to directly define input isn't convenient. For example, in the biosciences often times sequence files (fasta, fastq, etc) might have names that are not conducive to sequential numerical ordering. In these cases, one option might be to store input filenames in a file and use the array ID to pull filenames from line numbers. This can be accomplished with the Bash command <code>sed</code>.</p> <p>As an example, let's say we have five input files, each with a protein sequence in fasta format whose structural disorder we want to calculate. Our directory structure might look something like the following:</p> Directory structure<pre><code>[netid@junonia array_filenames_example]$ tree\n.\n\u251c\u2500\u2500 input\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 AJD81427.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 AMK26954.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 AZU90721.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 QDZ58854.fa\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 QFP98592.fa\n\u251c\u2500\u2500 output\n\u2514\u2500\u2500 predict_disorder.slurm\n\n2 directories, 6 files\n</code></pre> <p>We can write these filenames to a list of input files by using the following: <pre><code>[netid@junonia array_filenames_example]$ ls -1 input/ &gt; InputFiles \n[netid@junonia array_filenames_example]$ cat InputFiles \nAJD81427.fa\nAMK26954.fa\nAZU90721.fa\nQDZ58854.fa\nQFP98592.fa\n</code></pre></p> <p>If we use the command <code>sed</code>, we can pull lines from this file using their line numbers. For example, to pull line three:</p> <pre><code>[netid@junonia array_filenames_example]$ sed \"3q;d\" InputFiles\nAZU90721.fa \n</code></pre> <p>If we replace the 3 in the above with <code>${SLURM_ARRAY_TASK_ID}</code> we can submit an array and pull a different line for each subjob. For example:</p> predict_disorder.slurm<pre><code>#!/bin/bash\n\n#SBATCH --job-name=predict_disorder\n#SBATCH --output=slurm_output/%A_%a.out\n#SBATCH --account=hpcteam\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=0:05:00\n#SBATCH --array=1-5\n\ninput_filename=\"$( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputFiles )\"\noutput_filename=\"$( echo $input_filename | awk '{sub(/\\.fa$/, \".out\"); print}')\"\n\niupred3 input/${input_filename} long &gt; output/${output_filename}\n</code></pre> <p>In the script above, the parts specific to array jobs are highlighted. We're setting the Bash variable <code>input_filenames</code> to the output of our <code>sed</code> command, which is the filename specific to that subjob. The command <code>awk</code> is being used to replace the file extension <code>.fa</code> with <code>.out</code> so we can save our output data. We then run our example analyses using the protein disorder prediction software <code>iupred3</code> on the input file and save it to the output file. </p> <p>Submitting with sbatch, we can track our five subjobs:</p> <pre><code>[netid@junonia array_filenames_example]$ sbatch predict_disorder.slurm \nSubmitted batch job 3207840\n\n[netid@junonia array_filenames_example]$ squeue --job=3207840 -r\n     JOBID      PARTITION NAME     USER  ST       TIME  NODES NODELIST(REASON)\n     3207840_1  standard  predict_ netid PD       0:00      1 (Priority)\n     3207840_2  standard  predict_ netid PD       0:00      1 (Priority)\n     3207840_3  standard  predict_ netid PD       0:00      1 (Priority)\n     3207840_4  standard  predict_ netid PD       0:00      1 (Priority)\n     3207840_5  standard  predict_ netid PD       0:00      1 (Priority)\n</code></pre> <p>When our analyses are complete, we should see an output file corresponding to each of our input files:</p> <pre><code>(ocelote) [netid@junonia array_filenames_example]$ tree\n.\n\u251c\u2500\u2500 input\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 AJD81427.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 AMK26954.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 AZU90721.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 QDZ58854.fa\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 QFP98592.fa\n\u251c\u2500\u2500 InputFiles\n\u251c\u2500\u2500 output\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 AJD81427.out\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 AMK26954.out\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 AZU90721.out\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 QDZ58854.out\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 QFP98592.out\n\u251c\u2500\u2500 predict_disorder.slurm\n\u2514\u2500\u2500 slurm_output\n    \u251c\u2500\u2500 3207840_1.out\n    \u251c\u2500\u2500 3207840_2.out\n    \u251c\u2500\u2500 3207840_3.out\n    \u251c\u2500\u2500 3207840_4.out\n    \u2514\u2500\u2500 3207840_5.out\n\n3 directories, 17 files\n</code></pre>"},{"location":"running_jobs/batch_jobs/batch_directives/","title":"Batch Directives","text":"<p>Tip</p> <p>For a full list of directives, see Slurm's official documentation.</p> <p>The first section of a batch script (after the shebang) always contains the Slurm Directives, which specify the resource requests for your job. The scheduler parses these in order to allocate CPUs, memory, walltime, etc. to your job request.</p>"},{"location":"running_jobs/batch_jobs/batch_directives/#allocations-and-partitions","title":"Allocations and Partitions","text":"<p>The partitions, or queues, on the UArizona HPC which determine the priority of your jobs and resources available to them are shown in the table below. With the exception of Windfall, these consume your monthly allocation. See our allocations documentation for more detailed information on each. The syntax to request each of the following is shown below:</p> <p>Buy-in users must use the <code>--qos</code> directive</p> <p>If you're a member of a buy-in group and are trying to use your high priority hours, ensure you are including a <code>--qos</code> directive. When this directive is missing, you will recieve the error: <pre><code>sbatch: error: QOSGrpSubmitJobsLimit\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n</code></pre></p> Partition Request Syntax Comments Standard <pre><code>#SBATCH --account=&lt;PI GROUP&gt;#SBATCH --partition=standard</code></pre> Request a CPU-only node using standard hours. Standard GPU <pre><code>#SBATCH --account=&lt;PI GROUP&gt;#SBATCH --partition=gpu_standard#SBATCH --gres=gpu:&lt;options&gt;</code></pre> Request GPU resources using standard hours. See the GPUs section below for details on the <code>gres</code> directive. Windfall <pre><code>#SBATCH --partition=windfall</code></pre> Unlimited access. Preemptible. Do not include an <code>--account</code> flag when requesting this partition. Windfall GPU <pre><code>#SBATCH --partition=gpu_windfall#SBATCH --gres=gpu:&lt;options&gt;</code></pre> Request GPU resources using windfall. See the GPUs section below for details on the <code>gres</code> directive. Do not include an <code>--account</code> flag when requesting this partition. High Priority <pre><code>#SBATCH --account=&lt;PI GROUP&gt;#SBATCH --partition=high_priority#SBATCH --qos=user_qos_&lt;PI GROUP&gt;</code></pre> Request a CPU-only node with high priority resources. Only available to buy-in groups. High Priority GPU <pre><code>#SBATCH --account=&lt;PI GROUP&gt;#SBATCH --partition=gpu_high_priority#SBATCH --qos=user_qos_&lt;PI GROUP&gt;#SBATCH --gres=gpu:&lt;options&gt;</code></pre> Request GPU resources with high priority hours. Only available to buy-in groups. See the GPUs section below for details on the <code>gres</code> directive. Qualified <pre><code>#SBATCH --account=&lt;PI GROUP&gt;#SBATCH --partition=standard#SBATCH --qos=qual_qos_&lt;PI GROUP&gt;</code></pre> Available to groups with an activate special project. Qualified GPU <pre><code>#SBATCH --account=&lt;PI GROUP&gt;#SBATCH --partition=gpu_standard#SBATCH --qos=qual_qos_&lt;PI GROUP&gt;#SBATCH --gres=gpu:&lt;options&gt;\nRequest GPU resources with qualified hours. Available to groups with an activate special project. See the GPUs section below for details on the <code>gres</code> directive."},{"location":"running_jobs/batch_jobs/batch_directives/#cpus","title":"CPUs","text":"<p>Each job must specify the requested number of CPUs with the <code>--ntasks</code> directive.  This can be done in one of two ways:</p>\n<ol>\n<li>\n<p>If your application is making use of MPI or is executing simultaneous distinct processes, you can request <code>&lt;N&gt;</code> CPUs with</p>\n<pre><code>#SBATCH --ntasks=&lt;N&gt;\n</code></pre>\n</li>\n<li>\n<p>If you are using a multithreaded application, then you can request <code>&lt;N&gt;</code> CPUs with:\n    <pre><code>#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=&lt;N&gt;\n</code></pre></p>\n</li>\n</ol>"},{"location":"running_jobs/batch_jobs/batch_directives/#nodes","title":"Nodes","text":"Single vs. Multi-Node Programs\n<p>In order for your job to make use of more than one node, it must be able to make use of something like MPI. </p>\n<p>If your application is not MPI-enabled, always set <code>--nodes=1</code></p>\n\n<p>The term node refers to the number of physical computers allocated to your job. The syntax to allocate <code>&lt;N&gt;</code> nodes to a job is:</p>\n<pre><code>#SBATCH --nodes=&lt;N&gt;\n</code></pre>"},{"location":"running_jobs/batch_jobs/batch_directives/#time","title":"Time","text":"<p>The syntax for requesting time for your job is <code>HHH:MM:SS</code> or <code>DD-HHH:MM:SS</code>. The maximum amount of time that can be requested is 10 days for a batch job. More details in Job Limits.</p>\n<pre><code>#SBATCH --time=HHH:MM:SS\n</code></pre>"},{"location":"running_jobs/batch_jobs/batch_directives/#memory-and-high-memory-nodes","title":"Memory and High Memory Nodes","text":"<p>Memory and CPUs are connected</p>\n<p>More detailed information on memory and CPU requests can be found on our CPUs and Memory page.</p>\n\n\n<p>Include Units</p>\n<p>If you exclude <code>gb</code> from your memory request, Slurm will default to <code>mb</code>.</p>\n\n<p>Memory is an optional flag. By default, the scheduler will allocate you the standard CPU/memory ratio available on the cluster. </p>\n<p>Memory can either be requested with the <code>--mem</code> or <code>--mem-per-cpu</code> flags. The <code>--mem</code> flag indicates the amount of Memory per node to allocate to your job. If you are running multi-node MPI jobs with this flag, the total amount of memory you will receive will be <code>mem</code>\\(\\times\\)<code>nodes</code></p>\n<p>The general syntax for requesting <code>&lt;N&gt;</code> GB of memory per node is\n<pre><code>#SBATCH --mem=&lt;N&gt;gb\n</code></pre>\nor, to request <code>&lt;N&gt;</code> GB of memory per CPU:\n<pre><code>#SBATCH --mem-per-cpu=&lt;N&gt;gb\n</code></pre></p>\n<p>High Memory Node Requests</p>\n<p>To request a high memory node, you will need the additional flag <code>--constraint=hi_mem</code>. It is recommended to use the exact directives below to avoid unexpected behavior.</p>\n\n\n\nCluster\nCommand\n\n\n\n\nOcelote\n<pre><code>#SBATCH --mem-per-cpu=41gb#SBATCH --constraint=hi_mem</code></pre>\n\n\nPuma\n<pre><code>#SBATCH --mem-per-cpu=32gb#SBATCH --constraint=hi_mem</code></pre>"},{"location":"running_jobs/batch_jobs/batch_directives/#gpus","title":"GPUs","text":"<p>GPU partitions must be used</p>\n<p>GPU jobs will need to use GPU-specific partitions. See the partitions section at the top of this page for details. </p>\n\n\n<p>GPU options are per node</p>\n<p>When using <code>--gres=gpu:N</code>, keep in mind that the total number of GPUs the job is allocated is <code>N</code> per node. </p>\n\n<p>GPUs are an optional resource that may be requested with the <code>--gres</code> directive. For an overview of the specific GPU resources available on each cluster, see our resources page. </p>\n\n  \n    \n    \n    \n  \n  \n    Cluster\n    Directive\n    Target\n  \n  \n    Puma\n    <pre><code>#SBATCH --gres=gpu:1</code></pre>\n    Request a single GPU. This will either target one Volta GPU (v100) or one A100 MIG slice, depending on availability. Only one GPU should be selected with this method to avoid being allocated multiple MIG slices.\n  \n  \n    <pre><code>#SBATCH --gres=gpu:nvidia_a100_80gb_pcie_2g.20gb</code></pre>\n    Target one A100 MIG slice.\n  \n  \n    <pre><code>#SBATCH --gres=gpu:volta:N</code>\n    Request <code>N</code> V100 GPUs where 1\u2264<code>N</code>\u22644\n  \n  \n    Ocelote\n    <pre><code>#SBATCH --gres=gpu:N#SBATCH --mem-per-cpu=8gb</code></pre>\n    Request <code>N</code> GPUs, where 1\u2264<code>N</code>\u22642. This will target either one or two Pascals (p100s)"},{"location":"running_jobs/batch_jobs/batch_directives/#job-arrays","title":"Job Arrays","text":"<p>Array jobs in Slurm allow users to submit multiple similar tasks as a single job. Each task within the array can have its own unique input parameters, making it ideal for running batch jobs with varied inputs or executing repetitive tasks efficiently. The flag for submitting array jobs is:</p>\n<p><pre><code>#SBATCH --array=&lt;N&gt;-&lt;M&gt;\n</code></pre>\nwhere <code>&lt;N&gt;</code> and <code>&lt;M&gt;</code> are integers. </p>\n<p>For detailed information on job arrays, see our job array tutorial.</p>"},{"location":"running_jobs/batch_jobs/batch_directives/#job-dependencies","title":"Job Dependencies","text":"<p>Slurm job dependencies allow users to submit to a series of jobs that depend on each other using the flag and options:\n<pre><code>--dependency=&lt;type:jobid[:jobid][,type:jobid[:jobid]]&gt;\n</code></pre></p>\n<p>For example, say job <code>B</code> depends on the successful completion of job <code>A</code>. Job <code>B</code> can be submitted as a dependency of job <code>A</code> using the following method:</p>\n<p><pre><code>[netid@junonia ~]$  sbatch A.slurm\nSubmitted batch job 1939000\n[netid@junonia ~]$ sbatch --afterok:1939000 B.slurm\n</code></pre>\nThis tells the scheduler to hold job <code>B</code> until job <code>A</code> completes. The <code>afterok</code> is the dependency <code>&lt;type&gt;</code>, in this case it ensures that job <code>B</code> runs only if job <code>A</code> completes successfully. The different options for <code>&lt;type&gt;</code> are show below:</p>\n\n\n\nDependency Type\nMeaning\n\n\n\n\n<code>after</code>\nJob can begin after the specified job(s) have started\n\n\n<code>afterany</code>\nJob can begin after the specified job(s) have terminated. Job(s) will start regardless of whether the specified jobs failed or ran successfully\n\n\n<code>afterok</code>\nJob can begin after the specified job(s) have completed successfully. If the specified job(s) fail, the dependency will never run.\n\n\n<code>afternotok</code>\nJob can begin after the specified job(s) have failed. If the specified job(s) complete successfully, the dependency will never run."},{"location":"running_jobs/batch_jobs/batch_directives/#output-filenames","title":"Output Filenames","text":"<p>The default output filename for a slurm job is <code>slurm-&lt;jobid&gt;.out</code>. If desired, this can be customized using the directives\n<pre><code>#SBATCH -o output_filename.out\n#SBATCH -e output_filename.err\n</code></pre></p>\n<p>Filenames take patterns that allow for job information substitution. A list of filename patterns is shown below. </p>\n\n\n\nVariable\nMeaning\nExample Slurm Directive(s)\nSample Output\n\n\n\n\n<code>%A</code>\nA job array's main job ID\n<pre><code>#SBATCH --array=1-2#SBATCH -o %A.out#SBATCH --open-mode=append</code></pre>\n<code>12345.out</code>\n\n\n<code>%a</code>\nA job array's index number\n<pre><code>#SBATCH --array=1-2#SBATCH -o %A_%a.out</code></pre>\n<code>12345_1.out</code><code>12345_2.out</code>\n\n\n<code>%J</code>\nJob ID plus stepid\n<pre><code>#SBATCH -o %J.out</code></pre>\n<code>12345.out</code>\n\n\n<code>%j</code>\nJob ID\n<pre><code>#SBATCH -o %j.out</code></pre>\n<code>12345.out</code>\n\n\n<code>%N</code>\nHostname of the first compute node allocated to the job\n<pre><code>#SBATCH -o %N.out</code></pre>\n<code>r1u11n1.out</code>\n\n\n<code>%u</code>\nUsername\n<pre><code>#SBATCH -o %u.out</code></pre>\n<code>netid.out</code>\n\n\n<code>%x</code>\nJob name\n<pre><code>#SBATCH --job-name=JobName#SBATCH -o %x.out</code></pre>\n<code>JobName.out</code>"},{"location":"running_jobs/batch_jobs/batch_directives/#additional-directives","title":"Additional Directives","text":"Command\nPurpose\n\n\n\n\n<pre><code>#SBATCH --job-name=JobName</code></pre>\nOptional: Specify a name for your job. This will not automatically affect the output filename.\n\n\n<pre><code>#SBATCH -e output_filename.err#SBATCH -o output_filename.out</code></pre>\nOptional: Specify output filename(s). If <code>-e</code> is missing, stdout and stderr will be combined.\n\n\n<pre><code>#SBATCH --open-mode=append</code></pre>\nOptional: Append your job's output to the specified output filename(s).\n\n\n<pre><code>#SBATCH --mail-type=BEGIN|END|FAIL|ALL</code></pre>\nOptional: Request email notifications. Beware of mail bombing yourself.\n\n\n<pre><code>#SBATCH --mail-user=email@address.xyz</code></pre>\nOptional: Specify email address. If this is missing, notifications will go to your UArizona email address by default.\n\n\n<pre><code>#SBATCH --export=VAR</code>\nOptional: Export a comma-delimited list of environment variables to a job.\n\n\n<pre><code>#SBATCH --export=all</code>\nOptional: Export your working environment to your job. This is the default.\n\n\n<pre><code>#SBATCH --export=none</code>\nOptional: Do not export working environment to your job."},{"location":"running_jobs/batch_jobs/batch_directives/#examples-and-explanations","title":"Examples and Explanations","text":"<p>The below examples are complete sections of Slurm directives that will produce valid requests. Other directives can be added (like output files), but they are not strictly necessary to submit a valid request. For simplicity, the Puma cluster is assumed when discussing memory and GPU resources. Note that these examples do not include the shebang <code>#!bin/bash</code> statement, which should be at the top of every Slurm script. Also, note that the order of directives does not matter.</p>\nSingle CPUSingle NodeSingle GPU NodeMulti-NodeHigh-Memory Node\n\n\n<pre><code>#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=01:00:00\n</code></pre>\n<p>This example requests one CPU on one node for one hour. Easy!</p>\n\n\n<pre><code>#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=10\n#SBATCH --time=01:00:00\n</code></pre>\n<p>10 CPUs are now requested. The default value of <code>mem-per-cpu</code> is assumed, therefore giving this job 50 GB of total memory. Specifying this value by including <code>#SBATCH --mem-per-cpu=5gb</code> will not change the behavior of the above request.</p>\n<p>The example below will produce an equivalent request as above:</p>\n<p><pre><code>#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --mem=50gb\n#SBATCH --time=01:00:00\n</code></pre>\nOn Puma, up to 94 CPUs or 470 GB of memory can be requested. </p>\n\n\n\n<p>NEW! July 31, 2024 Partitions update</p>\n<p>Beginning July 31, GPU jobs must use a GPU partition. See the partitions section at the top of this page for details.</p>\n\n<pre><code>#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=gpu_standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=10\n#SBATCH --time=01:00:00\n#SBATCH --gres=gpu:1\n</code></pre>\n<p>Note the <code>gres=gpu:1</code> option and <code>gpu_standard</code> partition.</p>\n\n\n<p>When requesting a multi-node job, up to 94 <code>--ntasks-per-node</code> can be requested on Puma. The numbers below are chosen for illustrative purposes and can be replaced with your choice, up to system limitations. It should be noted that there is no advantage to requesting multiple nodes when the total number of CPUs needed is less than or equal to the number of CPUs on one node.  </p>\n<pre><code>#SBATCH --job-name=Multi-Node-MPI-Job\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --ntasks=30\n#SBATCH --nodes=3\n#SBATCH --ntasks-per-node=10\n#SBATCH --time=01:00:00   \n</code></pre>\n\n\n<p>When requesting a high memory node, include both the <code>--mem-per-cpu</code> and <code>--constraint</code> directives.</p>\n<pre><code>#SBATCH --job-name=High-Mem-Job\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=94\n#SBATCH --mem-per-cpu=32gb\n#SBATCH --constraint=hi_mem\n#SBATCH --time=01:00:00   \n</code></pre>\n\n\n\n\n\n<ol>\n<li>\n<p>Groups and users are subject to limitations on resource usage. For more information, see job limits.\u00a0\u21a9</p>\n</li>\n</ol>"},{"location":"running_jobs/batch_jobs/environment_variables/","title":"Slurm Environment Variables","text":"<p>Every Slurm job has environment variables that are set by default. These can be used in a job to control job behavior, for example setting the names of output files or directories, setting CPU count for multithreaded processes, controlling input parameters, etc. </p> Variable Purpose Example Value <code>$SLURM_ARRAY_JOB_ID</code> Job array's parent ID <code>399124</code> <code>$SLURM_ARRAY_TASK_COUNT</code> Total number of subjobs in an array <code>4</code> <code>$SLURM_ARRAY_TASK_ID</code> Job index number (unique for each job in an array) <code>1</code> <code>$SLURM_ARRAY_TASK_MAX</code> Maximum index for the job array <code>7</code> <code>$SLURM_ARRAY_TASK_MIN</code> Minimum index for the job array <code>1</code> <code>$SLURM_ARRAY_TASK_STEP</code> Job array's index step size <code>2</code> <code>$SLURM_CLUSTER_NAME</code> Which cluster your job is running on <code>elgato</code> <code>$SLURM_CONF</code> Points to the Slurm configuration file <code>/var/spool/slurm/d/conf-cache/slurm.conf</code> <code>$SLURM_CPUS_ON_NODE</code> Number of CPUs allocated to target node <code>3</code> <code>$SLURM_GPUS_ON_NODE</code> Number of GPUs allocated to the target node <code>1</code> <code>$SLURM_GPUS_PER_NODE</code> Number of GPUs per node. Only set if <code>--gpus-per-node</code> is specified <code>1</code> <code>$SLURM_JOB_ACCOUNT</code> Account being charged <code>groupname</code> <code>$SLURM_JOB_GPUS</code> The global GPU IDs of the GPUs allocated to the job. Only set in batch and interactive jobs. <code>0</code> <code>$SLURM_JOB_ID</code> Your Slurm Job ID <code>399072</code> <code>$SLURM_JOB_CPUS_PER_NODE</code> Number of CPUs per node. This can be a list if there is more than one node allocated to the job. The list has the same order as <code>SLURM_JOB_NODELIST</code> <code>3,1</code> <code>$SLURM_JOB_NAME</code> The job's name <code>interactive</code> <code>$SLURM_JOB_NODELIST</code> The nodes that have been assigned to your job <code>gpu[73-74]</code> <code>$SLURM_JOB_NUM_NODES</code> The number of nodes allocated to the job <code>2</code> <code>$SLURM_JOB_PARTITION</code> The job's partition <code>standard</code> <code>$SLURM_JOB_QOS</code> The job's QOS/Partition <code>qos_standard_part</code> <code>$SLURM_JOB_USER</code> The username of the person who submitted the job <code>netid</code> <code>$SLURM_JOBID</code> Same as <code>SLURM_JOB_ID</code>, your Slurm Job ID <code>399072</code> <code>$SLURM_MEM_PER_CPU</code> The memory/CPU ratio allocated to the job <code>4096</code> <code>$SLURM_NNODES</code> Same as <code>SLURM_JOB_NUM_NODES</code> \u2013 the number of nodes allocated to the job <code>2</code> <code>$SLURM_NODELIST</code> Same as <code>SLURM_JOB_NODELIST</code>, The nodes that have been assigned to your job <code>gpu[73-74]</code> <code>$SLURM_NPROCS</code> The number of tasks allocated to your job <code>4</code> <code>$SLURM_NTASKS</code> Same as <code>SLURM_NPROCS</code>, the number of tasks allocated to your job <code>4</code> <code>$SLURM_SUBMIT_DIR</code> The directory where <code>sbatch</code> was used to submit the job <code>/home/u00/netid</code> <code>$SLURM_SUBMIT_HOST</code> The hostname where <code>sbatch</code> was used to submit the job <code>wentletrap.hpc.arizona.edu</code> <code>$SLURM_TASKS_PER_NODE</code> The number of tasks to be initiated on each node. This can be a list if there is more than one node allocated to the job. The list has the same order as <code>SLURM_JOB_NODELIST</code> <code>3,1</code> <code>$SLURM_WORKING_CLUSTER</code> Valid for interactive jobs, will be set with remote sibling cluster's IP address, port and RPC version so that any sruns will know which cluster to communicate with. <code>elgato:foo:0000:0000:000</code>"},{"location":"running_jobs/batch_jobs/gnu_parallel_jobs/","title":"Jobs with GNU Parallel","text":"<p>GNU Parallel is a powerful tool to submit multiple, independent but similar tasks efficiently within one job. In essence, it is similar to Slurm's array jobs and can be used both as an alternative to array jobs, or together with array jobs. GNU Parallel is available as a module on the HPC, and can be loaded with <code>module load parallel</code> in your batch script. The examples below show how you can use GNU Parallel by itself, and with array jobs.</p>"},{"location":"running_jobs/batch_jobs/gnu_parallel_jobs/#basic-gnu-parallel-job","title":"Basic GNU Parallel Job","text":"<p>The Slurm script for a basic GNU Parallel job will look like the following:</p> basic-parallel-job.slurm<pre><code>#!/bin/bash\n#SBATCH --ntasks=28\n#SBATCH --nodes=1\n#SBATCH --time=00:01:00\n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n\nmodule load parallel\nseq 1 100 | parallel 'DATE=$( date +\"%T\" ) &amp;&amp; sleep 0.{} &amp;&amp; echo \"Host: $HOSTNAME ; Date: $DATE; {}\"'\n</code></pre> <p>In this example, we're make use of a full node with GNU Parallel. The meat of the command lies here:</p> <pre><code>seq 1 100 | parallel 'DATE=$( date +\"%T\" ) &amp;&amp; sleep 0.{} &amp;&amp; echo \"Host: $HOSTNAME ; Date: $DATE; {}\"'\n</code></pre> <p><code>seq 1 100</code> generates a list between 1 and 100 (inclusive), and we pipe that into a parallel command which will generate one task per element (so 100 tasks). GNU Parallel will find the space on our node as it works through the relevant tasks. </p> <p>Inside the command:</p> <ul> <li><code>DATE=$( date + \"%T\" )</code> sets <code>DATE</code> so we can visualize tasks and when they're being executed.</li> <li><code>sleep 0.{}</code> forces each task to sleep for <code>0.n</code> seconds, where <code>n</code> is the input integer from the <code>seq</code> command. This means, for example, the 2nd task will wait longer than the 10th task, as can be seen in the output file. This is used to demonstrate that these tasks are being executed in parallel. </li> <li><code>echo \"HOST: $HOSTNAME ; Date: $DATE; {}\"</code> prints out information about the task. <code>{}</code> is piped input which, in this case, is an integer generated by <code>seq</code> between 1 and 100. </li> </ul> <p>You can submit the script with <code>sbatch basic-parallel-job.slurm</code>. Unlike an array job, this will produce only one output file: <pre><code>(ocelote) [netid@junonia ~]$ head slurm-74027.out \nHost: i10n18 ; Date: 16:45:55; 1\nHost: i10n18 ; Date: 16:45:55; 10\nHost: i10n18 ; Date: 16:45:55; 11\nHost: i10n18 ; Date: 16:45:55; 12\nHost: i10n18 ; Date: 16:45:55; 13\nHost: i10n18 ; Date: 16:45:55; 14\nHost: i10n18 ; Date: 16:45:55; 15\nHost: i10n18 ; Date: 16:45:55; 16\nHost: i10n18 ; Date: 16:45:55; 17\nHost: i10n18 ; Date: 16:45:55; 2\n</code></pre></p>"},{"location":"running_jobs/batch_jobs/gnu_parallel_jobs/#array-jobs-and-gnu-parallel","title":"Array Jobs and GNU Parallel","text":"<p>Sometimes you need to run a lot of jobs. More than can be reasonably accomplished using arrays since submitting thousands of jobs can be a problem for the system, and GNU Parallel can be challenging to make work in a multi-node environment. In this case, we can combine the forces of GNU Parallel and array jobs to distribute a chunk of tasks across multiple nodes where GNU Parallel will execute them.</p> <p>The Slurm script in this case will look like the following: array-and-parallel.slurm<pre><code>#!/bin/bash\n#SBATCH --job-name=Sample_Array_With_GNU_Parallel\n#SBATCH --ntasks=94\n#SBATCH --nodes=1                    \n#SBATCH --time=00:05:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array=1-2\n\nmodule load parallel\nBLOCK_SIZE=200\nseq $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE)) | parallel echo \"JOB ID: $SLURM_JOB_ID HOST NODE: $HOSTNAME EXAMPLE COMMAND: ./executable input_{}\"\n</code></pre></p> <p>The main difference with the basic example is setting up a \"block size\".  This is the number of tasks GNU Parallel will be executing in each subjob</p> <pre><code>BLOCK_SIZE=200\n</code></pre> <p>In this case, we're asking for 200 tasks per subjob and since we're submitting an array job, that totals 400 tasks. The array indices are used to differentiate tasks. <code>seq n m</code> generates a sequence of integers from <code>n</code> to <code>m</code> (inclusive).</p> <p><code>SLURM_ARRAY_TASK_ID</code> in this case is either 1 or 2, depending on the subjob, so combined with <code>BLOCK_SIZE</code>:</p> <p>Subjob 1: Generates numbers from 1 to 200</p> <pre><code>seq $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE))\n# Doing the math --&gt; seq 1*200-200+1 1*200 --&gt; seq 1 200\n</code></pre> <p>Subjob 2: Generates numbers from 201 to 400 <pre><code>seq $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE))\n# Doing the math --&gt; seq 2*200-200+1 2*200 --&gt; seq 201 400\n</code></pre></p> <p>You can submit the script with <code>sbatch array-and-parallel.slurm</code>. It will produce two output files, one for each job: <pre><code>(puma) [netid@junonia ~]$ ls *.out\nslurm-1693973_1.out  slurm-1693973_2.out\n</code></pre></p> <pre><code>(puma) [netid@junonia ~]$ head *.out\n==&gt; slurm-1693973_1.out &lt;==\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_1\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_2\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_3\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_4\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_5\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_6\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_7\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_8\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_9\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_10\n\n==&gt; slurm-1693973_2.out &lt;==\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_201\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_202\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_203\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_204\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_205\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_206\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_207\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_208\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_209\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_210\n</code></pre>"},{"location":"running_jobs/batch_jobs/intro/","title":"Intro to Batch","text":""},{"location":"running_jobs/batch_jobs/intro/#introduction-to-batch-jobs","title":"Introduction to Batch Jobs","text":""},{"location":"running_jobs/batch_jobs/intro/#what-are-batch-jobs","title":"What are Batch Jobs?","text":"<p>Batch jobs differ from interactive jobs and graphical jobs because they do not require user input while running. Instead, the user writes a script containing the instructions (code) that is sent to a compute node via the scheduler (Slurm). This allows your workflow to run automatically without you needing to be physically present. Here are a few benefits of using batch jobs:</p> <ol> <li> <p>No Need to Stay Logged In: You don\u2019t have to remain logged into the HPC system for your work to continue. This avoids potential issues like your terminal timing out, local internet disruptions, or your computer going to sleep\u2014all of which could prematurely end your analysis, especially for long-running jobs.</p> </li> <li> <p>Submit Many Jobs at Once: Some workflows require running hundreds or thousands of analyses. For example, you might want to run the same script with different input values multiple times. Doing this interactively could be cumbersome or even impossible. Batch jobs can easily handle this use case.</p> </li> </ol>"},{"location":"running_jobs/batch_jobs/intro/#batch-job-workflow-and-analogy","title":"Batch Job Workflow and Analogy","text":"<p>Think of a batch job like a researcher who wants something custom-made at a factory. There are a few steps they need to take:</p> <ol> <li>Get the Address: First, they need to know where the factory is so they can contact the right person to make their request.</li> <li>Provide Instructions: Next, they need to write instructions, or schematics, for the person who will do the manufacturing.</li> <li>Send the Instructions: Finally, they need to send these instructions to the factory so the builder can receive them and start working.</li> </ol> <p>We'll continue with this analogy, breaking down each step in more detail below.</p>"},{"location":"running_jobs/batch_jobs/intro/#batch-scripts","title":"Batch Scripts","text":"<p>When you want to run analyses in batch mode, you need to inform the system of two things:</p> <ol> <li> <p>What Resources You Need: This includes specifying the number of CPUs, the amount of memory, GPUs, and other resources necessary to run your work. These instructions guide the system in selecting the appropriate compute resources and target compute nodes for your workflow. Continuing with our factory analogy, these resources can be thought of as the postal address on the outside of the envelope.</p> </li> <li> <p>The Instructions to Execute: This is a list of commands that the compute node will follow once your workflow starts. These commands are written in Bash and include everything you would normally type on the command line if you were running your work interactively. For example, you would <code>cd</code> to the relevant working directory, <code>module load</code> any required software, <code>source</code> virtual environments, and so forth. In our analogy, these commands are like the schematics inside the envelope that you're sending to the factory.</p> </li> </ol>"},{"location":"running_jobs/batch_jobs/intro/#batch-script-structure","title":"Batch Script Structure","text":"<p>A batch script is a text file that is written with three sections:</p> <pre>#!/bin/bash</pre> <pre>#SBATCH --option=value</pre> <pre>[code here]</pre> <ol> <li>The \"shebang\" will always be the line <code>#!/bin/bash</code>. This tells the system to interpret your file as a Bash script. Our HPC systems use Bash for all our environments, so it should be used in your scripts to get the most consistent, predictable results.</li> <li>The directives section will have multiple lines, all of which start with <code>#SBATCH</code>. These lines are interpreted as directives by the scheduler and are how you request resources on the compute nodes, set your output filenames, set your job name, request emails, etc. A list of directives is shown in Batch Directives.</li> <li>The code section in your script is a set of bash commands that tells the system how to run your analyses. This includes any module load commands you'd need to run to access your software, software commands to execute your analyses, directory changes, etc. </li> </ol> <p>An example batch script might look like the following:</p> <pre><code>#!/bin/bash# --------------------\n### Directives Section\n# --------------------\n#SBATCH --job-name=hello_world\n#SBATCH --account=&lt;your_group&gt;\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:01:00# --------------------\n### Code Section\n# --------------------\nmodule load python/3.9\ncd ~/hello_world\npython3 hello_world.py</code></pre> <p>hello_world.py</p> <p>In the example above, we'll assume the hello_world.py script that's being executed is located in the user's home directory with the contents <pre><code>#!/usr/bin/env python3\nprint(\"Hello world!\")\n</code></pre></p> <p>Finding your group name</p> <p>Not sure what your group name is? Check out our allocations documentation.</p>"},{"location":"running_jobs/batch_jobs/intro/#submit-your-job","title":"Submit Your Job","text":"<p>After writing your batch script, the next step is to submit it to the scheduler. The scheduler is software designed to coordinate jobs efficiently across the system. It reads the batch directives from the second section of your batch script (described above) and uses those resource requests to determine where to send your workflow. Continuing with our factory analogy, the scheduler acts like the postman, serving as the intermediary between you and the compute nodes (the factory) and handling all communication and resource scheduling.</p> <p>To submit your batch job to the scheduler, use the <code>sbatch</code> command. This will return a Job ID, which works like a tracking number when you send a package via the post office. You can use your Job ID to monitor your analysis as it runs and retrieve resource usage data once it\u2019s completed.</p> <p>An example of using <code>sbatch</code> is shown below:</p> <pre><code>(puma) [netid@wentletrap ~]$ sbatch hello_world.slurm \nSubmitted batch job 10233657\n</code></pre> <p>We can then check on its status using the command <code>squeue</code>.  <pre><code>(puma) [netid@wentletrap ~]$ squeue --job 10233657\n        JOBID PARTITION   NAME        PRIORITY   USER    ACCOUNT ST CPUS MIN_M NOD  NODELIST(REASON) TIME_LEFT\n     10233657 standard    hello_worl      5001   netid   hpcteam PD    1    5G   1        (Priority) 1:00\n</code></pre> The <code>ST</code> shown above stands for \"State\" and indicates that the job is currently pending (<code>PD</code>). Once the job starts running, the state will change to <code>R</code>. When the job is complete, <code>squeue</code> will come back blank. </p> <p>The amount of time your job spends waiting before it starts running often is determined by various factors which include:</p> <ol> <li> <p>Job Size. This includes the number of CPUs, nodes, GPUs, etc. The more resources you request, the longer it may take before your job starts running.</p> </li> <li> <p>Job Duration. Jobs with shorter runtimes will often start faster than long-running jobs. </p> </li> <li> <p>Node Type. Our clusters have many standard nodes, fewer GPU nodes, and very few high memory nodes. The high memory nodes in particular may have very long wait times. If you do not need a lot of memory for your job (e.g., on Puma, less than 470 GB), it may be more efficient to run your work on a standard node. If your job has been stuck in the queue for a very long time, ensure that you have not accidentally targeted a high memory node. </p> </li> <li> <p>Cluster Usage. The more jobs running on the cluster, the longer the wait times may be. To check how busy the cluster is, try running the command <code>nodes-busy</code>.</p> </li> <li> <p>Maintenance. Quarterly maintenance cycles impact queue times. We will send announcements typically a week in advance of any planned maintenance cycles and will include announcement banners in our documentation. See our maintenance section for more information. </p> </li> </ol>"},{"location":"running_jobs/batch_jobs/intro/#retrieve-your-results","title":"Retrieve Your Results","text":"<p>Once your job starts running, a file will be generated in the directory where you submitted your batch script. This file logs the job's standard output (stdout) as it runs\u2014essentially, what would have been printed to the terminal if you had run the job interactively. The file is updated in real-time, so you can log into the cluster at any time to check your job's progress. By default, this file is named <code>slurm-&lt;jobid&gt;.out</code>, but you can customize the filename using batch directives if desired. </p> <p>Let\u2019s look at the output for our example job:</p> <p><pre><code>(puma) [netid@wentletrap ~]$ cat slurm-10233657.out \nHello world!\n</code></pre> As we can see, the script ran successfully, and Hello world! was printed to the terminal.</p>"},{"location":"running_jobs/batch_jobs/intro/#conclusion","title":"Conclusion","text":"<p>Batch jobs are a powerful way to automate your workflows on the HPC system, allowing you to efficiently use resources and manage long-running tasks without needing to stay logged in. By understanding how to write and submit batch scripts, you can run complex analyses, monitor their progress, and retrieve their results.</p> <p>If you're just getting started, here are a few additional tips:</p> <ol> <li>Start Small: Test your scripts with smaller jobs to ensure everything is working as expected before scaling up to larger, more resource-intensive tasks.</li> <li>Explore More: Take some time to explore additional batch directives and experiment with different resource requests to optimize your jobs.</li> <li>Ask for Help: Don\u2019t hesitate to reach out to our HPC support team if you have any questions or run into issues. We're here to help you get the most out of the system.</li> </ol>"},{"location":"running_jobs/batch_jobs/job_dependencies/","title":"SLURM Job Dependencies Example","text":"<p>Click here to download the example</p>"},{"location":"running_jobs/batch_jobs/job_dependencies/#overview","title":"Overview","text":"<p>Sometimes projects need to be split up into multiple parts where each step is dependent on the step (or several steps) that came before. SLURM dependencies are a way to automate this process. </p> <p>In this example, we'll create a number of three-dimensional plots using Python and will combine them into a gif as the last step. A job dependency is a good solution in this case since the job that creates the gif is dependent on all the images being present.</p>"},{"location":"running_jobs/batch_jobs/job_dependencies/#data-structure","title":"Data structure","text":"<p>We'll try to keep things in order by partitioning our data, output, and images in distinct directories. These directories and files can be downloaded by clicking the button at the top of the page.</p> <pre><code>(elgato) [user@wentletrap volcano]$ tree\n.\n\u251c\u2500\u2500 create_gif.slurm\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano.csv\n\u251c\u2500\u2500 generate_frames.slurm\n\u251c\u2500\u2500 images\n\u251c\u2500\u2500 output\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 archives\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurm_files\n\u251c\u2500\u2500 submit-gif-job\n\u2514\u2500\u2500 volcano.py\n</code></pre>"},{"location":"running_jobs/batch_jobs/job_dependencies/#scripts","title":"Scripts","text":""},{"location":"running_jobs/batch_jobs/job_dependencies/#python-script","title":"Python script","text":"<p>The Python example script was pulled and modified from the Python graph gallery and the CSV file used to generate the image was downloaded from: https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/volcano.csv</p> <p>Below you'll notice one modification to the original script: <code>n = int(sys.argv[1])</code>. We're going to execute this in an array job and will be importing the array indices (integers <code>n</code> where <code>1 \u2264 n \u2264 360</code>) into this script as arguments so that we can manipulate the viewing angle (<code>ax.view_init(30, 45 + n)</code>). Each frame will be slightly different and, when combined into a gif, will allow us to execute a full rotation of the 3D volcano plot. </p> <pre><code>#!/usr/bin/env python3\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport sys\n\nn = int(sys.argv[1]) # &lt;- We'll import an array index to rotate our image\n\n# Original example from: https://www.python-graph-gallery.com/3d/\n# CSV available from   : https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/volcano.csv\ndata = pd.read_csv(\"data/volcano.csv\")\n\n# Transform it to a long format\ndf=data.unstack().reset_index()\ndf.columns=[\"X\",\"Y\",\"Z\"]\n\n# And transform the old column name in something numeric\ndf['X']=pd.Categorical(df['X'])\ndf['X']=df['X'].cat.codes\n\n# Make the plot\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.set_axis_off()\nax.plot_trisurf(df['Y'], df['X'], df['Z'], cmap=plt.cm.viridis, linewidth=0.2)\nax.view_init(30, 45 + n)\nplt.savefig('images/image%s.png'%n,format='png',transparent=False)\n</code></pre>"},{"location":"running_jobs/batch_jobs/job_dependencies/#slurm-script-to-generate-images","title":"Slurm Script to Generate Images","text":"<p>This is the job where we will generate all of our images. In each step, we will pass our array index to our python script to determine the viewing angle of our plot. </p> <p>Script: <code>generate_frames.slurm</code></p> <pre><code>#!/bin/bash\n\n#SBATCH --account=hpcteam\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:05:00\n#SBATCH --job-name=generate_frames\n#SBATCH -o output/slurm_files/%x-%A.out\n#SBATCH -e output/slurm_files/%x-%A.err\n#SBATCH --open-mode=append\n#SBATCH --array=1-360\n\npython3 volcano.py $SLURM_ARRAY_TASK_ID\n</code></pre>"},{"location":"running_jobs/batch_jobs/job_dependencies/#slurm-script-to-combine-frames-into-gif","title":"SLURM script to combine frames into gif","text":"<p>Once each frame has been generated, we'll use ffmpeg to combine our images into a gif and will clean up our workspace. The bash script shown in the next section is what will ensure that this script isn't run until the array has completed. </p> <p>Script: <code>create_gif.slurm</code></p> <pre><code>#!/bin/bash\n\n#SBATCH --account=hpcteam\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:10:00\n#SBATCH --job-name=make_gif\n#SBATCH -o output/slurm_files/%x-%j.out\n#SBATCH -e output/slurm_files/%x-%j.err\n\nmodule load ffmpeg \nffmpeg -framerate 25 -i $PWD/images/image%d.png -r 30  -b 5000k volcano.mp4\nffmpeg -i volcano.mp4 -loop 0 -vf scale=400:240 volcano.gif\nrm volcano.mp4\ncd images\nDATE_FORMAT=$(date +%m-%d-%Y.%H:%M:%S)\ntar czvf volcano-images-${DATE_FORMAT}.tar.gz image*.png\nmv *tar.gz ../output/archives\nrm -rf ./*.png\n</code></pre>"},{"location":"running_jobs/batch_jobs/job_dependencies/#script-to-automate-job-submissions","title":"Script to Automate Job Submissions","text":"<p>This simple bash script is what implements the SLURM job dependency magic. Each step is described in detail below.</p> <p>Script: <code>submit-gif-job</code></p> <pre><code>#!/bin/bash\n\nprintf \"Submitting job to generate images\\n\"\njobid=$(sbatch --parsable generate_frames.slurm)\n\nprintf \"Job submitted with ID: $jobid\\n\\n\"\n\nprintf \"Submitting job dependency. Combines images into a gif\\n\"\nsbatch --dependency=afterany:$jobid create_gif.slurm \n</code></pre>"},{"location":"running_jobs/batch_jobs/job_dependencies/#step-by-step","title":"Step-by-step:","text":"<p>1) <code>jobid=$(sbatch --parsable generate_frames.slurm)</code></p> <p>In this case, we're capturing the job ID output from our array submission. Typically, when you submit a SLURM job without arguments, you get back something that looks like:  <pre><code>(elgato) [user@wentletrap ~]$ sbatch example.slurm \nSubmitted batch job 448243\n</code></pre> The parsable option is what reduces this to simply the job ID and allows us to easily capture it: <pre><code>(elgato) [user@wentletrap ~]$ sbatch --parsable example.slurm \n448244\n</code></pre> As a general comment, when you run something like: <pre><code>VAR=$(command)\n</code></pre> You are running <code>command</code> and setting the variable <code>VAR</code> to the output. In the specifc case of our bash script, we've set the bash variable <code>jobid</code> to the output of our <code>sbatch --parsable</code> command. </p> <p>2) <code>sbatch --dependency=afterany:$jobid create_gif.slurm</code></p> <p>Now that we have the Job ID, we'll submit the next job with a dependency flag: <code>--dependency=afterany:$jobid</code>. </p> <p>The <code>dependency</code> option tells the scheduler that this job should not be run until the job with Job ID <code>$jobid</code> has completed. The <code>afterany</code> specifies that the exit status of the previous job does not matter. Other options are <code>afterok</code> (meaning only execute the dependent job if the previous job ended successfully) or <code>afternotok</code> (meaning only execute if the previous job terminated abnormally, e.g. was cancelled or failed). You might consider setting up multiple job dependencies that depend on the previous job's exit status. </p>"},{"location":"running_jobs/batch_jobs/job_dependencies/#submitting-the-jobs","title":"Submitting the Jobs","text":"<p>Once we've gotten everything set up, it's time to execute our workflow. We can check our jobs once we've run our bash script. In this case, while the array job used to generate the different image frames is running, the <code>make_gif</code> job will sit in queue with the reason <code>(Dependency)</code> indicating that it is waiting to run until its dependency has been satisfied. </p> <pre><code>(elgato) [user@wentletrap volcano]$ bash submit-gif-job \nSubmitting job to generate images\nJob submitted with ID: 447878\n\nSubmitting job dependency. Combines images into a gif file\nSubmitted batch job 447879\n\n(elgato) [user@wentletrap volcano]$ squeue --user user\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n    447878_[9-360]  standard generate     user PD       0:00      1 (None)\n            447879  standard make_gif     user PD       0:00      1 (Dependency)\n          447878_1  standard generate     user  R       0:02      1 cpu16\n          447878_2  standard generate     user  R       0:02      1 cpu37\n          447878_3  standard generate     user  R       0:02      1 cpu37\n          447878_4  standard generate     user  R       0:02      1 cpu37\n          447878_5  standard generate     user  R       0:02      1 cpu37\n          447878_6  standard generate     user  R       0:02      1 cpu37\n          447878_7  standard generate     user  R       0:02      1 cpu37\n          447878_8  standard generate     user  R       0:02      1 cpu37\n</code></pre> <p>Once the job has completed, you should see something that looks like the following structure with output files: <pre><code>(elgato) [user@wentletrap volcano]$ tree\n.\n\u251c\u2500\u2500 create_gif.slurm\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano.csv\n\u251c\u2500\u2500 generate_frames.slurm\n\u251c\u2500\u2500 images\n\u251c\u2500\u2500 output\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 archives\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano-images-10-25-2022.12:52:19.tar.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano.gif\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurm_files\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 generate_frames-447878.err\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 generate_frames-447878.out\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 make_gif-447879.err\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 make_gif-447879.out\n\u251c\u2500\u2500 submit-gif-job\n\u2514\u2500\u2500 volcano.py\n\n6 directories, 11 files\n</code></pre></p>"},{"location":"running_jobs/batch_jobs/job_dependencies/#output","title":"Output","text":"<p>If everything is successful, there should be a gif of a rotating volcano under <code>./output/gifs/volcano.gif</code></p> <p></p> <p> </p>"},{"location":"running_jobs/batch_jobs/submitting_jobs/","title":"Submitting Batch Jobs","text":""},{"location":"running_jobs/batch_jobs/submitting_jobs/#submitting-a-job","title":"Submitting a Job","text":"<p>To submit a batch job to the scheduler, use the command <code>sbatch</code>. This will place your job in line for execution and will return a job ID that you can use to track and monitor your job. </p> <p>As an example:</p> <pre><code>[netid@gpu66 hello_world]$ sbatch hello_world.slurm\nSubmitted batch job 807387\n[netid@gpu66 hello_world]$ squeue --job 807387\n             JOBID PARTITION     NAME     USER ST       TIME  NODES\n            807387  standard hello_wo    netid PD       0:06      1 \n</code></pre> <p>The command <code>squeue</code> gives us detailed information about our batch jobs while they're in queue or running. Under the heading <code>ST</code> you can check the state of your job. In this case, it's pending (<code>PD</code>) which means it's waiting in line with other jobs. Once the job starts running, it's state will change to <code>R</code>, and when the job has completed running, <code>squeue</code> will return a blank line. </p>"},{"location":"running_jobs/batch_jobs/submitting_jobs/#submitting-multiple-jobs","title":"Submitting Multiple Jobs","text":"<p>Frequently, users need to submit multiple, related jobs. It may be tempting to do this using a bash loop, but there are several drawbacks to this method, primarily that it can affect the performance of the job scheduling software.  </p> <p>Use Arrays instead of Loops for large numbers of jobs</p> <p>Users submitting large numbers (&gt; 100s) of jobs using loops will be contacted and asked to adjust their workflows. Requests that persistently affect the performance of the job scheduler will be cancelled by HPC infrastructure.</p> <p>The best way to submit related jobs is to use job arrays. Jobs arrays allow users to submit multiple related jobs using a single script and single <code>sbatch</code> command. Each task within the array can have its own unique input parameters, making it ideal for running batch jobs with varied inputs or executing repetitive tasks efficiently. See Array Jobs for specifics on how to submit these sorts of workflows.</p>"},{"location":"running_jobs/batch_jobs/submitting_jobs/#output-files","title":"Output Files","text":"<p>Once your job completes, you should see an output file in the directory where you submitted the batch script. This output file captures anything that would have been printed to the terminal if you had run it interactively. By default, output filenames will be <code>slurm-&lt;jobid&gt;.out</code>(1). In the example above, this translates to filename <code>slurm-807387.out</code>. </p> <ol> <li>Custom output filenames can be set with batch directives.</li> </ol>"},{"location":"running_jobs/cpus_and_memory/","title":"CPUs and Memory","text":""},{"location":"running_jobs/cpus_and_memory/#cpu-and-memory-correlation","title":"CPU and Memory Correlation","text":"<p>Before submitting your job to the scheduler, it's important to know that the number of CPUs you allocate to your job determines the amount of memory you receive. </p> <p>Each cluster has a fixed amount of memory per CPU based on the node type. Accepted values by cluster and node type are listed below:</p> Cluster Standard Node High-Memory Node GPU Node Puma 5 GB 32 GB 5 GB Ocelote 6 GB 41 GB 8 GB El Gato 4 GB - - <p>For example, using the table above we can see on Puma standard nodes you get 5 GB for each CPU you request. This means a standard job using 4 CPUs gets 5 GB/CPU \u00d7 4 CPUs = 20 GB of total memory.</p> <p>The video below shows the relationship between memory and CPUs, specifically looking at one of our Puma nodes. </p> <p> <p></p>"},{"location":"running_jobs/cpus_and_memory/#determining-job-resources","title":"Determining Job Resources","text":"<p>The following flowchart describes the process of determining the amount of memory and number of CPUs to allocate to a job. For simplicity, this shorthand will be used:</p> <ul> <li>N: number of CPUs desired</li> <li>M: total memory desired</li> <li>MpC: the default value of memory per CPU for fixed cluster and node type as listed in the table above</li> </ul> <p>If a decimal value is encountered, round up in all cases. </p> <pre><code>graph LR\n   A[Is my job CPU&lt;br&gt;or Memory limited?] \n   B[\"`set CPUs=**N**`\"]\n   C[\"`do not specify\n   memory or mem/cpu`\"]\n   D[\"`Set CPUs=**M**/**MpC**`\"]\n   E[\"`Set mem/CPU=**MpC**`\"]\n   Z[Done]\n   A --&gt;|CPU| B --&gt; C --&gt; Z\n   A --&gt;|mem| D -.-&gt; E --&gt; Z\n   D --&gt; Z</code></pre> <p>The dotted line above indicates that setting mem/CPU in your job is not generally necessary. If you are requesting a standard node, this value is set for you by the scheduler. The only times you will need to set this value is:</p> <ol> <li>If you're requesting a non-standard node (e.g. a high memory or Ocelote GPU node)</li> <li>If you're requesting an OnDemand application session. There is a field where you will fill in your mem/CPU requirement. </li> </ol> <p>Note that there is no deterministic method of finding the exact amount of memory needed by a job in advance. A general rule of thumb is to overestimate it slightly and then scale down based on previous runs. Significant overestimation, however, can lead to inefficiency of system resources and unnecessary expenditure of CPU time allocations. </p>"},{"location":"running_jobs/cpus_and_memory/#things-to-watch-out-for","title":"Things to Watch out for","text":"<p>Be careful when requesting memory and memory per CPU. Note that if you request invalid mem/cpu values, unpredictable results may occur:</p> <ul> <li> <p>Memory and CPU Mismatches</p> <p>In batch scripts, if you request <code>--memory</code> instead of <code>--mem-per-cpu</code>, the scheduler will automatically increase your CPU allocation to align with your memory requirements in case of a mismatch. For instance, if you request one CPU and 50 GB of total memory, the scheduler will adjust your resource requirements by allocating 10 CPUs to match your memory request.</p> </li> <li> <p>Invalid Mem/CPU Options</p> <p>If you request a mem/CPU value that isn't valid, the scheduler won't outright reject your job. Instead, it will attempt to accommodate your request. This could involve your job being moved to a high memory node if you ask for more than the standard ratio. However, high memory nodes usually have considerably longer wait times compared to standard nodes, potentially resulting in a longer queue time than anticipated.</p> <p>Alternatively, you might end up with less memory allocated than you expected. For instance, there aren't any machines with a memory ratio exceeding 41 GB/CPU. Therefore, if you request 100 GB/CPU, your job will still be constrained by the physical limits of available memory.</p> </li> </ul>"},{"location":"running_jobs/example_batch_jobs/","title":"Index","text":"<p>foo</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/","title":"Array Job With Text Filenames","text":"<p>Click here to download example files</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#what-problem-does-this-help-fix","title":"What problem does this help fix?","text":"<p>If you want to run multiple jobs where each opens a different file to analyze but the naming scheme isn't conducive to automating the process using simple array indices (i.e., 1.txt, 2.txt, ...)</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#example","title":"Example","text":""},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#submission-script","title":"Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=Array-Read-Filenames\n#SBATCH --ntasks=1\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array=1-4\n\nCurrentFile=\"$( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputFiles )\"\necho \"JOB NAME: $SLURM_JOB_NAME, JOB ID: $SLURM_JOB_ID, EXAMPLE COMMAND: ./executable -o output${SLURM_ARRAY_TASK_ID} ${CurrentFile}\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#input-file","title":"Input File","text":"<p>For this example, you'll want to have a file called InputFiles in your working directory. This will contain one filename per line. Contents: <pre><code>SRR2309587.fastq\nSRR3050489.fastq\nSRR305356.fastq\nSRR305p0982.fastq\n</code></pre></p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#script-breakdown","title":"Script Breakdown","text":"<p>For each of the four subjobs, we'll make use of <code>SLURM_ARRAY_TASK_ID</code> to pull the line number (line numbers 1 to 4) from InputFiles:</p> <pre><code>CurrentFile=\"$( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputFiles )\"\n</code></pre> <p>We will print a sample command that includes our filename to verify that everything is working as expected for demonstration purposes:</p> <pre><code>echo \"JOB NAME: $SLURM_JOB_NAME, JOB ID: $SLURM_JOB_ID, EXAMPLE COMMAND: ./executable -o output${SLURM_ARRAY_TASK_ID} ${CurrentFile}\"\n</code></pre> <p>To generate your own InputFile, you can either manually add your filenames or can automate the process, for example if you have all your files in a single location:</p> <pre><code>$ ls *fastq &gt; InputFiles\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#script-submission-command","title":"Script Submission Command:","text":"<pre><code>(puma) [netid@junonia ~]$ sbatch Array-Read-Filenames.slurm \nSubmitted batch job 1694071\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#output-files","title":"Output Files","text":"<p>Each of the subjobs in the array will output its own file of the form <code>slurm-&lt;job_id&gt;_&lt;array_id&gt;.out</code> as seen below:</p> <pre><code>(puma) [netid@junonia ~]$ ls *.out\nslurm-1694071_1.out  slurm-1694071_2.out  slurm-1694071_3.out\nslurm-1694071_4.out\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_filenames/#file-contents","title":"File Contents:","text":"<pre><code>(puma) [netid@junonia ~]$ cat *.out | grep fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694072, EXAMPLE COMMAND: ./executable -o output1 SRR2309587.fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694073, EXAMPLE COMMAND: ./executable -o output2 SRR3050489.fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694074, EXAMPLE COMMAND: ./executable -o output3 SRR305356.fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694071, EXAMPLE COMMAND: ./executable -o output4 SRR305p0982.fastq\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/","title":"Array Jobs With Multiple Input Parameters","text":"<p>Click here to download example files</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#example","title":"Example","text":"<p>This script demonstrates how to feed parameters to different subjobs in an array by pulling them from an input file, e.g.:</p> <ul> <li>Job 1: <code>./executable job1_variable1 job1_variable2 job1_variable3</code></li> <li>Job 2: <code>./executable job2_variable1 job2_variable2 job2_variable3</code> etc.</li> </ul>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#submission-script","title":"Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=Array-Read-Parameters\n#SBATCH --ntasks=1\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array=1-10\n\nread first_parameter second_parameter third_parameter &lt;&lt;&lt; $( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputParameters )\necho \"Job ID: $SLURM_JOB_ID ; Host Node : $HOSTNAME ; Sample Command : ./executable $first_parameter $second_parameter $third_parameter\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#input-file","title":"Input file","text":"<pre><code>job1_param1 job1_param2 job1_param3\njob2_param1 job2_param2 job2_param3\njob3_param1 job3_param2 job3_param3\njob4_param1 job4_param2 job4_param3\njob5_param1 job5_param2 job5_param3\njob6_param1 job6_param2 job6_param3\njob7_param1 job7_param2 job7_param3\njob8_param1 job8_param2 job8_param3\njob9_param1 job9_param2 job9_param3\njob10_param1 job10_param2 job10_param3\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#script-breakdown","title":"Script Breakdown","text":"<p>The line number that corresponds with the job's <code>SLURM_ARRAY_TASK_ID</code> is read in and parsed to extract the input parameters. The parameters here should be space-delimited (of course, you can modify your script to change these specifications). There are three parameters per line that are assigned to the variables on the left:</p> <pre><code>read first_parameter second_parameter third_parameter &lt;&lt;&lt; $( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputParameters )\n</code></pre> <p>A sample command is printed along with job information for demonstration purposes:</p> <pre><code>echo \"Job ID: $SLURM_JOB_ID ; Host Node : $HOSTNAME ; Sample Command : ./executable $first_parameter $second_parameter $third_parameter\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#script-submission-command","title":"Script Submission Command","text":"<pre><code>(puma) [netid@junonia ~]$ sbatch Array-Read-Parameters.slurm \nSubmitted batch job 1694093\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#output-files","title":"Output Files","text":"<pre><code>(puma) [netid@junonia ~]$ ls *.out\nslurm-1694093_10.out  slurm-1694093_1.out  slurm-1694093_2.out\nslurm-1694093_3.out   slurm-1694093_4.out  slurm-1694093_5.out\nslurm-1694093_6.out   slurm-1694093_7.out  slurm-1694093_8.out\nslurm-1694093_9.out\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/array_read_parameters/#file-contents","title":"File Contents","text":"<pre><code>(puma) [netid@junonia ~]$ cat *.out | grep param\nJob ID: 1694093 ; Host Node : r1u03n2 ; Sample Command : ./executable job10_param1 job10_param2 job10_param3\nJob ID: 1694094 ; Host Node : r1u03n1 ; Sample Command : ./executable job1_param1 job1_param2 job1_param3\nJob ID: 1694095 ; Host Node : r1u03n1 ; Sample Command : ./executable job2_param1 job2_param2 job2_param3\nJob ID: 1694096 ; Host Node : r1u03n1 ; Sample Command : ./executable job3_param1 job3_param2 job3_param3\nJob ID: 1694097 ; Host Node : r1u03n1 ; Sample Command : ./executable job4_param1 job4_param2 job4_param3\nJob ID: 1694098 ; Host Node : r1u03n1 ; Sample Command : ./executable job5_param1 job5_param2 job5_param3\nJob ID: 1694099 ; Host Node : r1u03n1 ; Sample Command : ./executable job6_param1 job6_param2 job6_param3\nJob ID: 1694100 ; Host Node : r1u03n1 ; Sample Command : ./executable job7_param1 job7_param2 job7_param3\nJob ID: 1694101 ; Host Node : r1u03n2 ; Sample Command : ./executable job8_param1 job8_param2 job8_param3\nJob ID: 1694102 ; Host Node : r1u03n2 ; Sample Command : ./executable job9_param1 job9_param2 job9_param3\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/","title":"Basic Array Job","text":"<p>Click to download example files</p> <p>Array jobs are used to execute the same script multiple times with different input.</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#what-problem-does-this-help-fix","title":"What problem does this help fix?","text":"<p>To execute multiple analyses, a user may be tempted to submit jobs with a scripted loop, e.g.: <pre><code>for i in $( seq 1 10 ); do sbatch script.slurm &lt;submission options&gt; ; done\n</code></pre></p> <p>For a large number of analyses, this isn't a good solution because it submits too many jobs too quickly and overloads the scheduler. Instead, an array job can be used to achieve the same ends.</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#example-submission-script","title":"Example Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array 1-5\n\necho \"./sample_command input_file_${SLURM_ARRAY_TASK_ID}.in\"\n ```\n\n## Script Breakdown\n\nWhat differentiates the script above from standard submissions is the ```--array``` directive. This is what tells Slurm  that you're submitting an array. Following this flag, you will specify the number of jobs you wish to run. In this case, we're running 5:\n\n```bash\n#SBATCH --array 1-5\n</code></pre> <p>Each job in the array has its own associated environment variable <code>SLURM_ARRARY_TASK_ID</code> that can be used to differentiate subjobs. To demonstrate how we can use each of these to read in different input files, we'll print a sample command:</p> <pre><code>echo \"./sample_command input_file_${SLURM_ARRAY_TASK_ID}.in\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#script-submission","title":"Script Submission","text":"<pre><code>(ocelote) [netid@junonia ~]$ sbatch basic_array_job.slurm \nSubmitted batch job 73958\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#output-files","title":"Output Files","text":"<p>Each of the subjobs in the array will produce its own output file of the form <code>slurm_jobid_arrayid.out</code> as seen below:</p> <pre><code>(ocelote) [netid@junonia ~]$ ls\nslurm-73958_1.out  slurm-73958_2.out      slurm-73958_3.out  slurm-73958_4.out\nslurm-73958_5.out  basic_array_job.slurm\n</code></pre> <p>For more information on naming Bash files, see our online documentation</p>"},{"location":"running_jobs/example_batch_jobs/array_jobs/basic_array/#file-contents","title":"File Contents:","text":"<p>Below is a concatenation of the job's output files. Notice how the array indices function to differentiate the input files in the sample command:</p> <pre><code>(ocelote) [netid@junonia ~]$ cat slurm-73958_* | grep sample\n./sample_command input_file_1.in\n./sample_command input_file_2.in\n./sample_command input_file_3.in\n./sample_command input_file_4.in\n./sample_command input_file_5.in\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/pipefail/","title":"Using a Pipefail","text":"<p>This script uses a pipefail to kill a SLURM job in the event of a failure at any point in the pipeline rather than continuing on to the next step.</p>"},{"location":"running_jobs/example_batch_jobs/general_examples/pipefail/#example-submission-script","title":"Example Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --account=YOUR_GROUP\n#SBATCH --partition=standard\n#SBATCH --ntasks=1\n#SBATCH --nodes=1\n#SBATCH --time=00:01:00\n\nsingularity exec --nv /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import foo\"\n\nset -oe pipefail # &lt;- kills the batch script on the next error it encounters\nsingularity exec --nv /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import bar\"\nsingularity exec --nv /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import baz\"\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/pipefail/#job-submission","title":"Job Submission","text":"<pre><code>[user@cpu37 fail_test]$ sbatch pipefail.slurm\nSubmitted batch job 361837\n</code></pre>"},{"location":"running_jobs/example_batch_jobs/general_examples/pipefail/#output","title":"Output","text":"<p>Notice that before the pipefail is set, the script moves on to subsequent commands (trying to import bar). After the pipefail is set, the job exits when it can't import bar and never tries to import baz. </p> <pre><code>[user@cpu37 fail_test]$ cat slurm-361837.out\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'foo'\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'bar'\n</code></pre>"},{"location":"running_jobs/interactive_jobs/","title":"Interactive Jobs","text":""},{"location":"running_jobs/interactive_jobs/#overview","title":"Overview","text":"<p>Interactive sessions are a way to gain access to a compute node from the command line. This is useful for checking and using available software modules, testing submission scripts, debugging code, compiling software, and running programs in real time. </p> <p>The term \"interactive session\" in this context refers to jobs run from within the command line on a terminal client. Opening a terminal in an interactive graphical desktop is also equivalent, but these sessions are fixed to the resources allocated to that Open OnDemand (OOD) session. As you'll see below, one has more control over their resources when requesting an interactive session via SSH in a terminal client.</p>"},{"location":"running_jobs/interactive_jobs/#clusters","title":"Clusters","text":"<p>An interactive session can be requested on any of our three clusters: El Gato, Ocelote, and Puma. </p> <p>Ocelote and El Gato share the same operating system (CentOS 7), system libraries, and software modules. El Gato typically has shorter wait times, so if you're compiling software or running small test jobs for workflows to run on either El Gato or Ocelote, it may be advantageous to request a session on El Gato for more immediate access. </p> <p>Puma runs a newer operating system (Rocky Linux 9), has newer system libraries, and its own software modules. If you are using Puma for your production work, you will want to stick to requesting interactive sessions on Puma for testing and compiling. Workflows may not be transferrable between the older clusters and Puma, so make sure to check which software ecosystem you're using to ensure the most predictable results. </p>"},{"location":"running_jobs/interactive_jobs/#how-to-request-an-interactive-session","title":"How to Request an Interactive Session","text":""},{"location":"running_jobs/interactive_jobs/#the-interactive-command","title":"The <code>interactive</code> Command","text":"<p>We have a built-in shortcut command that will allow you to quickly and easily request a session by simply entering: <code>interactive</code></p> <p>The <code>interactive</code> command is essentially a convenient wrapper for a Slurm command called <code>salloc</code>. This can be thought of as similar to the <code>sbatch</code> command, but for interactive jobs rather than batch jobs. When you request a session using interactive, the full <code>salloc</code> command being executed will be displayed for reference.</p> <pre><code>(ocelote) [netid@junonia ~]$ interactive\nRun \"interactive -h for help customizing interactive use\"\nSubmitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1    --ntasks=1 --time=01:00:00 --account=windfall --partition=windfall\nsalloc: Pending job allocation 531843\nsalloc: job 531843 queued and waiting for resources\nsalloc: job 531843 has been allocated resources\nsalloc: Granted job allocation 531843\nsalloc: Waiting for resource configuration\nsalloc: Nodes i16n1 are ready for job\n[netid@i16n1 ~]$\n</code></pre> <p>Notice in the example above how the command prompt changes once your session starts. When you're on a login node, your prompt will show <code>junonia</code> or <code>wentletrap</code>. Once you're in an interactive session, you'll see the name of the compute node you're connected to. </p> <p>Customizing Your Resources</p> <p>The command <code>interactive</code> when run without any arguments will allocate a windfall session using one CPU for one hour which isn't ideal for most use cases. You can modify this by including additional flags. To see the available options, you can use the help flag <code>-h</code></p> <p><pre><code>(ocelote) [netid@junonia ~]$ interactive -h\nUsage: /usr/local/bin/interactive [-x] [-g] [-N nodes] [-m memory per core] [-n total number of tasks] [-Q optional qos] [-t hh::mm:ss] [-a account to charge]\n</code></pre> The values shown in the output can be combined and each mean the following:</p> Flag Explanation Example <code>-a</code> This is followed by your group's name and will switch you to using the standard partition. This is highly recommended to keep your sessions from being interrupted and to help them start faster. <code>-a my_group</code> <code>-t</code> The amount of time to reserve for your job in the format <code>hhh:mm:ss</code>. <code>-t 05:00:00</code> <code>-n</code> Total number of tasks (CPUs) to allocate to your job. By default, these CPUs will be allocated on a single node. <code>-n 16</code> <code>-N</code> Total number of nodes (physical computers) to allocate to your job. <code>-N 2</code> <code>-m</code> Total amount of memory per CPU. See CPUs and Memory for more details and information on potential complications. <code>-m 5gb</code> <code>-Q</code> Used to access high priority or qualified hours. Only for groups with buy-in or special project hours. High Priority: <code>-Q user_qos_&lt;PI NETID&gt;</code>Qualified: <code>-Q qual_qos_&lt;PI NETID&gt;</code> <code>-g</code> Request one GPU. This flag takes no arguments. On Puma, you may be allocated either a v100 or a MIG slice. If you want more control over your resources, you can use <code>salloc</code> directly using GPU batch directives. <code>-g</code> <code>-x</code> Enable X11 forwarding. This flag takes no arguments. <code>-x</code> <p>You may also create your own salloc commands using any desired Slurm directives for maximum customization.</p>"},{"location":"running_jobs/interactive_jobs/#the-salloc-command","title":"The <code>salloc</code> Command","text":"<p>If <code>interactive</code> is insufficient to meet your resource requirements (e.g., if you need to request more than one GPU or a GPU MIG slice), you can use the Slurm command <code>salloc</code> to further customize your job. </p> <p>The command <code>salloc</code> expects Slurm directives as input arguments that it uses to customize your interactive session. For comprehensive documentation on using <code>salloc</code>, see Slurm's official documentation.</p> <p>Single CPU Example</p> <p><pre><code>salloc --account=&lt;YOUR_GROUP&gt; --partition=standard --nodes=1 --ntasks=1 --time=1:00:00 --job-name=interactive\n</code></pre> Single Node, Multi-CPU Example</p> <pre><code>salloc --account=&lt;YOUR_GROUP&gt; --partition=standard --nodes=1 --ntasks=16 --time=1:00:00 --job-name=interactive\n</code></pre> <p>Multi-GPU Example (Puma) <pre><code>salloc --account=&lt;YOUR_GROUP&gt; --partition=gpu_standard --nodes=1 --ntasks=1 --time=1:00:00 --job-name=multi-gpu --gres=gpu:volta:2\n</code></pre></p> <p>GPU MIG Slice Example <pre><code>salloc --account=&lt;YOUR_GROUP&gt; --partition=gpu_standard --nodes=1 --ntasks=1 --time=1:00:00 --job-name=mig-gpu --gres=gpu:nvidia_a100_80gb_pcie_2g.20gb\n</code></pre></p>"},{"location":"running_jobs/job_limits/","title":"Job Limits","text":""},{"location":"running_jobs/job_limits/#job-limits_1","title":"Job Limits","text":"<p>To ensure equitable access and efficient use of our clusters, we impose restrictions on the total compute resources available to individual users, jobs, and HPC groups. These limits vary based on the specific cluster and partition in use. </p> <p>The table below outlines the maximum resources that can be concurrently used at both the user and group levels.</p> <p>Open OnDemand jobs have different limits</p> <p>Note that the below limits apply to interactive terminal jobs and batch jobs. Open OnDemand jobs have further restrictions that are detailed in the Open OnDemand Job Limit section below. </p> <p>High priority job limits</p> <p>Buy-in groups are limited to using the resources they purchased. If you are a member of a buy-in group, you can check your specific limits by following the instructions in the Checking Limits and Usage below.</p> <p>Available hardware</p> <p>If you're looking for more information on the physical resources available for jobs, see our Compute Resources page for more information. </p> Puma Ocelote El Gato Global Limits Maximum Simultaneous User Jobs 1000 1000 1000 Maximum Job Walltime 10 days 10 days 10 days Standard Job Resources Maximum Memory Per Group 16996 GB 10000 GB 10000 GB Maximum CPUs per Group 3290 1024 1024 Maximum GPUs per Group 4 10 N/A Windfall Job Resources Maximum Memory per User Unlimited Unlimited Unlimited Maximum CPUs per User 6000 6000 6000 Maximum GPUs per Job Unlimited Unlimited N/A High Priority Job Resources Maximum CPUs per Group Number Purchased N/A N/A Maximum GPUs per Group Number Purchased N/A N/A"},{"location":"running_jobs/job_limits/#open-ondemand-jobs-limits","title":"Open OnDemand Jobs Limits","text":"<p>Open OnDemand graphical jobs are more limited in size and duration than batch or interactive terminal jobs. Jobs that need more resources or longer runtimes can be submitted as batch jobs.</p> <p>Single node resources</p> <p>Note that the maximum CPUs and GPUs listed in the table below are based on the resources available by node. For information, see Compute Resources.</p> Puma Ocelote El Gato Maximum Nodes per Job 1 1 1 Maximum CPUs per Job 94 28 (Standard/GPU nodes)48 (High memory node) 16 Maximum GPUs per Job 4 2 N/A Maximum Walltime per Job 4 days 4 days 4 days"},{"location":"running_jobs/job_limits/#checking-limits-and-usage","title":"Checking Limits and Usage","text":"<p>You can view job limits for a specific group by using the <code>job-limits &lt;group_name&gt;</code> command in a terminal session. The output will display the current usage of your group and personal jobs, so the results may vary.</p> <p>Below is an example output from this command for a sample user who is a member of a buy-in group. The buy-in resources available to them are indicated in the High Priority field. If you are not a member of a buy-in group, this field will not appear.</p> <p>Please note that the output of <code>job-limits</code> is specific to the queried group, meaning the User Limits displayed apply only to jobs submitted by that user using the group's account.</p> PumaOceloteEl Gato <pre><code>(puma) [&lt;your_netid&gt;@wentletrap ~]$ job-limits &lt;group&gt;\n\n                        Group Limits: &lt;group&gt;                            \n--------------------------------------------------------------------------------\nJob Type      |    Memory    |     CPU      |        GPU         |  Job Number   \n              |Running/Limit |Running/Limit |   Running/Limit    |Submitted/Limit \n--------------------------------------------------------------------------------\nStandard      |   -/16998G   |    -/3290    |    -/gres/gpu=4    |     -/-       \nHigh Priority |   -/16998G   |     -/94     |    -/gres/gpu=0    |     -/-       \n--------------------------------------------------------------------------------\n\n\n                            User Limits: &lt;your_netid&gt;                          \n--------------------------------------------------------------------------------\nJob Type      |    Memory    |     CPU      |        GPU         | Job Number*   \n              |Running/Limit |Running/Limit |   Running/Limit    |Submitted/Limit \n--------------------------------------------------------------------------------\nWindfall      |     -/-      |    -/6000    |        -/-         |    -/1000     \nStandard      |   -/16998G   |    -/3290    |    -/gres/gpu=4    |               \nHigh Priority |   -/16998G   |     -/94     |    -/gres/gpu=0    |               \n--------------------------------------------------------------------------------\n*Max jobs across all groups and partitions.\n\n\n                            Individual Job Limits                             \n--------------------------------------------------------------------------------\nJob Type      |    Memory    |     CPU      |        GPU         |     Time      \n--------------------------------------------------------------------------------\nWindfall      |    16998G    |     6000     |                    | 10-00:00:00   \nStandard      |    16998G    |     3290     |     gres/gpu=4     | 10-00:00:00   \nHigh Priority |    16998G    |      94      |     gres/gpu=0     | 10-00:00:00   \n--------------------------------------------------------------------------------\n</code></pre> <pre><code>(ocelote) [&lt;your_netid&gt;@wentletrap ~]$ job-limits &lt;group&gt;\n\n                        Group Limits: &lt;group&gt;                            \n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |   Job Number    \n        | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit  \n--------------------------------------------------------------------------------\nStandard |     -/10T      |     -/1024     |   -/gres/gpu=10    |      -/-        \n--------------------------------------------------------------------------------\n\n\n                            User Limits: &lt;your_netid&gt;                            \n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |  Job Number*    \n        | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit  \n--------------------------------------------------------------------------------\nWindfall |      -/-       |     -/6000     |        -/-         |     -/1000      \nStandard |     -/10T      |     -/1024     |   -/gres/gpu=10    |                 \n--------------------------------------------------------------------------------\n*Max jobs across all groups and partitions.\n\n\n                            Individual Job Limits                             \n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |      Time       \n--------------------------------------------------------------------------------\nWindfall |     8064G      |      6000      |                    |  10-00:00:00    \nStandard |     8064G      |      1024      |    gres/gpu=10     |  10-00:00:00    \n--------------------------------------------------------------------------------\n</code></pre> <pre><code>(elgato) [&lt;your_netid&gt;@wentletrap ~]$ job-limits &lt;group&gt;\n\n                        Group Limits: &lt;group&gt;                            \n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |   Job Number    \n        | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit  \n--------------------------------------------------------------------------------\nStandard |     -/10T      |     -/1024     |        -/-         |      -/-        \n--------------------------------------------------------------------------------\n\n\n                            User Limits: &lt;your_netid&gt;                            \n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |  Job Number*    \n        | Running/Limit  | Running/Limit  |   Running/Limit    |Submitted/Limit  \n--------------------------------------------------------------------------------\nWindfall |      -/-       |     -/6000     |        -/-         |     -/1000      \nStandard |     -/10T      |     -/1024     |        -/-         |                 \n--------------------------------------------------------------------------------\n*Max jobs across all groups and partitions.\n\n\n                            Individual Job Limits                             \n--------------------------------------------------------------------------------\nJob Type |     Memory     |      CPU       |        GPU         |      Time       \n--------------------------------------------------------------------------------\nWindfall |     8064G      |      6000      |                    |  10-00:00:00    \nStandard |     8064G      |      1024      |                    |  10-00:00:00    \n--------------------------------------------------------------------------------\n</code></pre>"},{"location":"running_jobs/monitoring_jobs_and_resources/","title":"Monitoring Jobs and Resources","text":""},{"location":"running_jobs/monitoring_jobs_and_resources/#monitoring-jobs","title":"Monitoring Jobs","text":"<p>Tip</p> <p>Some of these functions are specific to the UArizona HPC, and may not work if invoked on other systems. </p> <p>Every job has a unique ID associated with it that can be used to track its status, view resource allocations, and resource usage. Below is a list of some helpful commands you can use for job monitoring.</p> Command Purpose Example <code>squeue --job=&lt;jobid&gt;</code> Retrieves a running or pending job's status. <code>squeue --job=12345</code> <code>squeue --me</code> Retrieves all your running and pending jobs <code>scontrol show jobs &lt;jobid&gt;</code> Retrieve detailed information on a running or pending job <code>scontrol show job 12345</code> <code>scancel &lt;jobid&gt;</code> Cancel a running or pending job <code>scancel 12345</code> <code>job-history &lt;jobid&gt;</code> Retrieves a running or completed job's history in a user-friendly format <code>job-history 12345</code> <code>seff &lt;jobid&gt;</code> Retrieves a completed job's memory and CPU efficiency <code>seff 12345</code> <code>past-jobs</code> Retrieves past jobs run by user. Can be used with option <code>-d &lt;n&gt;</code> to search for jobs run in the past <code>&lt;n&gt;</code> days <code>past-jobs -d 5</code> <code>job-limits &lt;group_name&gt;</code> View your group's job resource limits and current usage. <code>job-limits mygroup</code>"},{"location":"running_jobs/monitoring_jobs_and_resources/#slurm-reason-codes","title":"Slurm Reason Codes","text":"<p>Sometimes, if you check a pending job there is a message under the field <code>Reason</code> indicating why your job may not be running. Some of these codes are non-intuitive so a human-readable translation is provided below:</p> Reason Explanation <code>AssocGrpCpuLimit</code> Your job is not running because your group CPU limit has been reached<sup>1</sup> <code>AssocGrpMemLimit</code> Your job is not running because your group memory limit has been reached<sup>1</sup> <code>AssocGrpCPUMinutesLimit</code> Either your group is out of CPU hours or your job will exhaust your group's CPU hours. <code>AssocGrpGRES</code> Your job is not running because your group GPU limit has been reached<sup>1</sup> <code>Dependency</code> Your job depends on the completion of another job. It will wait in queue until the target job completes. <code>QOSGrpCPUMinutesLimit</code> This message indicates that your high priority or qualified hours allocation has been exhausted for the month. <code>QOSMaxWallDurationPerJobLimit</code> Your job's time limit exceeds the max allowable and will never run<sup>1</sup> <code>Nodes_required_for_job_are_DOWN,_DRAINED_or_reserved_or_jobs_in_higher_priority_partitions</code> This very long message simply means your job is waiting in queue until there is enough space for it to run <code>Priority</code> Your job is waiting in queue until there are enough resources for it to run. <code>QOSMaxCpuPerUserLimit</code> Your job is not running because your per-user CPU limit has been reached<sup>1</sup> <code>ReqNodeNotAvail, Reserved for maintenance</code> Your job's time limit overlaps with an upcoming maintenance window. Run \"uptime_remaining\" to see when the system will go offline. If you remove and resubmit your job with a shorter walltime that does not overlap with maintenance, it will likely run. Otherwise, it will remain pending until after the maintenance window. <code>Resources</code> Your job is waiting in queue until the required resources are available."},{"location":"running_jobs/monitoring_jobs_and_resources/#monitoring-system-resources","title":"Monitoring System Resources","text":"<p>We have a number of system commands that can be used to view the availability of resources on each cluster. This may be helpful in determining which cluster to use for your analyses</p> Command Purpose <code>nodes-busy</code> Display a visualization of each node on a cluster and overall usage. Use <code>nodes-busy --help</code> for more detailed options. <code>cluster-busy</code> Display a visual overview of each cluster's usage <code>system-busy</code> Display a text-based summary of a cluster's usage <ol> <li> <p>For more information on resource limitations, see: Job Limits.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"running_jobs/open_on_demand/","title":"Open OnDemand","text":"<p>Open OnDemand (OOD), which is an NSF-funded open-source HPC portal, is available for users and provides web browser access for graphically interfacing with HPC. This service is available at https://ood.hpc.arizona.edu/. </p>"},{"location":"running_jobs/open_on_demand/#why-use-open-ondemand","title":"Why use Open OnDemand?","text":"<p>Since there are many ways to access HPC, including the command line terminal, why use Open OnDemand? Here are a few of the main reasons:</p> <ul> <li> <p> GUI software available</p> <p>Many commonly used research software packages, such as RStudio and ANSYS, have graphical interfaces that streamline analysis workflows. Open OnDemand allows for easy access to these applications without the complications of server setup or image/port forwarding. </p> </li> <li> <p> User friendly </p> <p>Open OnDemand includes an interactive desktop application which mimics what you might find on your local workstation. This environment may be more intuitive to navigate when getting familiarized with the HPC.</p> </li> <li> <p> Standardized batch access </p> <p>Open OnDemand includes forms to submit batch jobs which include all the relevant parameters in one place. </p> </li> </ul>"},{"location":"running_jobs/open_on_demand/#list-of-available-features-in-open-ondemand","title":"List of Available Features in Open OnDemand","text":"Basic Functions GUI Software Servers Batch/Slurm File Browser Abaqus Jupyter Notebook Job Composer Interactive Desktop ANSYS R Studio Job Viewer Mathematica Shell MATLAB Stata VS Code"},{"location":"running_jobs/open_on_demand/#command-line-access","title":"Command Line Access","text":"<p>Need command line access to a terminal on HPC? No problem! Simply select the Clusters dropdown menu to connect to one of HPC's login nodes. This is also detailed under System Access</p> <p></p>"},{"location":"running_jobs/open_on_demand/#file-browser","title":"File Browser","text":"<p>The file browser provides easy access to your <code>/home</code>, <code>/xdisk</code>, and <code>/groups</code> directories and allows you to view, edit, copy, and rename your files. You may also transfer small files (under 64 MB) between HPC and your local workstation using this interface. For larger transfers, see our section on Transferring Data for more efficient methods. </p> <p>Access</p> <p>In the browser at the top of the screen, select the Files dropdown</p> <p></p> <p>You will be able to select your <code>/home</code> directory, <code>/groups</code>, or <code>/xdisk</code>. If you select <code>/groups</code> or <code>/xdisk</code>, enter your PI's NetID in the Filter field to find your shared group space.</p> <p></p> <p>Editing Files</p> <p>First, navigate to the file you wish to edit. Then, click the vertical ellipses on the right-hand side and select Edit</p> <p></p> <p>This will open a file editor in your browser where you may select your color theme, text size, and syntax highlighting.</p> <p></p>"},{"location":"running_jobs/open_on_demand/#job-viewer-and-composer","title":"Job Viewer and Composer","text":"<p>Job Viewer</p> <p>The Job Viewer allows you to check the status and time remaining of your running jobs. You can also cancel your jobs using this interface. Note: be careful looking at All Jobs since this will likely timeout trying to organize them all. To use the Job Viewer, navigate to the Jobs dropdown and select Active Jobs</p> <p></p> <p>This will open a new page listing all your running and pending jobs. You may delete them by clicking the red trash icon under Actions, or view more information about individual jobs using the dropdown on the left next to the ID.</p> <p></p> <p>Job Composer</p> <p>The Job Composer lets you create and run a Slurm script on any of our three clusters. It should be noted that the Job Composer creates a special string of directories in your <code>/home</code> starting with <code>ondemand/</code> which is where both your submission scripts and output files will be stored. Make note of the path to your files on the right-hand side of the Job Composer screen under Script location.</p> <p></p>"},{"location":"running_jobs/open_on_demand/#interactive-graphical-applications","title":"Interactive Graphical Applications","text":"<p>Open OnDemand provides access to graphical interfaces for some popular software. These can be found under Interactive Apps through the Open OnDemand web browser. The process of starting and accessing these jobs is the same regardless of which application you select.</p> <p>July 31, 2024 changes</p> <p>Beginning on July 31, 2024, OnDemand graphical applications will be limited to four days of runtime to improve general resource availability. Jobs that take longer than four days to run may be converted to batch jobs. If you have questions about using batch jobs, reach out to our consultants. </p> <p>Web Form</p> <p>First, select the desired application from Interactive Apps. This will take you to a form where you will enter your job information. This includes the entries in the following table:</p> <p></p> Field Description Example Cluster Select which cluster to submit the job request to. Puma Run Time The maximum number of hours the job can run. Please note that the maximum possible run time is 10 days (240 hours). 4 Core Count on a single node The number of CPUs needed. This affects the amount of memory your job is allocated. The maximum that can be requested is dependent on which cluster you choose. 16 Memory per core The amount of memory needed per core in GB. The amount that can be requested is dependent on which cluster you choose and your desired node type. For more information, see our CPUs and Memory page. 5 GPUs required The number and type of GPUs needed for your job, if any. One A100 20GB GPU PI Group Your accounting group. If you do not know your group name, you can either check in the user portal, or can run <code>va</code> on the command line. If the group you entered does not exist, you will receive an error <code>sg: group 'group_name' does not exist</code> your-group Queue The queue, or partition, to use. Standard is the most common. If your group has buy-in hours, you may use High Priority. Standard <p>Once you've entered all your details, click Launch at the bottom of the page. This will take you to a tile with information about your job including job ID and session ID. This information can used for debugging purposes. </p> <p>When you first submit your job, it will show as having a status of \"Queued\". Once your job reaches the front of the queue, it will show a status of \"Starting\". When your session is ready, you can launch the application using Connect at the bottom of the tile.</p>"},{"location":"running_jobs/open_on_demand/#applications-available","title":"Applications Available","text":"Virtual DesktopJupyter NotebooksRStudioMatlabAnsysAbaqus <p>Anaconda compatibility issues</p> <p>If you have Anaconda initialized in your account, ensure you have turned off Conda's auto-activation feature. If auto-activation is enabled, your desktop jobs will likely fail with the error <code>Could not connect to session bus</code> </p> <p>One nice feature of Open OnDemand is the ability to interact with HPC using a virtual Desktop environment. This provides a user-friendly way to run applications, perform file management, and navigate through your directories as though you were working with a local computer. Additionally, it eliminates the need to use X11 forwarding when working with GUI applications allowing an easy way to interact with software such as Matlab, VisIt, or Anaconda.</p> <p></p> <p>Tip</p> <p>To access your own python packages in Jupyter, you can create custom kernels either using a python module or using anaconda.</p> <p>The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.</p> <p>When you start a Jupyter notebook, by default your working directory will be your home. If you would like to change this so that your session starts in a different location, you'll need to add a line to the hidden file <code>~/.bashrc</code>. To do this, open your <code>~/.bashrc</code> in a text editor and add the following, substituting your desired path in for <code>&lt;/path/to/directory&gt;</code>:</p> <pre><code>export NOTEBOOK_ROOT=&lt;/path/to/directory&gt;\n</code></pre> <p></p> <p>RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. For an overview of the RStudio IDE, see: https://www.rstudio.com/products/RStudio/.</p> <p>For information on using R on HPC, see our online documentation on Using R Packages.</p> <p></p> <p>A GUI for multiple versions of Matlab is available. You can select which version to use in the web form when specifying your resources.</p> <p></p> <p>Multiple versions of the engineering application Ansys are available. You can specify which version to use in the web form when specifying your resources. To receive Ansys-specific support, see: Community and External Resources</p> <p></p> <p>A GUI for Abaqus is available.</p> <p></p>"},{"location":"running_jobs/overview/","title":"Overview","text":"<p>HPC operates as a shared system with resources in high demand. Computational tasks must be executed as jobs on dedicated compute resources. These resources are granted to each user for a limited time per session, and sessions are organized by Slurm, an open source, fault-tolerant, and highly scalable cluster manager and task scheduler. Users can interact with Slurm via an internet browser through Open OnDemand, or via an SSH connection to one of the login nodes to start an interactive job or submit a batch job. </p> Do not run jobs on Login Nodes <p>A login node serves as a staging area where you can perform housekeeping work, edit scripts, and submit job requests for execution on one/some of the cluster\u2019s compute nodes. It is important to know that the login nodes are not the location where scripts are run. Heavy computation on the login nodes slows the system down for all users and will not give you the resources or performance you need. Additionally, modules are not available on the login nodes. </p> <p>Tasks run on the login nodes that impact usability will be identified and cancelled by HPC infrastructure without warning. </p>"},{"location":"running_jobs/overview/#new-to-hpc","title":"New to HPC?","text":"<p>If you are new to the UArizona HPC system, or to HPC systems in general, we recommend reviewing our quick start guide before getting into the details of running jobs. You may also want to take a look at our workshops which cover topics including introduction to HPC, parallel computing, and containers, among other topics. </p>"},{"location":"running_jobs/overview/#best-practices","title":"Best Practices","text":"<p>When creating a job request, please keep the following in mind:</p> <ul> <li> <p> Don't ask for more resources than you really need.</p> <p>The scheduler will have an easier time finding a slot for the two hours you need rather than the 48 hours you request.  When you run a job it will report back on the time used which you can use as a reference for future jobs. However don't cut the time too tight.  If something like shared I/O activity slows it down and you run out of time, the job will fail.</p> <p>Additionally, please do not request more CPUs than you plan to use. Increasing the number of CPUs without a clear plan of how your software is going to use them will not result in faster computation. Unused CPUs in a job represent wasted system resources, and will cost more CPU-hours to your allocation than you actually needed to spend. </p> </li> <li> <p> Test your submission scripts.</p> <p>Start small. You can use an interactive session to help build your script and run tests in real time.</p> </li> <li> <p> Respect memory limits. </p> <p>If your application needs more memory than is available, your job could fail and leave the node in a state that requires manual intervention. The failure messages might reference '''OOM''' which means '''Out Of Memory'''</p> </li> <li> <p> Do not run loops automating a large number of job submissions. </p> <p>Executing large numbers of job submissions in rapid succession (e.g. in a scripted loop) can overload the system's scheduler and cause problems with overall system performance. Small numbers of automated jobs may be acceptable (e.g. less than 100), but a better alternative in almost all cases is to use job arrays instead.</p> </li> <li> <p> Hyperthreading is turned off. </p> <p>Running multiple threads per core is generally not productive.  MKL is an exception to that if it is relevant to your workflow. Instead, running one thread per core and using multiple cores (i.e. \"multiprocessing\" rather than \"multithreading\") is a suggested alternative.</p> </li> <li> <p> Open OnDemand Usage</p> <p>Please be mindful of other users' needs and avoid monopolizing resources for extended periods when they are not actively being utilized. In practice, this means actively terminating sessions when you are finished rather than leaving the session open. Closing the browser tab does not terminate the session. This ensures fair access for all members of our community and promotes a collaborative environment, and ensures you are only charged for the time you actually used.</p> </li> </ul>"},{"location":"running_jobs/overview/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Below is a FAQ that includes answers to common questions and misconceptions about running jobs on HPC. Some are general information about HPC systems, but some contain information specific to the UArizona HPC. We recommend reviewing this FAQ whether you are a new user getting started on our system, or an experienced user returning to our documentation.</p> What is a Job? <p>A single instance of allocated computing resources is known as a Job. Jobs can be in the format of a graphical Open OnDemand application instance, interactive terminal session, or batch submission (a script scheduled for  execution at a later time), and require an active allocation of CPU-time under an associated PI account. See Time Allocations for more information.      </p> How much am I charged for a job? <p>Each PI group is given a free monthly allocation of CPU hours on each cluster (see Time Allocations for more information). The amount charged to this allocation is equal to the number of CPUs used for a job multiplied by its total run time in hours.       </p> How many jobs can I run? <p>Each user can submit a maximum of 1000 jobs per cluster, and there are further limits on memory, CPUs, and GPUs concurrently used per group. More information is available at Job Limits. If you intend to submit a large number of jobs, especially if they are similar, they should be submitted as array jobs.      </p> How much memory can I request? <p>The total amount of memory assigned to a job depends on the number of CPUs assigned to a job. Memory is physically mounted to the CPUs, so there is a fixed amount available to the job equal to the memory of a single CPU multiplied by the number of CPUs. For example, if there is 64GB of memory and 16 cores the ratio is always 4GB per core. You can ask for less, but never more.      </p> What compute resources are available on each cluster? <p>We have three clusters available for use: Puma, Ocelote, and El Gato. See Compute Resources for details on each.      </p> I submitted a job request, but it isn't running yet. Why? <p>Job requests go to our task scheduler, Slurm, which manages all incoming job requests and determines the optimal order to run them in order to maximize the efficiency of the system. The amount of wait time for your job depends on the number and size of jobs before your job in the queue; the amount of compute resources requested by your job; and the requested wall time for your job. Variation in wait times is a natural consequence of this system. Typically, our most advanced cluster, Puma, experiences longer wait times due to increased usage. Smaller or interactive jobs are recommended to run on Ocelote or El Gato.       </p> Why is my job running slower on the HPC than on my laptop? <p> The strongest use cases for HPC are either running more jobs or larger jobs than your laptop. The advantages come from running jobs in parallel or parallelizing your code. A job that runs well on your laptop could be slower on HPC because you are waiting for other jobs to end or you haven't taken advantage of the parallel features of HPC. See Parallelization for more info.       </p> I need some software to run my analysis, can you install it for me? <p>First, check our list of installed software by accessing an interactive terminal session, then using module commands to check for your software. If you do not see your desired software package there, then there are two options.      <ol> <li> You can attempt to install the software for yourself in one of your own directories.</li> <li> Or you can request HPC Consult to install the software as a system module.</li> </ol> </p> <p> Many programs can be installed with option 1 by users without root privileges, especially packages that are available via package managers like Pip, Conda, and CRAN. Most software like this does not make sense to install as a system module since it is generally easy to install on a per-user basis. Other software that is not available through package managers might be able to be installed by downloading and compiling the source code in one of your folders. The complexity of this process varies greatly from software to software, so if you run into any trouble while doing this, feel free to contact HPC Consulting for support.       </p> <p> For information on what's appropriate to install as a system module, see our software policies guide.       </p>"},{"location":"running_jobs/parallelization/","title":"Parallelization","text":""},{"location":"running_jobs/parallelization/#overview","title":"Overview","text":"<p>You might be surprised to learn that if you move code from a local computer to a supercomputer, it will not automatically run faster and may even run slower. This is because the power of a supercomputer comes from the volume of resources available (compute nodes, CPUs, GPUs, etc.) and not the clockspeed of the processors themselves. Performance boosts come from optimizing your code to make use of the additional processors available on HPC, a practice known as parallelization.</p> <p>Parallelization enables jobs to 'divide-and-conquer' independent tasks within a process when multiple threads are available. In practice, this typically means running a job with multiple CPUs on the HPC. On your local machine, running apps like your web browser is natively parallelized, meaning you don't have to worry about having so many tabs open. However, on the HPC, parallelization must almost always be explicitly configured and called from your job. This process is highly software-dependent, so please research the proper method for running your program of choice in parallel.</p> <p>If you are interested in learning more about parallelization, please check out our Parallel Computing Workshop</p>"},{"location":"running_jobs/parallelization/#scaling","title":"Scaling","text":"<p>A classic example of the advantages of parallel computing is multiplying an \\(N \\times N\\) matrix by a scalar, which takes \\(N^2\\) floating-point operations (flops). On one CPU, this will take an amount of time \\(t = N^2 / f\\) where \\(f\\) is the clock frequency of the CPU. On an integer \\(M\\) number of CPUs, this computation will instead take \\(t' = \\frac{t}{M}\\). Most analyses involve computations which are not always as independent as matrix multiplication, leading to less than perfect speedup times. </p> <p>Amdahl's Law (strong scaling)</p> <p>The behavior of predicted speedup time for an analysis of fixed size is known as Strong Scaling. It depends on the fraction of code in a particular program which is parallelizable. </p> <p>Gustafson's Law (weak scaling)</p> <p>When scaling up an analysis by a factor of N, and running it on N times as many processors, the theoretical limit is an equivalent runtime. In practice, the runtime is typically longer.</p> <p>Most advancements in research are made from increasing the individual problem size, rather than decreasing time to execution, and therefore benefits more from so-called \"weak\" scaling. </p>"},{"location":"running_jobs/parallelization/#single-versus-multi-node-parallelism","title":"Single versus Multi-Node Parallelism","text":"<p>Single Node</p> <p>Each node on HPC has multiple CPUs. These can be utilized simultaneously in shared-memory parallelism. </p> <p>Multi-Node</p> <p>Multiple nodes can be accessed simultaneously on HPC, but memory is distributed rather than shared. In this case, additional software is needed in order to facilitate communication between processes, such as OpenMPI or Intel MPI. </p> <p>Please be aware of the type of parallelism used in your program. Some software is configured only for shared-memory parallelism and will not be able to use processors on multiple nodes. </p>"},{"location":"running_jobs/parallelization/#implementation","title":"Implementation","text":"<p>Some software is configured to be parallel out of the box, and other software needs explicit configuration. Check with your provider to determine whether parallelism is available, and which kind. For example, Python is natively serial, but libraries are available to enable either shared-memory or distributed-memory parallelism.</p> <p>Providing a detailed guide on how to code parallel processing for all software available on the HPC is beyond the scope of this documentation. Instead, please refer to the following online guides:</p> <p>Python</p> <p><code>multiprocessing</code> - enables shared-memory parallelism. Reference. </p> <p><code>import mpi4py</code> - enables distributed-memory parallelism. Reference.</p> <p>R</p> <p>R provides the <code>parallel</code> library for multiprocessing. Reference.</p> <p>MATLAB</p> <p>MATLAB provides the Parallel Computing Toolbox. Reference.</p> <p>MPI</p> <p>We provide several versions and implementations of MPI as system modules:</p> <pre><code>[netid@cpu39 ~]$ module avail mpi\n\n---------------- /opt/ohpc/pub/moduledeps/gnu8 ----------------\n   mpich/3.3.1    openmpi3/3.1.4 (L)    openmpi4/4.1.1\n</code></pre>"},{"location":"running_jobs/visualization/","title":"Visualization Consulting","text":"<p>To see a gallery of past projects please visit https://rtdatavis.github.io/.   <p></p>"},{"location":"running_jobs/visualization/#software","title":"Software","text":""},{"location":"running_jobs/visualization/#paraview","title":"Paraview","text":"<p>Paraview\u00a0 is available as a module. You will use those in server mode where the calculations are done on HPC nodes and the rendering is back on your workstation. View our material on\u00a0 Visualization With ParaView for more information.</p> <ul> <li>Essential Ideas for     ParaView</li> <li>Getting Started With     ParaView</li> <li>ParaView Cameras and     Keyframes</li> <li>Visualizing NetCDF     Files</li> <li>[Getting Started With The Paraview Terminal     (PvPython)](Paraview/Getting_Started_With_The_Paraview_Terminal_PvPython_</li> <li>Graphs and Exporting     Data</li> <li>Paraview Headless Batch     Rendering</li> <li>Speeding up Filters &amp; Rendering with Client Server     configurations</li> <li>Advanced Computational Geometry in Paraview with CGAL &amp;     Vespa</li> <li>Demystifying Paraview Python Plugins (or at least a little less     mystery)</li> </ul>"},{"location":"running_jobs/visualization/#blender","title":"Blender","text":"<p>Blender is a tremendously useful open source 3D modeling program. Detailed instructions for Blender are available in several tutorials. </p> <p></p> <ul> <li>Blender Command Line     Rendering</li> <li>Scaling up Blender     rendering</li> </ul>"},{"location":"running_jobs/visualization/#visit","title":"Visit","text":"<p>Step by step instructions for using VisIt are on this page, and it is also available as a module on our system.</p>"},{"location":"running_jobs/visualization/#guis-through-open-ondemand","title":"GUIs Through Open OnDemand","text":"<p>You can utilize the GUI version of an application using a Virtual Desktop or through one of our interactive applications using Open OnDemand.</p> <p>When you choose an application from the Interactive Apps dropdown at the top of the page and enter the resources you need (like cores and wall time hours) you will have access to a session that will let you use a compute node interactively through a virtual desktop or software-specific GUI application.\u00a0\u00a0 </p> <p></p>"},{"location":"running_jobs/visualization/#vpn-virtual-private-network","title":"VPN - Virtual Private Network","text":"<p>A VPN service (Virtual Private Network) is available for HPC, primarily for applications that cannot navigate the bastion host for visualization, and because the performance is frequently better than tunneling through the Bastion host. This is separate from the UA VPN. We suggest you first try the Desktop application of OnDemand for GUI applications as shown for Matlab immediately above.</p> <p>VPN - Virtual Private Network </p>"},{"location":"running_jobs/visualization/#x11-examples","title":"X11 Examples","text":"<p>The alternative to using the Open OnDemand desktop is to use X11 tunneling, though it should be noted that in most cases the performance is not as good. The examples below use X11 forwarding to access the graphical interfaces for the listed software.</p>"},{"location":"running_jobs/visualization/#ansys-workbench","title":"Ansys Workbench","text":"<p>In this example, we use the GUI for launching Ansys projects. Note: Ansys is also available as a GUI application through Open OnDemand. You can expect it to be much faster using OnDemand</p> <pre><code>$ ssh -X netid@hpc.arizona.edu\n$ shell -X\n$ interactive -x -a your_group # starts an interactive session for one hour. The -x option will enable image forwarding\n$ module load cuda11           # Provides the GLX functionality needed by some components like CFD Post\n$ module load ansys\n$ runwb2\n</code></pre>"},{"location":"running_jobs/visualization/#glx-useful-for-visit","title":"GLX - Useful for VisIt","text":"<p>The functionality of GLX requires synchronization between your workstation and the compute nodes. \u00a0Keep in mind that the login nodes do not have a graphics so you must request a compute using an interactive session. \u00a0</p> <p>These steps provide a practical example for using the Visualization software called VisIt</p> <p></p> <pre><code>$ ssh -X netid@hpc.arizona.edu\n$ shell -X                      \n$ interactive -a your_group -x  # The -x flag provides the graphics tunneling \n$ module load cuda11            # Provides support for GLX. A GPU node is not required\n$ glxinfo                       # **See notes below if you get an error during this step\n$ glxspheres64                  # This is a good test of rendering back on the workstation\n$ module load visit\n$ visit\n</code></pre> <p>** You might get a \"Bad Value\" error on the Mac command line. This can by fixed by entering the following on the command line</p> <pre><code>$ defaults write org.macosforge.xquartz.X11 enable_iglx -bool true\n</code></pre> <p>On a Linux workstation, you might need to create a file /etc/X11/xorg/conf with the following section:</p> <pre><code>Section \"ServerFlags\"\n\n\u00a0\u00a0\u00a0\u00a0Option \"AllowIndirectGLX\" \"on\"\u00a0 \n\u00a0\u00a0\u00a0\u00a0Option \"IndirectGLX\" \"on\" \u00a0\nEndSection\n</code></pre> <p>{\"serverDuration\": 17, \"requestCorrelationId\": \"0a7963d46a704cf6895bca5ac6e13934\"}</p>"},{"location":"running_jobs/visualization/arcgis_pro_heightmaps/","title":"ArcGIS Pro Heightmaps","text":"<p>This documentation is provided courtesy of Glenn Ingram, Graduate Assistant of Kiri Carini in the Main Library Data Cooperative. His personal information can be found at</p> <ul> <li>https://data.library.arizona.edu/geo/about-geospatial-team</li> <li>https://glenningram.github.io/</li> </ul> <p>LiDAR Raster for Blender 3D</p> <p>This tutorial illustrates how to use ArcGIS Pro to prepare a raster suitable for Blender 3D out of .laz Lidar Data. This is also an illustration of the first step for a workflow that will utilize Blender Command Line Rendering.  </p> <p>ArcGIS Pro is utilized because all the steps can be completed in a single software - if you do not have access to ArcGIS Pro, a similar process can be achieved using LASTools and QGIS.</p> <p>Step One: Finding Data:</p> <p>The University of Arizona provides a .laz Lidar Dataset for Arizona. To start:</p> <ol> <li>Find the corresponding tiles to your area of interest on the     Index Map.</li> <li>Select as many tiles as needed, but make sure to use a square or     rectangular grid of tiles.</li> <li>Record the tile labels.</li> </ol> <p></p> <p>Using the tiles labels, find the LAZ Files in CyVerse here (Make sure you download the .laz file format)</p> <p>Step Two: Convert LAZ</p> <p>Unlike LAS (.las) files, LAZ (.laz) files cannot be opened or added directly to ArcGIS Pro to display point cloud data on a map. However, it is possible to convert the LAZ files to LAS datasets to show on the map.\u00a0</p> <p>Open ArcGIS Pro. Under geoprocessing, use the Convert LAS tool to convert each .laz file to a .las. Fill in the tool parameters as required and complete this step for each .laz file.</p> <p>For additional information or help, refer to Convert LAS.</p> <p> </p> <p>Step Three: Create LAS Dataset</p> <p>Once files are converted to .las, we can build a dataset. Use the Create LAS Dataset tool found in geoprocessing. Refer to Create LAS Dataset for more information.</p> <p> </p> <p>Step Four: Dataset to Raster</p> <p>The dataset can be converted to a raster once all .las files are collected in the dataset. Use the LAS Dataset to Raster tool. More information here.</p> <p></p> <p>Step Five: Project Raster</p> <p>Once the raster goes into Blender, it will lose any coordinate data and any projection occurring in ArcGIS. Reprojecting the raster will ensure that the relief produced in Blender aligns with any other vectors or imagery being used in the project.</p> <p>Determine which projection is best for your project and reproject the new raster. The rest of the project will have to remain in this projection to stay aligned.</p> <p></p> <p>Step Six: Raster Calculator</p> <p>Your raster likely included decimal values, which Blender will not process well. Use the Raster Calculator and the \u201cInt\u201d function to change raster values to whole numbers.</p> <p>While using the Raster Calculator, we need to rescale the raster for use in Blender. A raster pixel can hold a value between 0 and 65,535; we will have to use the maximum value to create the highest quality render in Blender. The Raster Calculator can be used to rescale the raster.</p> <p>The equation we will be using is\u00a0</p> <p>(Pixel Value \u2013 Lowest Value) \u00f7 (Highest Value \u2013 Lowest Value) * 65,535</p> <p>In the Raster Calculator, this will look like</p> <p>(\u201craster\u201d \u2013 Lowest Value) / (Highest Value \u2013 Lowest Value) * 65535</p> <p></p> <p>Step Six: Copy Raster</p> <p>With our raster rescaled, it is time to export. We will use the Copy Raster tool. Set the Pixel Type to \u201c16 bit unsigned\u201d and Format to \u201cTiff.\u201d and output to a location on your computer.</p> <p></p> <p></p> <p>Bonus Step: Open in Photoshop</p> <p>Opening the raster and resaving as a tiff is an easy step that \u201ccleans\u201d the file and reduces the file size without losing quality if you have access to Photoshop. You can also use photoshop to view your rasters pixel size - a needed input for Blender.</p> <p></p> <p>Final Step: Blender</p> <p>Cartographer Daniel Huffman has an extensive and detailed tutorial walk-through for setting up Blender for creating shaded reliefs. Use these steps for creating your Blender file. When the Blender settings are complete, save the file as a package, and you are ready to use the HPC!</p> <p>Remember, your raster has already been prepared for Blender in ArcGIS Pro, so some early steps in the tutorial will be redundant.\u00a0</p> <p>Good luck!</p> <p>{\"serverDuration\": 20, \"requestCorrelationId\": \"531c1ee744e546d58d1dc765a3d589c4\"}</p>"},{"location":"running_jobs/visualization/blender/command_line_rendering/","title":"Roadmap","text":"<ul> <li>Roadmap</li> <li>Overview</li> <li>OOD Remote Desktop Session</li> <li>Download and Configure Blender</li> <li>Running the Blender Scene Render from the Command Line</li> <li>Viewing Your Results</li> </ul>"},{"location":"running_jobs/visualization/blender/command_line_rendering/#overview","title":"Overview","text":"<p>Blender is a tremendously useful open source 3D modeling program, and can be run on the HPC to create high fidelity visualizations of data and general 3D modeling. One of the benefits of running Blender on the HPC is you're able to transfer in a blender file containing your scene already configured and you can start the rendering process and leave it running overnight or during the week depending on the size of the rendering that needs to be done. Using command line rendering makes it possible to render from blender in a headless fashion, meaning no GPU is has to be allocated for your rendering.\u00a0 Another benefit of rendering on the supercomputer is large single frame Renderings can take up a significant amount of computer memory which HPC environments have to spare.</p>"},{"location":"running_jobs/visualization/blender/command_line_rendering/#ood-remote-desktop-session","title":"OOD Remote Desktop Session","text":"<p>Start up Open On Demand by going to ood.hpc.arizona.edu or reading up about OOD within our documentation</p> <p>class=\"confluence-embedded-file-wrapper confluence-embedded-manual-size\"&gt;</p> <p>Enter details of your request. Here we are requesting a 5 hour 16 core interactive session on elgato. Important to note that your own PI Group must be entered instead of <code>visteam</code> </p> <p></p> <p>when the request clears the allocation queue you will be able to launch your interactive desktop</p> <p></p>"},{"location":"running_jobs/visualization/blender/command_line_rendering/#download-and-configure-blender","title":"Download and Configure Blender","text":"<p>In this section we will download Blender, and unzip it, and then write a rendering python script</p> <p>At the remote desktop start screen open a terminal</p> <p></p> <p>Navigate to an existing folder where you have space</p> <p></p> <p>Download blender with this <code>wget</code>\u00a0 command from their clarkson mirror</p> <pre><code>wget \"../../all_images/mirror.clarkson.edu/blender/release/Blender2.93/blender-2.93.8-linux-x64.tar.xz\"\n</code></pre> <p>This lets us download the Long Term Support version of blender from their homepage ../../all_images/www.blender.org/download/lts/2-93/</p> <p>Also make sure to uncompress the <code>tar.xz</code>\u00a0 at the end using this command</p> <pre><code>tar -xf blender-2.93.8-linux-x64.tar.xz\n</code></pre> <p></p> <p>Here is the code that we will put into the <code>render.py</code>\u00a0 file  </p> <pre><code>#get the blender python library\nimport bpy\n#set the scene \nscene = bpy.context.scene\n# set the output format as .tif\nscene.render.image_settings.file_format = \"TIFF\"\n# specify where the rendered image/s will go and what their names should be\nscene.render.filepath = \"./frames/render\"\n# set the engine to high performance CYCLES renderer\nscene.render.engine = \"CYCLES\"\n# set the resolution percentage down for testing, turn this up to 100 when it's worked once\nscene.render.resolution_percentage = 25\n# how many cpu threads we should create, this is a good default for Elgato, but should be higher on Puma and Ocelote\n# set it to the number of CPU cores you have in your allocation\nscene.render.threads = 15\nscene.render.threads_mode = \"FIXED\"\n# write a still frame render of the scene\nbpy.ops.render.render(write_still=1)\n</code></pre>"},{"location":"running_jobs/visualization/blender/command_line_rendering/#running-the-blender-scene-render-from-the-command-line","title":"Running the Blender Scene Render from the Command Line","text":"<p>Once you have created/edited the <code>render.py</code>\u00a0 using a command line editor like vi, use this command to start the headless rendering</p> <p>This will ensure that the blender file you have configured gets run in the background (\u00a0<code>-b</code>\u00a0 ) and that the\u00a0<code>render.py</code>\u00a0 script is used as a python (\u00a0<code>-P</code>\u00a0 ) script\u00a0</p> <pre><code>blender-2.93.8-linux-x64/blender -b &lt;blender file here&gt; -P render.py\n</code></pre> <p>With any luck your terminal will output that the process is initializing. This scene renders a landscape mesh of flagstaff using a heightmap in a tiff format which is why we see messages like <code>TIFFFetchNormalTag</code> , which shouldn't appear on your screen unless you are doing a similar task.</p> <p></p> <p>This output shows the initialization of other blender systems that may or may not be part of the rendering that you are doing. Unused systems are unlikely to detract from the rendering performance and are simply listed for diagnostic purposes it seems.</p> <p></p> <p>Once the initialization has completed the individual tiles of the larger image will begin rendering. This is where the massive multicore environments can really shine because a rendering thread can be dispatched for each core provided there's enough memory to support all of them running at the same time.</p> <p></p> <p>There is also a time estimate which is usually an over estimate for the full duration of the rendering task.</p> <p></p> <p>If you want to make sure that all the cpu cores you have allocated are in use use the <code>htop -u &lt;username&gt;</code>\u00a0 in a new terminal tab.</p> <p></p> <p>When the process completes you will see blender quit after saving out an image to the folder you specify</p> <p></p>"},{"location":"running_jobs/visualization/blender/command_line_rendering/#viewing-your-results","title":"Viewing Your Results","text":"<p>This is a view of the image produced by the workflow</p> <p></p> <p></p> <p>{\"serverDuration\": 21, \"requestCorrelationId\": \"ca405ed9ac024358b0ff2ea1e8cbddc2\"}</p>"},{"location":"running_jobs/visualization/blender/scaling_up_blender_rendering/","title":"Rendering a single frame headlessly:","text":""},{"location":"running_jobs/visualization/blender/scaling_up_blender_rendering/#pre-reqs","title":"Pre Reqs","text":"<ul> <li>This tutorial assumes you have followed steps from Blender Command     Line     Rendering     to get a copy of blender and a folder to work in.</li> <li>Download the blender file     test-headless.blend     to your hpc workspace</li> <li>It also assumes an understanding of singularity commands for working     with     containers\u00a0Containers,     and\u00a0 Introduction to Containers on     HPC</li> </ul>"},{"location":"running_jobs/visualization/blender/scaling_up_blender_rendering/#attribution","title":"Attribution","text":"<p>This article forms the base and background of this technique</p> <p>https://linuxmeerkat.wordpress.com/2014/10/17/running-a-gui-application-in-a-docker-container/</p>"},{"location":"running_jobs/visualization/blender/scaling_up_blender_rendering/#summary","title":"Summary","text":"<p>The example that follows aims at producing a rendered frame from a default blender scene using `Xvfb` within a preconfigured singularity container. In each section there will be screen shot footage on the right and a code block provided for each command entered to make copying and reproducing the steps easier.</p> <p></p> <p>image credit ( https://www.youtube.com/watch?v=FNhUnPWzVaw )</p> <p>The first step here is to start an interactive job and get our copy of the singularity image from the github container repository. The code used are</p> <pre><code>interactive -a &lt;your account name&gt; -n 4\n</code></pre> <p>With your account specified here instead of the <code>visteam</code> account. Then navigate to a folder where you would like do do your work.</p> <p>You will then need a singularity container configured for<code>Xvfb</code></p> <pre><code>singularity pull -F docker://ghcr.io/devinbayly/xvfb:test\n</code></pre> <p>This retrieves the docker image from github container repository and converts it into a singularity image. The <code>-F</code> is for forcing the command to overwrite an existing image if you have already pulled this in the past. This helps make sure that you work with the newest image if there were changes made.</p> <p></p> <p></p> <p>Now we go inside the container and start the Xvfb program to generate a virtual display named 99 with a screen 0 associated of 1024x720 resolution with a depth of 16 images.</p> <pre><code>singularity shell xvfb_test.sif\nXvfb :99 -screen 0 1024x720x16 &amp;&gt; xvfb.log &amp;\n</code></pre> <p>This will ensure the display remains as a background task and sends messages to the log file.</p> <p>To check whether this is working we can grep for the X process.</p> <pre><code>ps -aux | grep X\n</code></pre> <p>But of course this also reveals any of the other users who are also trying to create displays, so I've blurred that out. Part of picking<code>:99</code> is to avoid using the same name as other folks on a node.</p> <p></p> <p></p> <p>We are now ready to try to run a headless blender render.</p> <p>The first screenshot shows the result of jumping the gun and running the command without mentioning which display to use. A more complete command that achieves the result is provided as follows.</p> <pre><code>DISPLAY=:99.0 blender-3.3.0-linux-x64/blender -b test-headless.blend -f 1\n</code></pre> <p>This will make the blender command use the correct display for our rendering of a single frame from the blender scene. There's an exhausting, ahem I mean exhaustive treatment of the command line rendering flags here https://docs.blender.org/manual/en/latest/advanced/command_line/render.html if you are curious.</p> <p>The output of the command will be lines suggesting the scene is loaded and that samples are being taken and rendered to the image. The final line shows where the resulting png frame ends up.</p> <p></p> <p></p> <p>If you would like to watch the steps in video form feel free.</p> <p>Your browser does not support the HTML5 video element</p> <p>{panel</p>"},{"location":"running_jobs/visualization/blender/scaling_up_blender_rendering/#scaling-up","title":"Scaling Up","text":"<p>The process of using many cpu cores with blender is as follows.</p> <p>The image for this step is pretty small, but you may click on it to view it in a larger window. That or you can trust me that the code blocks below are all you need.</p> <p>The contents of the file on the left simply capture what was done in the interactive session in the previous step. So create a file called <code>run_blender.sh</code> with these contents.</p> <pre><code>#!/bin/bash\nXvfb :99 -screen 0 1024x720x16 &amp;&gt; xvfb.log &amp;\nDISPLAY=:99.0 blender-3.3.0-linux-x64/blender -b test-headless.blend -o ./test_$1_ -f 1\n</code></pre> <p>We don't need to use the singularity shell command because that will be part of our slurm batch submission script. Also the<code>-o ./test_$1_</code>lets us name the output images with a templated pattern using a number passed to the script.</p> <p>The file on the right is our slurm batch submission script named <code>batch.sh</code>. Read more about them in Jobs and Scheduling. The content of this one is:</p> <pre><code>#!/bin/bash\n#SBATCH --output=Sample_SLURM_Job-%a.out\n#SBATCH --ntasks=4\n#SBATCH --nodes=1             \n#SBATCH --time=00:15:00   \n#SBATCH --partition=standard\n#SBATCH --account=&lt;your account name&gt;   \n#SBATCH --array 0-64\n\n# SLURM Inherits your environment. cd $SLURM_SUBMIT_DIR not needed\nsingularity exec xvfb_test.sif bash run_blender.sh ${SLURM_ARRAY_TASK_ID}\n</code></pre> <p>The primary change you will need to make is to write your own account in the place where it says <code>--account=&lt;your account name&gt;</code>. Besides that you will notice that the variable<code>${SLURM_ARRAY_TASK_ID}</code>is being passed into the shell script so that each separate task will create a frame png output. To run this batch file we use:</p> <pre><code>sbatch batch.sh\n</code></pre> <p>Once the command to run the batch.sh is provided we see that all our 64 tasks with 4 cores each get kicked off.</p> <p>To view the results you may return to the open on demand panel and select one of the many finished pngs. I'm hopeful that you will find better ways to apply this to visualizing data besides leveraging 256 cores to render a gray cube on a gray background.</p> <p></p> <p></p> <p></p> <p> </p> <p></p> <p>If you would like to watch the steps unfold instead of looking at the still frames feel free to use this video.</p>"},{"location":"running_jobs/visualization/paraview/cameras_and_keyframes/","title":"Cameras and Keyframes","text":"<p>When working with ParaView, it can be helpful to generate images or videos to show to others. Fortunately, ParaView has many built-in features that allow you to do just that.</p>"},{"location":"running_jobs/visualization/paraview/cameras_and_keyframes/#basic-image-capture","title":"Basic Image Capture","text":"<p>The simplest way to capture an image from ParaView is to click the camera icon to the left just above the 3D view (the one with four corners around it). Clicking it will open a window asking where you would like to save the image just as you would any file. Holding Ctrl, Shift, or Alt will take the photo and copy it to your clipboard to paste it somewhere rather than saving it to your files. The photo will always be whatever you currently see in your viewport. To change your view you can use the left, middle, and right mouse buttons to move around or click the camera with the pencil on it located above the viewport to enter numeric values.</p>"},{"location":"running_jobs/visualization/paraview/cameras_and_keyframes/#animation-capture","title":"Animation Capture","text":"<p>Rather than repeatedly clicking the save image button, ParaView offers ways of storing animations of your data.</p> <p>The simplest way to do this is using extractors. To get a better understanding of how extractors work, check out Essential Ideas for more information.</p> <p>From the toolbar, select Extractors \u2192 Image then select PNG or JPG depending on your preference. PNG is generally higher quality with a bigger file size while JPG is lower quality with a smaller file size.</p> <p>After this, you're all set to save your images. Position your camera wherever you'd like then do File \u2192 Save Extracts. This will ask you to select a folder to save them to, then save the files to that folder. By default, it will save every frame of your scene as an image. You can control this and other properties in the Properties tab of the extractor.</p>"},{"location":"running_jobs/visualization/paraview/cameras_and_keyframes/#animation-with-a-moving-camera","title":"Animation With A Moving Camera","text":"<p>If you would like the camera to move through your animation, this can be occasionally tedious but nonetheless doable. You\u2019ll first need to open the animation view from View \u2192 Animation View, click the dropdown next to the blue plus sign in the animation view, select Camera, select the movement option that works best for you, then press the blue plus. I've outlined the different movement options below:</p>"},{"location":"running_jobs/visualization/paraview/cameras_and_keyframes/#orbit","title":"Orbit","text":"<p>Orbit allows you to define a circle that the camera will follow over the animation. The three parameters are as follows. Center: A vector representing the point the circle surrounds (and where the camera looks at). Normal: A vector that will be perpendicular to the circle (aka the axis that you want to rotate around). Origin: A vector indicating the starting point of the camera.</p>"},{"location":"running_jobs/visualization/paraview/cameras_and_keyframes/#follow-path","title":"Follow Path","text":"<p>Follow Path will create an orbit around the selected object's starting position. This is a simpler way of creating an orbit but it does not provide as many options.</p>"},{"location":"running_jobs/visualization/paraview/cameras_and_keyframes/#follow-data","title":"Follow Data","text":"<p>Follow Data has the camera track the movement of the selected object. The camera's starting position will be wherever it is when this is added to the animation view. The tracking is done by following the average position of all the points and thus isn't affected by uniform scaling or rotation.</p>"},{"location":"running_jobs/visualization/paraview/cameras_and_keyframes/#interpolate-camera-path-splinelinear","title":"Interpolate Camera Path (Spline/Linear)","text":"<p>This option is the most versatile but also most difficult to use option. It allows you to create a custom path for your camera based on where it should be at certain times. After adding this to the animation view, double-click the new Camera animation target. This will bring up a page where you can add all the points of your path. As a workflow for this, I recommend moving to the frame and position you want to capture, adding a keyframe in the editor, changing its time value to the current frame, double click on the position, and selecting Use Current. This is still tedious, just slightly less so that individually filling in values. The difference between spline and linear has to do with where the camera is between keyframes. If you choose linear, it will transition with straight lines between the points. If you choose spline, it will give a more flowing path.</p>"},{"location":"running_jobs/visualization/paraview/cameras_and_keyframes/#capturing-graphs-and-other-visualizations","title":"Capturing Graphs and Other Visualizations","text":"<p>The process for capturing graphs and other types of visualizations is very similar to the process for the viewport. For a single frame, a camera icon is shown in the top left of the graph view. For an animation, add a PNG/JPG extractor as you would with the viewport but make sure that the plot is selected rather than the viewport (click on it, a blue border should appear). To see what image capture is capturing what, click between your viewport and graph(s). If the blue eye icon appears to the left of the capture device in the pipeline browser, it is capturing the selected view. Once everything is set up, go to File \u2192 Save Extracts as before.</p> <p>If you'd like to give this a try, you can use the can2 example from Getting Started With ParaView GUI to try to create this video:</p> <p>The data in the histogram is the magnitude of the displacement vectors (displ).</p> <p>Note that as ParaView exports individual images per frame, the use of other image processing software such as Adobe Premiere is needed to make a video.</p>"},{"location":"running_jobs/visualization/paraview/cgal_vespa/","title":"CGAl Vespa","text":"<p>If you need advanced Computational Geometry support for your visualization and analysis, then we have you covered!</p> <p>Follow these steps to get up and running with an OOD remote desktop and a custom compiled paraview container providing cgal vespa.</p> <pre><code># pull the container\napptainer pull docker://ghcr.io/devinbayly/cgal_vespa:latest\n</code></pre> <p>When this is complete you must run the container and start paraview</p> <pre><code>apptainer exec cgal_vespa_latest.sif /opt/paraview_build/bin/paraview\n</code></pre> <p>When paraview starts you need to import the Vespa paraview plugin which will be at this system path</p> <pre><code>/usr/local/lib/paraview-5.11/plugins/VESPAPlugin/VESPAPlugin.so\n</code></pre> <p>Then the following filters can help for estimating volumes contained within areas of interest in imaging data</p> <ul> <li>threshold</li> <li>extract surface</li> <li>tetrahedralize</li> <li>Alpha Wrap (CGAL Vespa)</li> <li>Connectivity</li> <li>Compute Connected Surface Properties</li> </ul> <p>Refer to the screen recording shared here for other details, as well as the demo_state.pvsm which you can just load in the custom paraview container to get started.</p> <p></p> <p></p> <p>{\"serverDuration\": 17, \"requestCorrelationId\": \"6bc9cdeba83c4a7bb82f184a96d6b19b\"}</p>"},{"location":"running_jobs/visualization/paraview/essential_ideas/","title":"Essential Ideas","text":"<p>To get the most out of ParaView, it is essential to understand not just the steps of doing things but also why those steps work. This will help you in debugging many of the issues you run across while using the program.</p>"},{"location":"running_jobs/visualization/paraview/essential_ideas/#nodes","title":"Nodes","text":"<p>Pretty much all of ParaView is structured in a system of nodes stored in the pipeline (see this with View \u2192 Pipeline Browser from the toolbar). Nodes are sorted into three main categories:</p> <p>Sources</p> <p>Sources are, as the name implies, the source for the ParaView pipeline. This is anything that you want to bring into your visualization including data, geometric shapes, annotations, etc. In short, these are the objects that all the other nodes base themselves on.</p> <p>Filters</p> <p>Filters are the main tool of analysis in ParaView. Anytime you want to add something to your data (such as showing velocities as arrows), modify your data in some way (like projecting onto an axis), or visualize with a graph, these are the nodes to use.</p> <p>Extractors</p> <p>Extractors, unlike sources or filters, do not add or modify data in any way. They simply act as a means of taking the data that you already have in ParaView and exporting it. Examples of this include saving images or exporting data as a spreadsheet.</p> <p>The general workflow for ParaView is importing your data with sources (this is what the Open button does), performing your analysis with filters, then exporting it either as an image or spreadsheet for further analysis.</p>"},{"location":"running_jobs/visualization/paraview/essential_ideas/#the-pipeline","title":"The Pipeline","text":"<p>The pipeline is where all of your nodes will be stored. It is stored in a tree-like format, with the root of the tree always being builtin. Generally, sources and extractors will be children of the root and filters will be children of sources. Any filters that a source has as a child will affect that source object.</p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_gui/","title":"Getting Started with Paraview GUI","text":"<p>ParaView is a powerful data visualization software that many researchers can find useful for getting a visual understanding of their data. This guide will be a brief introduction to ParaView including how to install it and use its GUI interface. For more information on how to use the ParaView terminal (PvPython), see Getting Started With ParaView Terminal.</p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_gui/#installing-paraview","title":"Installing ParaView","text":"<p>You can setup ParaView on our HPC cluster or on your workstation. Follow the instructions below based on which system you want to install ParaView on.</p> <p>Choose your system</p> HPCWorkstation <p>These instructions will get you setup with the Paraview GUI on our HPC systems. Feel free to copy and paste this code into an OOD Remote Desktop Terminal, and consult the lower explanations for details about each line.</p> <pre><code>apptainer pull docker://ghcr.io/devinbayly/vtk:latest # (1)!\nwget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&amp;version=v5.11&amp;type=binary&amp;os=Linux&amp;downloadFile=ParaView-5.11.0-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview.tar.gz # (2)!\ntar xf paraview.tar.gz # (3)!\napptainer exec vtk_latest.sif ./ParaView-5.11.0-MPI-Linux-Python3.9-x86_64/bin/paraview # (4)!\n</code></pre> <ol> <li>This pulls an Apptainer container with packages prepared by our visualization consultant to run ParaView on the HPC. For more information on Apptainer containers, see What are Containers.</li> <li>This pulls the <code>paraview</code> binary from their downloads, and renames it to <code>paraview.tar.gz</code>.</li> <li>This line extracts the contents of the gzipped tar file.</li> <li>This line executes the <code>paraview</code> binary and launches the GUI.</li> </ol> <p>ParaView is available for Windows, Mac, and Linux and can be downloaded from https://www.paraview.org/download/. You can choose which version to download from a drop-down menu on the page. This tutorial uses version 5.11.0, but you can use other versions if those work for you better. Run the downloaded executable (on Linux you will have to extract the files from a tarball) and follow the instructions to install it.</p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_gui/#running-paraview-gui","title":"Running ParaView GUI","text":"<p>To run the ParaView GUI, find the ParaView executable and run it on your computer.</p> <p>Once open, close any popups and you will see the default layout.</p> <p></p> <p>If you plan on working with Python, from the top drop-down you can click View \u2192 Python Shell. This Python shell is equivalent to PvPython in the ParaView Terminal option.</p> <p>To get started with the terminal, first, add a shape. Try adding a sphere to the scene from the drop-down by clicking Sources \u2192 Geometric Shapes \u2192 Sphere. While nothing is added to the view here, you can see that in the pipeline tab on the left, a sphere has been added. To show this in the view, you can click the eye icon to the left of the new object.</p> <p>To maneuver in the view you can use your left mouse button to rotate, your right mouse button to zoom, and your middle mouse button to pan. Alternatively, Shift + right mouse button can be used to pan and the scroll wheel can be used to zoom. The view can be reset at any time with the Reset button shown below:</p> <p></p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_gui/#loading-data","title":"Loading Data","text":"<p>Let\u2019s load some example data into ParaView. Click the Open button from the toolbar (shown below), or go to File \u2192 Open, or press Crtl + O.</p> <p></p> <p>On the left side under Favorites, there should be a directory called <code>Examples</code>. From that, click <code>can.ex2</code> and hit OK.</p> <p></p> <p>No data should appear yet but information about the data should appear in the properties tab. In this, we can select what information from our data set we want to load. For this example, we can leave all the default settings. Click Apply to confirm the settings and you should see the example data appear in the view.</p> <p></p> <p></p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_gui/#changing-the-visualization","title":"Changing The Visualization","text":"<p>This visualization can be played with the green play button at the top of the screen.</p> <p></p> <p>Currently, the colors just show the two different shapes. Let\u2019s say we want to visualize the acceleration of all the points of the shape. In the toolbar, find the drop-down that currently says <code>vtkBlockColors</code> and switch it to <code>VEL</code>. Play the animation again to visualize the result.</p> <p></p> <p>While this does provide some useful information, it may not be the best way of visualizing velocity. Instead, let's add some vectors to see how each point is moving.</p> <p>Start by adding a Glyph to your data. With your data selected click the Glyph button from the toolbar. Next, set both the Orientation Array and Scale Array to <code>VEL</code> and hit Apply. You should notice that the arrows do get added but they are way too large. Change this by editing your scale factor until it looks right (0.0005 worked for me).</p> <p></p> <p>And there you have it! You can play the animation and watch as the arrows change with time.</p> <p>Later tutorials will work with specific data types as well as other features included in ParaView but after this, you should have a base understanding of how to navigate the scene and load some basic data.</p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_gui/#paraview-on-hpc-video-walkthrough","title":"ParaView on HPC video walkthrough","text":""},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_terminal/","title":"Getting Started with Paraview Terminal","text":"<p>Welcome to this tutorial on PvPython, ParaView\u2019s terminal that holds all the same functionality as ParaView\u2019s GUI interface, but allows for an interface more familiar to terminal users. If you haven\u2019t already, check out Getting Started for the installation and basic functionality of the software. This tutorial will follow the same procedure as the GUI tutorial, of course, except that it uses the terminal. A note on using the terminal: PvPython is not particularly intuitive and not particularly well documented. If you're interested in creating scripts I would recommend reading this entire article (especially How To Find Python Commands) before getting started.</p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_terminal/#paraview-terminal","title":"ParaView Terminal","text":"<p>To get started with PvPython, find the <code>pvpython</code> executable on your computer. (1)</p> <ol> <li>In Windows you can do this by searching PvPython from the Start menu.</li> </ol> <p>The first thing you should do after opening the terminal is run</p> <pre><code>from paraview.simple import *\n</code></pre> <p>This will give you all the base functionality. To access methods outside the simple package, those need to be imported separately.</p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_terminal/#basic-example","title":"Basic Example","text":"<p>Let's start off by creating a sphere and rendering it from the terminal.</p> <p>First, you'll want to create a sphere object with the following:</p> <pre><code>sphere = Sphere()\n</code></pre> <p>This creates the sphere but we also need to \u201cshow\u201d it to make it visible in the render. Do this by calling</p> <pre><code>Show()\n</code></pre> <p>Finally, to view the sphere call the command</p> <pre><code>Render()\n</code></pre> <p>This will bring up a window in which you can see your sphere. Notice that if you hover over this window it may say Not Responding. That is expected, the window will close when you close PvPython and will update whenever <code>Render</code> is called in the terminal. If you want, you can call <code>Interact()</code> to be able to interact with the view, including moving the camera, moving the window, and closing the window.</p> <p>You may notice the sphere has a pretty low resolution. Let's try making it rounder by updating its parameters. Spheres have two parameters that contribute to their resolution: <code>ThetaResolution</code> and <code>PhiResolution</code>. <code>ThetaResolution</code> is the number of points you have for each horizontal plane (lines of latitude) and <code>PhiResolution</code> is the number of points in each vertical slice (lines of longitude). Let's try increasing both of these to get a better sphere.</p> <pre><code>sphere.ThetaResolution = 20\nsphere.PhiResolution = 20\nRender()  # Changes wont update untill Render() is called\n</code></pre> <p>There you go! The sphere should get a bit smoother. Feel free to mess around with different parameters and <code>Render()</code> them to see the result. A list of parameters can be seen by calling <code>dir(sphere)</code>.</p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_terminal/#how-to-find-python-commands","title":"How To Find Python Commands","text":"<p>While some PvPython commands are intuitive or well documented, there are many commands that are difficult if not impossible to find information about. If this is the case and you're trying to find a way to do something you know you can do in the GUI version, you can use the following steps.</p> <ol> <li>In the ParaView GUI, click Tools \u2192 Start Trace from the toolbar.</li> <li>Click OK on the popup. </li> <li>Next, do whatever action you would like the PvPython command for.</li> <li>Once you're done, click Tools \u2192 End Trace and a window should pop up giving you the python commands for all the actions you did while the trace was running. </li> </ol> <p>This is also a useful way to build scripts without the need for a lot of coding. It is important to note however that the trace generates a lot of redundant or unnecessary code. If you want your program running as fast as possible, it's best to go through and only keep what you need.</p>"},{"location":"running_jobs/visualization/paraview/getting_started_with_paraview_terminal/#running-pvpython-or-a-pvpython-script-within-paraview-gui","title":"Running PvPython or a PvPython script within ParaView GUI","text":"<p>To do this, all you need to do is click View \u2192 Python Shell from the toolbar. This will open a PvPython shell the same as the one in the terminal. At the bottom of this window, there is a button labeled Run Script. Clicking this will allow you to select a PvPython script to run within ParaView.</p>"},{"location":"running_jobs/visualization/paraview/graphs_and_exporting_data/","title":"Graphs and Exporting Data","text":"<p>While visualizations in the terminal are ParaView's strength, it also includes tools to plot data for different types of visualizations.</p>"},{"location":"running_jobs/visualization/paraview/graphs_and_exporting_data/#creating-graphs-in-paraview","title":"Creating Graphs In Paraview","text":"<p>For this, I\u2019m going to use the same example as in Getting Started With ParaView GUI. The first thing you want to do is select the source you want to work with. Next, from the toolbar select Filters \u2192 Data Analysis then select whichever plot works best for you. For this example, I\u2019m going to select Plot Data Over Time. For some options, a warning will pop up stating that it could take a long time to plot the data. This is certainly the case for large data sets but if you're using the example, it should take only a second. Click apply in the properties tab and an indicator in the bottom right should appear as your data gets processed.</p> <p></p> <p>Once your data is fully processed, a graph should appear, splitting the viewport in two. Keep in mind that what is visible is view specific so selecting the 3D viewport will cause an eye to appear next to the base source in the pipeline (indicating that it is what's being shown in this view), and likewise selecting the graph view will remove the eye on the source and put it on the new graph filter.</p> <p></p> <p>To demonstrate, compare the two images above. In the first, I clicked on the viewport giving it a blue border to indicate it's selected and also placing the eye icon on can.ex2 in the pipeline (because it's what's being shown in the viewport). Then in the second image, once I click on the graph, the blue border changes and the eye icon moves to PlotDataOverTime1.</p> <p>From this point, you can use the properties menu to modify the values of the graph as you see fit, making sure to have the graph selected in the Pipeline Browser. After changes are made, many will require you to press Apply at the top of the properties menu for them to take effect.</p>"},{"location":"running_jobs/visualization/paraview/graphs_and_exporting_data/#exporting-data-to-a-spreadsheet","title":"Exporting Data To A Spreadsheet","text":"<p>While ParaView does have many data management tools and graphs, it's certainly not as many as other data analysis software. To use the data in other programs, you'll need to export it to a format of your choice, probably the most simple of which is a CSV (a standard format for spreadsheets).</p> <p>Select your source whose data you'd like to export, then, from the toolbar, go to Extractors \u2192 Data \u2192 CSV. Make the desired changes in the properties tab, press Apply, then go to File \u2192 Save Extracts. At this point select where you would like to save the data and it should generate CSVs in the specified folder.</p>"},{"location":"running_jobs/visualization/paraview/graphs_and_exporting_data/#common-issues","title":"Common Issues","text":"<p>If your data has outliers or null values, many plotting functions have difficulties correctly reading the data. If this is the case, apply a Threshold filter (Filters \u2192 Common \u2192 Threshold), scale the data to whatever range you want, then apply the graph to the new threshold filter. Note that you'll have to remove the old graph, add the threshold, then re-add the graph to the threshold.</p> <p>For example (before and after adding Threshold):</p> <p></p>"},{"location":"running_jobs/visualization/paraview/headless_batch_rendering/","title":"Headless Batch Rendering","text":"<p>When producing a rendering video we can speed things up tremendously by recognizing that each frame is independent of every other. This allows us to perform the rendering of each frame of a video output in parallel. In this tutorial you will learn to prepare a scene in ParaView, and save its \"state\" so that Slurm array jobs can then render all the frames in parallel, taking a fraction of the time it would otherwise take. At the end of the process you will use FFmpeg to stitch all the frames together into a movie.</p>"},{"location":"running_jobs/visualization/paraview/headless_batch_rendering/#making-an-animation-with-paraview-gui","title":"Making an animation with ParaView GUI","text":"<p>Follow the steps in Getting Started With ParaView GUI to launch ParaView. Generate a simple geometric shape, such as a cone, for this tutorial. You can select one from the Sources \u2192 Geometric Shapes drop down menu. Select Apply to see the geometry in the viewport on the right.</p> <p></p> <p>Select the Animation View option from the View drop down menu. A pane titled Animation View will appear in the bottom of the window.</p> <p></p> <p>The Animation View pane will show an option to add an orbit camera animation track. Click the blue cross to add it. For more information on cameras, see Cameras And Keyframes.</p> <p></p> <p>After adding a Camera, double click on it. This will open a window titled Animation Keyframes, containing a table. Double click the cell that says Path.</p> <p></p> <p>This will open up a window titled Key Frame Interpolation.</p> <p></p> <p>Selecting the Camera Position element will add a yellow line added to the viewport containing the cone geometry. You can move the yellow line from the middle of the geometric shape with the middle mouse button.</p> <p></p> <p>Select the Up Direction element in the Key Frame Interpolation window and change the values to 0, 0, 1. Without this the camera will face sideways. </p> <p></p> <p>Click Ok, and return to the Animation View. Set both the Number of Frames and End Time to 1000. This will be important later when we use Slurm array jobs to produce a rendering video, as it will allow Slurm to identify frames by whole numbers. </p> <p></p> <p>Click the Play button and see the camera orbiting around the geometric shape in the viewport. If this looks correct, save the ParaView state file by either clicking the Save State button, or selecting Save State from the File drop down menu.</p> <p></p>"},{"location":"running_jobs/visualization/paraview/headless_batch_rendering/#parallel-rendering-with-slurm-array-jobs","title":"Parallel Rendering with Slurm Array Jobs","text":"<p>The ParaView state file represents the data corresponding to the geometric shape and the animated camera perspective. We will render it in parallel with Slurm array jobs and create a video. </p> <p>Start an interactive session on the HPC, and pull an Apptainer container with a headless version of ParaView 5.11.0 in it: <pre><code>apptainer pull oras://ghcr.io/e-eight/containers/paraview-headless\n</code></pre> This will create a file called <code>paraview-headless_latest.sif</code>. Headless means that the program does not have access to a display to make graphical outputs. This works because we will use ParaView only to render the frames, not to view the movie. If the pulling the container fails for any reason, you can create one from an Apptainer definition file containing the following: <pre><code>Bootstrap: docker\nFrom: debian:bookworm-slim\n\n%post\n  apt update -y\n  apt install -y libgomp1 curl wget libglu1-mesa-dev freeglut3-dev mesa-common-dev libxcursor*\n  cd /opt\n  wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&amp;version=v5.11&amp;type=binary&amp;os=Linux&amp;downloadFile=ParaView-5.11.0-osmesa-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview.tar.gz\n  tar xf paraview.tar.gz\n  rm paraview.tar.gz\n  apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n%environment\n  export PATH=/opt/ParaView-5.11.0-osmesa-MPI-Linux-Python3.9-x86_64/bin:$PATH\n</code></pre> See Containers to learn more about Apptainer containers, and how to create them. </p> <p>We will now construct one Slurm script and one Python script that we will use for our rendering. The Slurm script will submit an array job. For more information, on Slurm array jobs, see Array Jobs. The Python script will contain all the logic for rendering the frames with ParaView.</p>"},{"location":"running_jobs/visualization/paraview/headless_batch_rendering/#slurm-script","title":"Slurm Script","text":"<p>In the following script replace <code>visteam</code> with your allocation account. </p> headless_batch.slurm<pre><code>#!/bin/bash\n#SBATCH --output=logs/%x-%a.out\n#SBATCH --error=logs/%x-%a.err\n#SBATCH --job-name=paraview-headless\n#SBATCH --ntasks=1\n#SBATCH --nodes=1\n#SBATCH --time=00:30:00\n#SBATCH --partition=standard\n#SBATCH --account=visteam\n\npvsm_pth=$1\napptainer exec paraview-headless_latest.sif pvpython render.py --pvsm \"$pvsm_pth\" --frame ${SLURM_ARRAY_TASK_ID}\n</code></pre> <p>As it is written this will allocate 1 CPU task for the task of headlessly rendering a single frame with an upper time limit of 30 mins. Note that there is a <code>${SLURM_ARRAY_TASK_ID}</code> environment variable in use but no <code>#SBATCH --array=</code> line. This is because it is often nice to have the option to specify the size of the array job at run time as we will see below. This will simply start the Apptainer container for each array job and execute a script that is a wrapper around the ParaView <code>pvpython</code> program. For more information on the <code>pvpython</code> program, see Getting Started with ParaView Terminal.</p> <p>This next file is where many of the interesting bits actually are. </p>"},{"location":"running_jobs/visualization/paraview/headless_batch_rendering/#python-script","title":"Python Script","text":"<p>This script was initially generated with the <code>trace</code> utility built into ParaView. With the <code>trace</code> utility you can make a Python script by recording interactions with the ParaView GUI. For more information, see Trace Recorder. The comments in the script explain what the code does.</p> render.py<pre><code># state file generated using paraview version 5.11.0\nfrom paraview.simple import *\nimport paraview\nimport os\nimport argparse\nfrom pathlib import Path\nimport time\n\n# this is for helping us determine the run time of each frame's render\nstart = time.perf_counter()\n# get frame number to render and the path to the pvsm file to load\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--pvsm\", type=Path)\nparser.add_argument(\"--frame\")\nargs = parser.parse_args()\n\n# load the pvsm file\n# this brings in our geometric shape, as well as the animation track that we created with the orbiting camera\nparaview.simple.LoadState(args.pvsm.name)\nparaview.compatibility.major = 5\nparaview.compatibility.minor = 11\n\n#### import the simple module from the paraview\n#### disable automatic camera reset on 'Show'\nparaview.simple._DisableFirstRenderCameraReset()\n\n# ----------------------------------------------------------------\n# setup views used in the visualization\n# ----------------------------------------------------------------\ntimekeeper = GetTimeKeeper()\n\nrenderView1 = GetActiveViewOrCreate(\"RenderView\")\n# ensure axes are hidden\nrenderView1.OrientationAxesVisibility = 0\nrenderView1.CenterAxesVisibility = 0\n\n# ----------------------------------------------------------------\n# setup color maps and opacity mapes used in the visualization\n# note: the Get..() functions create a new object, if needed\n# ----------------------------------------------------------------\n\nprint(GetSources())\n\n# find out how many frames are in our animation\n\nanim = GetAnimationScene()\nprint(\"animation length is\",anim.EndTime)\n\n\nif __name__ == '__main__':\n# figure out what the output folder is for the frame\n\n    render_folder = Path(f\"{Path(args.pvsm.stem)}_renders\")\n\n# we will make this folder if it doesn't already exist\n    render_folder.mkdir(parents=True,exist_ok =True)\n\n# get the argument provided  indicating which frame we are supposed to render\n    frame = int(args.frame)\n\n# find out the maximum array job id\n    max_array_id = int(os.environ[\"SLURM_ARRAY_TASK_MAX\"])\n    print(\"starting render\")\n\n# as long as we aren't above the last frame number continue to run\n    while (frame &lt; int(anim.EndTime)):\n      print(\"running frame,\",frame)\n\n# move our animation track head to the frame we want to render\n      anim.AnimationTime = float(frame)\n\n# establish the HD and 4K file paths for our still frame render\n      png_pth_hd = Path(f\"{render_folder}/frame_{frame:06d}_HD.png\")\n      png_pth_4k = Path(f\"{render_folder}/frame_{frame:06d}_4K.png\")\n\n# if there's already renders for this frame go ahead and skip\n      if png_pth_hd.exists() and png_pth_4k.exists():\n        print(\"skipping, already exists\")\n        frame+=max_array_id\n        continue\n\n# otherwise go ahead and render them out using the correct resolutions\n      SaveScreenshot(f\"{png_pth_hd}\",renderView1,ImageResolution=[1920,1080])\n      SaveScreenshot(f\"{png_pth_4k}\",renderView1,ImageResolution=[3840,2160])\n      print(\"saved\")\n\n# move on to the next frame that might need rendering using the number of array jobs as our offset\n# this ensures that we can use smaller sets of array jobs and still render all the frames we need.\n      frame+=max_array_id\n\n# get an end time for performance measuring\n    end = time.perf_counter()\n    print(f\"elapsed {end-start}\")\n</code></pre>"},{"location":"running_jobs/visualization/paraview/headless_batch_rendering/#submitting-array-jobs","title":"Submitting Array Jobs","text":"<p>Submit a Slurm job array with the following, replacing <code>&lt;state_file&gt;</code> with the path to the ParaView state file that you had saved earlier: <pre><code>sbatch --array=0-900 headless_batch.sh &lt;state_file&gt;\n</code></pre> The <code>--array</code> option allows us to dynamically specify the number of array jobs that we want to run, <code>0-900</code> means we specified a job array with array indices 0 to 900. After submitting the job, it might take a while to start, depending on the resource availability of the cluster. You can check the status of the job with <code>squeue -u &lt;netid&gt;</code>, replacing <code>&lt;netid&gt;</code> with your NetID. Assuming you have not submitted any other jobs and this job has started, you will see an output like the following: <pre><code>             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n 2125119_[101-900]  standard paraview sohampal PD       0:00      1 (None)\n         2125119_4  standard paraview sohampal  R       0:01      1 cpu39\n         2125119_5  standard paraview sohampal  R       0:01      1 cpu43\n         2125119_6  standard paraview sohampal  R       0:01      1 cpu43\n         2125119_7  standard paraview sohampal  R       0:01      1 cpu45\n         2125119_8  standard paraview sohampal  R       0:01      1 cpu45\n         2125119_9  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_10  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_11  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_12  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_13  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_14  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_15  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_16  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_17  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_18  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_19  standard paraview sohampal  R       0:01      1 cpu45\n        2125119_20  standard paraview sohampal  R       0:01      1 cpu45\n...\n</code></pre> The output has been truncated to fit in to the space. You can see the jobs with array indices 101 to 900 are yet to start, while jobs with lower array indices are running. It will take close to 14 seconds per frame for a simple animated scene like this, and since there are more frames to render than there are jobs to run, some jobs will take longer than others because they will render more than one frame. Perform either an HD or a 4K render of your material, instead of both, if the render times become too long.</p>"},{"location":"running_jobs/visualization/paraview/headless_batch_rendering/#rendering-movie","title":"Rendering Movie","text":"<p>From an interactive session load the <code>ffmpeg</code> module:</p> <p><pre><code>module load ffmpeg\n</code></pre> Stitch together all the frames to generate a video, replacing <code>&lt;video_name&gt;</code> with the desired name for your video file:</p> <p><pre><code>ffmpeg -i frame_%06d_HD.png -r 10 -c:v copy &lt;video_name&gt;\n</code></pre> You can automate the creation of this video with Slurm job dependencies. For more information, see Job Dependencies. This is the highest possible quality video that you can use in other video editing software without any codec. It shows the entire render in sequence even though all the frames were generated in parallel. Below is a gif created from the video file for a cone geometry.</p> <p></p>"},{"location":"running_jobs/visualization/paraview/paraview_python_plugins/","title":"Paraview Python Plugins","text":"<p>If you've spent more than a minute with ParaView you're already aware that there are some things that could use a little more explanation. There's really just too much to cover in the depth that most beginners/intermediate users need. This page is meant to help shed a little bit of light on another subject that is shrouded in a deep layer of mystery, namely making plugins with Python. Unfortunately, this tutorial is not without sections that are less well understood, but hopefully in between those sections there's some value!</p> <p>For many purposes it might be useful to look through examples of scripts and plugins, so for access to an ever evolving scratch space of these kinds of files please refer to https://github.com/DevinBayly/paraview_code.</p> <p>Before going to much further just know this page will not document or explain things like the decoration options that create widgets for filter options. In fact, there are quite a few things related to the <code>@</code>-prefixing of methods that won't be discussed until a later date. Due to this limitation it may be useful to layout upfront that much of the true instructional material is captured in comments on the source code from https://github.com/Kitware/ParaView/blob/cdb6fa76693885dd14af49d91d9b6177710f7267/Examples/Plugins/PythonAlgorithm/PythonAlgorithmExamples.py, and more generally from https://github.com/Kitware/ParaView/tree/cdb6fa76693885dd14af49d91d9b6177710f7267/Examples/Plugins.</p>"},{"location":"running_jobs/visualization/paraview/paraview_python_plugins/#example-plugin","title":"Example Plugin","text":"<p>https://github.com/DevinBayly/paraview_code/blob/main/time_series_reader_plugin.py</p> <p>This plugin was designed to make it possible to point ParaView to a folder of Python NumPy array files that each store a point cloud at a particular moment in time. The benefit of using this plugin is that now in ParaView as we scrub through a timeline interface element the plugin will load new NumPy files and update the viewport. The sections that follow will attempt to explain what the different methods of the plugin do, and this will ideally provide departure points for reader's own usecases.</p> <p>First the entire source is shown and then the step by step sections will follow.</p> <pre><code>\"\"\"This module demonstrates various ways of adding\nVTKPythonAlgorithmBase subclasses as filters, sources, readers,\nand writers in ParaView\"\"\"\n\n\n# This is module to import. It provides VTKPythonAlgorithmBase, the base class\n# for all Python-based vtkAlgorithm subclasses in VTK and decorators used to\n# 'register' the algorithm with ParaView along with information about UI.\nfrom paraview.util.vtkAlgorithm import *\nimport uuid\nfrom pathlib import Path\n\n\n\n#------------------------------------------------------------------------------\n# A reader example.\n#------------------------------------------------------------------------------\ndef createModifiedCallback(anobject):\n    import weakref\n    weakref_obj = weakref.ref(anobject)\n    anobject = None\n    def _markmodified(*args, **kwars):\n        o = weakref_obj()\n        if o is not None:\n            o.Modified()\n    return _markmodified\n\n# To add a reader, we can use the following decorators\n#   @smproxy.source(name=\"PythonnumpyReader\", label=\"Python-based numpy Reader\")\n#   @smhint.xml(\"\"\"&lt;ReaderFactory extensions=\"csv\" file_description=\"NumPy numpy files\" /&gt;\"\"\")\n# or directly use the \"@reader\" decorator.\n@smproxy.reader(name=\"Sama Lidar Numpy Reader\",\n                label=\"Python-based Numpy pcd Reader for timeseries data\",\n                extensions=\"npy\",\n                file_description=\"Numpy files\")\nclass PythonNumpyPCDReader(VTKPythonAlgorithmBase):\n    \"\"\"A reader that reads a NumPy file. If the NumPy has a \"time\" column, then\n    the data is treated as a temporal dataset\"\"\"\n    def __init__(self):\n        VTKPythonAlgorithmBase.__init__(self, nInputPorts=0, nOutputPorts=1, outputType='vtkPolyData')\n        self._filename = None\n        self._ndata = None\n        self._timesteps = None\n        print(\"starting\",uuid.uuid1())\n\n        from vtkmodules.vtkCommonCore import vtkDataArraySelection\n        self._arrayselection = vtkDataArraySelection()\n        self._arrayselection.AddObserver(\"ModifiedEvent\", createModifiedCallback(self))\n\n    def _get_raw_data(self, requested_time=None):\n        import numpy\n        if self._ndata is not None:\n            if requested_time is not None:\n                ##### load specific npy file from fnmes\n                fname = self.fnames[int(requested_time)]\n                self._ndata = numpy.load(fname)\n                print(self._ndata.dtype)\n                # self._ndata.dtype = numpy.dtype([(\"x\",numpy.float32),(\"y\",numpy.float32),(\"z\",numpy.float32),(\"intensity\",numpy.float32)])\n                return self._ndata\n            return self._ndata\n\n        if self._filename is None:\n            # Note, exceptions are totally fine!\n            raise RuntimeError(\"No filename specified\")\n\n        # self._ndata = numpy.genfromtxt(self._filename, dtype=None, names=True, delimiter=',', autostrip=True)\n        self.pth = Path(self._filename)\n        self.fnames = list(self.pth.parent.rglob(\"*npy\"))\n        self.fnames.sort()\n        times =  [i for i,e in enumerate(self.fnames)]\n        self._ndata = 0\n        self._timesteps = times\n\n        return self._get_raw_data(requested_time)\n\n    def _get_timesteps(self):\n        self._get_raw_data()\n        return self._timesteps if self._timesteps is not None else None\n\n        def _get_update_time(self, outInfo):\n        executive = self.GetExecutive()\n        timesteps = self._get_timesteps()\n        if timesteps is None or len(timesteps) == 0:\n            return None\n        elif outInfo.Has(executive.UPDATE_TIME_STEP()) and len(timesteps) &gt; 0:\n            utime = outInfo.Get(executive.UPDATE_TIME_STEP())\n            print(\"using inner method get update time\",utime)\n            dtime = timesteps[0]\n            for atime in timesteps:\n                if atime &gt; utime:\n                    return dtime\n                else:\n                    dtime = atime\n            return dtime\n        else:\n            assert(len(timesteps) &gt; 0)\n            return timesteps[0]\n\n    def _get_array_selection(self):\n        return self._arrayselection\n\n    @smproperty.stringvector(name=\"FileName\")\n    @smdomain.filelist()\n    @smhint.filechooser(extensions=\"npy\", file_description=\"Numpy pcd file\")\n    def SetFileName(self, name):\n        \"\"\"Specify filename for the file to read.\"\"\"\n        print(name)\n        if self._filename != name:\n            self._filename = name\n            self._ndata = None\n            self._timesteps = None\n            self.Modified()\n\n    @smproperty.doublevector(name=\"TimestepValues\", information_only=\"1\", si_class=\"vtkSITimeStepsProperty\")\n    def GetTimestepValues(self):\n        print(\"getting time steps\")\n        return self._get_timesteps()\n\n    # Array selection API is typical with readers in VTK\n    # This is intended to allow ability for users to choose which arrays to\n    # load. To expose that in ParaView, simply use the\n    # smproperty.dataarrayselection().\n    # This method **must** return a `vtkDataArraySelection` instance.\n    @smproperty.dataarrayselection(name=\"Arrays\")\n    def GetDataArraySelection(self):\n        return self._get_array_selection()\n\n    def RequestInformation(self, request, inInfoVec, outInfoVec):\n        print(\"requesting information\")\n        executive = self.GetExecutive()\n        outInfo = outInfoVec.GetInformationObject(0)\n        outInfo.Remove(executive.TIME_STEPS())\n        outInfo.Remove(executive.TIME_RANGE())\n\n        timesteps = self._get_timesteps()\n        if timesteps is not None:\n            for t in timesteps:\n                outInfo.Append(executive.TIME_STEPS(), t)\n            outInfo.Append(executive.TIME_RANGE(), timesteps[0])\n            outInfo.Append(executive.TIME_RANGE(), timesteps[-1])\n        return 1\n\n    def RequestData(self, request, inInfoVec, outInfoVec):\n        print(\"requesting data\")\n        from vtkmodules.vtkCommonDataModel import vtkPolyData\n        from vtkmodules.numpy_interface import dataset_adapter as dsa\n        import vtk\n\n        data_time = self._get_update_time(outInfoVec.GetInformationObject(0))\n\n        output = dsa.WrapDataObject(vtkPolyData.GetData(outInfoVec, 0))\n        points = self._get_raw_data(data_time)\n        #points = self._ndata\n\n        vpoints = vtk.vtkPoints()\n        vpoints.SetNumberOfPoints(points.shape[0])\n        intensity = vtk.vtkFloatArray()\n        intensity.SetNumberOfComponents(1)\n        intensity.SetName(\"Intensity\")\n        intensity.SetNumberOfTuples(points.shape[0])\n        for i in range(points.shape[0]):\n            vpoints.SetPoint(i, points[i][:3])\n            intensity.SetTuple1(i, points[i][3])\n\n        output.GetPointData().SetScalars(intensity)\n\n        output.SetPoints(vpoints)\n\n        vcells = vtk.vtkCellArray()\n\n        for i in range(points.shape[0]):\n            vcells.InsertNextCell(1)\n            vcells.InsertCellPoint(i)\n\n        output.SetVerts(vcells)\n\n        if data_time is not None:\n            output.GetInformation().Set(output.DATA_TIME_STEP(), data_time)\n        return 1\n</code></pre> <p>Breaking this down we will begin with a helper function from the top.</p> <pre><code>\"\"\"This module demonstrates various ways of adding\nVTKPythonAlgorithmBase subclasses as filters, sources, readers,\nand writers in ParaView\"\"\"\n\n\n# This is module to import. It provides VTKPythonAlgorithmBase, the base class\n# for all Python-based vtkAlgorithm subclasses in VTK and decorators used to\n# 'register' the algorithm with ParaView along with information about UI.\nfrom paraview.util.vtkAlgorithm import *\nimport uuid\nfrom pathlib import Path\n\n\n\n#------------------------------------------------------------------------------\n# A reader example.\n#------------------------------------------------------------------------------\ndef createModifiedCallback(anobject):\n    import weakref\n    weakref_obj = weakref.ref(anobject)\n    anobject = None\n    def _markmodified(*args, **kwars):\n        o = weakref_obj()\n        if o is not None:\n            o.Modified()\n    return _markmodified\n</code></pre> <p>Starting off with a bit of a still unclear boilerplate helper function. Best guess is this function is used later in the class to make sure that ParaView tracks that our object (NumPy point cloud) has been modified, and the downstream rendering updates need to happen. This may be because ParaView has a lazy pipeline evaluation implementation, where filters in the processing pipeline only evaluate when changes have occurred.</p> <p>Now lets inspect the beginning of the actual class defining the NumPy array reader plugin.</p> <p>So this class can be named whatever we like, but it must inherit from the Python algorithm base. In my case I've filled in some of the values in the header so that the reader works with \"npy\" extension files.</p> <p>As with other inherited classes in Python it makes sense to initialize the parent class within the <code>init</code> method of the subclass.</p> <p>The lines creating member variables are used later on within the class methods, but aren't intended to be accessed by the user so they start with underscores.</p> <p>The print statement is also just a sanity check that we have started the initialization of the plugin prior reading in an actual file. It's nice for development's sake to have a message that looks slightly different if changes to the source are made and the plugin is reloaded.</p> <pre><code># To add a reader, we can use the following decorators\n#   @smproxy.source(name=\"PythonnumpyReader\", label=\"Python-based Numpy Reader\")\n#   @smhint.xml(\"\"\"&lt;ReaderFactory extensions=\"csv\" file_description=\"Numpy numpy files\" /&gt;\"\"\")\n# or directly use the \"@reader\" decorator.\n@smproxy.reader(name=\"Sama Lidar Numpy Reader\", label=\"Python-based Numpy pcd Reader for timeseries data\",\n                extensions=\"npy\",\n                 file_description=\"Numpy files\")\nclass PythonNumpyPCDReader(VTKPythonAlgorithmBase):\n    \"\"\"A reader that reads a NumPy file. If the NumPy has a \"time\" column, then\n    the data is treated as a temporal dataset\"\"\"\n    def __init__(self):\n        VTKPythonAlgorithmBase.__init__(self, nInputPorts=0, nOutputPorts=1, outputType='vtkPolyData')\n        self._filename = None\n        self._ndata = None\n        self._timesteps = None\n        print(\"starting\",uuid.uuid1())\n\n        from vtkmodules.vtkCommonCore import vtkDataArraySelection\n        self._arrayselection = vtkDataArraySelection()\n        self._arrayselection.AddObserver(\"ModifiedEvent\", createModifiedCallback(self))\n</code></pre> <p>This method is used by a different non private method, but is intended to respond to ParaView's requests to get data. These requests are usually the result of something in the pipeline before a filter changing, or in our case having a new timestamp selected from the animation window.</p> <p>We are importing NumPy here because we will be loading filenames and in some cases changing the data type of the array. If the point cloud (taken from a Lidar) was not just point positions but also intensity it becomes easier to access the intensity as a attribute for rendering if we make the array a named datatype array.</p> <p>The top of the method handles cases when we have already read the initial time point and are querying for new data. If we already have data in the member variable <code>_ndata</code>, and the requested <code>_time</code> was provided when the method was called then we use that information to look up another file from a list of filenames (<code>self.fnames</code>).</p> <p>NOTE, this code is written for a case when the timeline is using positive integer time steps. This means that we can use the requested <code>_time</code> as an index into the filenames list to load the requested timesteps point cloud. If you have floating point timeline values a different approach will be required.</p> <p>Later in the method is the code that is used when we are getting the raw data for the first time. We take the user selected file from the filebrowser and work out the path to the parent folder. Then we get a list of the contents of this folder that match the extension that we care about in this file reader. We can then fill in the member variables for <code>_ndata</code> and <code>_timesteps</code>. At this point we now can re-issue the call to this method and it will take a different path through loading data from the NumPy array out of a file. This allows us to still make use of the same code that all other invocations will use even though this time the method wasn't called in response to a change in the timeline.</p> <pre><code>def _get_raw_data(self, requested_time=None):\n    import numpy\n    if self._ndata is not None:\n        if requested_time is not None:\n            ##### load specific npy file from fnmes\n            fname = self.fnames[int(requested_time)]\n            self._ndata = numpy.load(fname)\n            print(self._ndata.dtype)\n            # self._ndata.dtype = numpy.dtype([(\"x\",numpy.float32),(\"y\",numpy.float32),(\"z\",numpy.float32),(\"intensity\",numpy.float32)])\n            return self._ndata\n        return self._ndata\n\n    if self._filename is None:\n        # Note, exceptions are totally fine!\n        raise RuntimeError(\"No filename specified\")\n\n    # self._ndata = numpy.genfromtxt(self._filename, dtype=None, names=True, delimiter=',', autostrip=True)\n    self.pth = Path(self._filename)\n    self.fnames = list(self.pth.parent.rglob(\"*npy\"))\n    self.fnames.sort()\n    times =  [i for i,e in enumerate(self.fnames)]\n    self._ndata = 0\n    self._timesteps = times\n\n    return self._get_raw_data(requested_time)\n</code></pre> <p>After the last method you get a break since this one is much shorter. When we looking to find out how many timesteps there are in the animation we need to know how many files are in the folder we are processing. The logic there is that we want a time step per file. In support of this we issue a call to the previous method <code>_get_raw_data</code> because that's what creates the member variable <code>_timesteps</code> .</p> <pre><code>def _get_timesteps(self):\n    self._get_raw_data()\n    return self._timesteps if self._timesteps is not None else None\n</code></pre> <p>This method for the most part is just recommended ParaView Python code straight from their section on Python programmable filters. Essentially it just returns back the updated time making sure its less than the values within the timesteps provided, otherwise it gives back the initial timestep.</p> <p>The <code>_get_array_selection(self)</code> method is included here too just because it's a one liner. I think this code might be dead as it's not actually getting set to anything in my case as far as I know. The <code>_arrayselection</code> is set in the <code>init</code>, but doesn't appear explicitly anywhere else.</p> <pre><code>def _get_update_time(self, outInfo):\n    executive = self.GetExecutive()\n    timesteps = self._get_timesteps()\n    if timesteps is None or len(timesteps) == 0:\n        return None\n    elif outInfo.Has(executive.UPDATE_TIME_STEP()) and len(timesteps) &gt; 0:\n        utime = outInfo.Get(executive.UPDATE_TIME_STEP())\n        print(\"using inner method get update time\",utime)\n        dtime = timesteps[0]\n        for atime in timesteps:\n            if atime &gt; utime:\n                return dtime\n            else:\n                dtime = atime\n        return dtime\n    else:\n        assert(len(timesteps) &gt; 0)\n        return timesteps[0]\n\ndef _get_array_selection(self):\n    return self._arrayselection\n\n\n# Array selection API is typical with readers in VTK\n# This is intended to allow ability for users to choose which arrays to\n# load. To expose that in ParaView, simply use the\n# smproperty.dataarrayselection().\n# This method **must** return a `vtkDataArraySelection` instance.\n@smproperty.dataarrayselection(name=\"Arrays\")\ndef GetDataArraySelection(self):\n    return self._get_array_selection()\n</code></pre> <p>This is the next most important part where we are actually setting a filename using a widget. This method will receive values back after the user selects a file in the filebrowser. To trigger all this they must load the plugin and then use the \"open\" button. Once they select a file who's extension matches with this, the custom plugin reader will show up with any others that match the extension. Last I checked there weren't any others that were stepping forward to handle NumPy array input.</p> <p>This method just checks whether the same file has been loaded twice, and then sets itself to modified to trigger the subsequent loading and rendering steps.</p> <pre><code>@smproperty.stringvector(name=\"FileName\")\n@smdomain.filelist()\n@smhint.filechooser(extensions=\"npy\", file_description=\"numpy pcd file\")\ndef SetFileName(self, name):\n    \"\"\"Specify filename for the file to read.\"\"\"\n    print(name)\n    if self._filename != name:\n        self._filename = name\n        self._ndata = None\n        self._timesteps = None\n        self.Modified()\n</code></pre> <p>This is the public wrapper of the <code>_get_timesteps()</code> method which will be called after using the reader on a file that matches the extension supported. This method needs to be implemented for the algorithm class we are inheriting from to work correctly.</p> <pre><code>@smproperty.doublevector(name=\"TimestepValues\", information_only=\"1\", si_class=\"vtkSITimeStepsProperty\")\ndef GetTimestepValues(self):\n    print(\"getting time steps\")\n    return self._get_timesteps()\n</code></pre> <p>This method is another of the ones that must be extended in the child class for the parent to work. This specifically is a method that responds to the changes in the timeline. For more information on this request for information please read lower down in this section https://docs.paraview.org/en/latest/ReferenceManual/pythonProgrammableFilter.html#understanding-the-programmable-modules. It will talk about how the <code>RequestInformation</code> is actually part of a \"pass in the pipeline's execution.\" Essentially this helps ParaView know that the reader plugin can provide more data at different timesteps. This method will also ensure that the animation timeline gets set to the right length as far as I know.</p> <pre><code>def RequestInformation(self, request, inInfoVec, outInfoVec):\n    print(\"requesting information\")\n    executive = self.GetExecutive()\n    outInfo = outInfoVec.GetInformationObject(0)\n    outInfo.Remove(executive.TIME_STEPS())\n    outInfo.Remove(executive.TIME_RANGE())\n\n    timesteps = self._get_timesteps()\n    if timesteps is not None:\n        for t in timesteps:\n            outInfo.Append(executive.TIME_STEPS(), t)\n        outInfo.Append(executive.TIME_RANGE(), timesteps[0])\n        outInfo.Append(executive.TIME_RANGE(), timesteps[-1])\n    return 1\n</code></pre> <p>You've almost made it! The last method does almost all the heavy lifting. This again is covered in the section on programmable sources, and gives ParaView back something to show at the end of the execution pass.</p> <p>We load in several packages that are required to convert to VTK datatypes from NumPy.</p> <p>Then we get the time that we are supposed to load as a point cloud.</p> <p>We must have a variable representing the output on which to set our results so that ParaView can show them.</p> <p>Then we get the actual NumPy array representing the points in the point cloud for a particular data <code>_time</code>.</p> <p>Then we construct a vpoints variable which is an empty VTK point set with an expected number of points matching the number in our NumPy data.</p> <p>If our data set has an intensity attribute then we can create a separate float array that will be set as a scalar point data on the output.</p> <p>After that we need to set up the correct cell to be shown in ParaView. As I understand it, this part is standard VTK Python coding. Once the output has it's cells set we are done.</p> <pre><code>def RequestData(self, request, inInfoVec, outInfoVec):\n    print(\"requesting data\")\n    from vtkmodules.vtkCommonDataModel import vtkPolyData\n    from vtkmodules.numpy_interface import dataset_adapter as dsa\n    import vtk\n\n    data_time = self._get_update_time(outInfoVec.GetInformationObject(0))\n\n    output = dsa.WrapDataObject(vtkPolyData.GetData(outInfoVec, 0))\n    points = self._get_raw_data(data_time)\n    #points = self._ndata\n\n    vpoints = vtk.vtkPoints()\n    vpoints.SetNumberOfPoints(points.shape[0])\n    intensity = vtk.vtkFloatArray()\n    intensity.SetNumberOfComponents(1)\n    intensity.SetName(\"Intensity\")\n    intensity.SetNumberOfTuples(points.shape[0])\n    for i in range(points.shape[0]):\n        vpoints.SetPoint(i, points[i][:3])\n        intensity.SetTuple1(i, points[i][3])\n\n    output.GetPointData().SetScalars(intensity)\n\n    output.SetPoints(vpoints)\n\n    vcells = vtk.vtkCellArray()\n\n    for i in range(points.shape[0]):\n        vcells.InsertNextCell(1)\n        vcells.InsertCellPoint(i)\n\n    output.SetVerts(vcells)\n\n    if data_time is not None:\n        output.GetInformation().Set(output.DATA_TIME_STEP(), data_time)\n    return 1\n</code></pre>"},{"location":"running_jobs/visualization/paraview/speeding_up_filters_and_rendering_with_client_server_configurations/","title":"Speeding up Filters & Rendering with Client Server configurations","text":"<p>For Both of these configurations it is recommended that you start up a remote desktop and launch paraview according to the instructions on the previous documented page Getting Started With ParaView. Using the Embedded graphics library paraview binary</p>"},{"location":"running_jobs/visualization/paraview/speeding_up_filters_and_rendering_with_client_server_configurations/#steps-to-launch-a-egl-server","title":"Steps To launch a EGL server","text":"<p>This workflow uses offscreen rendering and a gpu to speed up interactive workflows.</p> <p>First request an allocation on Ocelote that has 1 gpu Please remember to replace the <code>&lt;your account&gt;</code> with your own account name</p> <p><code>salloc -A &lt;your account&gt; -p standard --gres=gpu:1 -t 3:00:00 -N 1 -n 16</code></p> <p>The following lines will download a version of paraview that we can use for offscreen headless rendering. The command will actually rename the export file in order to make it easier to uncompress it.</p> <p><code>wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&amp;version=v5.11&amp;type=binary&amp;os=Linux&amp;downloadFile=ParaView-5.11.1-egl-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview_egl.tar.gz</code></p> <p>Here we uncompress the tar file</p> <p><code>tar xf paraview_egl.tar.gz</code></p> <p>The folder we get from this will probably include the full paraview version name so we will use a * pattern match character to go into the bin folder where the <code>pvserver</code> program we need to use resides</p> <p><code>cd ParaView-5.11.1*/bin/</code></p> <p>Here's where the exciting part occurs. We will use the <code>pvserver</code> program to start up a process which listens for connections from the graphical interface and fulfills requests on its behalf. The extra benefit is that it will use GPU hardware accelerated rendering to get back results to the viewport even faster. We will also be making use of the embedded graphics library EGL, to perform these renderings without the need for an x-server display. The flag <code>--displays</code> is used to specify the gpu card index that we want to make use of, starting from the number 0. So if we had another card we wished to use instead we would use <code>--displays=1</code>. The potential to leverage multiple gpus is available but requires a different compiled version of paraview with a specific nvidia license so it will not be covered here.</p> <p><code>./pvserver --displays=0</code></p> <p>For completeness here are the commands all together for copying in pasting into a terminal on the HPC</p> <pre><code>wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&amp;version=v5.11&amp;type=binary&amp;os=Linux&amp;downloadFile=ParaView-5.11.1-egl-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview_egl.tar.gz\ncd ParaView-5.11.1*/bin/\ntar xf paraview_egl.tar.gz\n./pvserver --displays=0\n</code></pre> <p>Take note of the address that is written to the screen at this point it will look something like <code>cs://&lt;node_name&gt;.&lt;cluster_name&gt;.arizona.edu:11111</code> and this will be used when we connect via the gui client Steps to launch a client to connect to the server</p> <p>For a first draft of this page we will rely on the documentation provided on the paraview read the docs page to understand the process of connecting from the graphical user interface. https://docs.paraview.org/en/latest/ReferenceManual/parallelDataVisualization.html#configuring-a-server-connection</p> <p>Once the connection has been made, it is possible to determine the extent to which the gpu is utilized by putting the running <code>./pvserver</code> in the background with <code>ctrl-z</code> and then typing <code>bg</code>. At this point you can now type <code>nvidia-smi -l 2</code> to get a log of the activity on the gpu in a text table.</p>"},{"location":"running_jobs/visualization/paraview/speeding_up_filters_and_rendering_with_client_server_configurations/#using-multinode-mpi-and-osmesa-for","title":"Using multinode MPI and osmesa for","text":"<p>This process is very similar to the above, but we will be asking for a slightly different allocation. Use the following command on Elgato for testing having two full nodes of 16 cpu cores to speed up filtering and rendering with paraview. After this tutorial feel free to test out using different machines or node and core counts. The rule being that the total number of cpu cores is specified in the <code>-n</code> flag and the total number of nodes is specified with the <code>-N</code> flag. For more information consult the basic Slurm scheduling pages. Please remember to replace the with your own account name</p> <p><code>interactive -a &lt;your account&gt; -N 2 -n 32 -t 1:00:00</code></p> <p>When the first of the multiple nodes is allocated go and download the latest paraview osmesa.</p> <p><code>wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&amp;version=v5.11&amp;type=binary&amp;os=Linux&amp;downloadFile=ParaView-5.11.1-osmesa-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview_mesa.tar.gz</code></p> <p>The next step is to untar it</p> <p><code>tar xf paraview_mesa.tar.gz</code></p> <p>Then at this point you are ready to navigate into the bin folder</p> <p><code>cd Paraview*/bin</code> Now use the provided mpiexec binary to start up a <code>pvserver</code> backed up by all our allocated cores</p> <p><code>./mpiexec -n 32 ./pvserver</code></p> <p>Once again this is an offscreen rendering system able to run without needing to have displays or anything. A side benefit is since this is an entire osmesa build, no missing opengl dependencies cause problems. Again we need to pay attention to the output of this command so that we can see the server's connection url to use. For instance if the mpi nodes are <code>gpu1&amp;gpu2</code> it will look like <code>cs://gpu2.elgato.hpc.arizona.edu:11111</code></p> <p>At this point we are now all set in order to start up the paraview gui in the remote desktop allocation. Again, please refer back to the Getting Started with Paraview documentation for instructions on this TODO replace this with the link in confluence.</p> <p>Once the gui is open, configure a new connection, and put in just the server address ie <code>gpu2.elgato.hpc.arizona.edu</code> for the host, then supply the port as <code>11111</code></p> <p>After about 10-30 seconds you will be connected and all of the filters and rendering will be accelerated by parallel multinode-mpi.</p> <p>To double check whether your cores are being fully utilised consider using the following. First open a separate terminal ssh into each of the mpi nodes and run htop to see their individual cpu core utilization. As you interact with the paraview gui client the multinode-mpi server will undergo periods of low and high activity observable in the htop display.</p> <p>Please consult the video below for any additional questions or reach out to the vislab-consult@list.arizona.edu</p> <p>**Note that this video is using less mpi resources because waiting on two full elgato nodes wasn't allocating very quickly and made it harder to record the video. Just remember instead of -n 8 you can use -n 32 and then go out for a quick snack.**</p>"},{"location":"running_jobs/visualization/paraview/visualizing_netcdf_files/","title":"Visualizing NetCDF Files","text":"<p>This tutorial will help you visualize your NetCDF Files in ParaView. If you are unfamiliar with ParaView, check out Getting Started With ParaView GUI.</p>"},{"location":"running_jobs/visualization/paraview/visualizing_netcdf_files/#opening-the-data","title":"Opening The Data","text":"<p>Once you have ParaView running. Click the open button and find your data. For this tutorial, I will be using the ParaView example data in Examples \u2192 netCDF. A window should appear showing you different options for opening your file. For most purposes, the NetCDF Reader will work.</p> <p></p> <p>In the properties window, check \u201cReplace Fill Value With Nan\u201d and press apply. You should see something like the image below</p> <p></p> <p>Finally, to visualize the data, select the color drop down from the toolbar (it should say \u201cSolid Color\u201d) and change it to <code>tos</code>.</p> <p></p> <p>Now, the data should appear on the sphere, allowing you to rotate around it and press play at the top for further visualization.</p> <p>If instead of the sphere you would prefer a flat view of the data, you can uncheck Spherical Coordinates from the properties menu, press apply, then reselect the new tos from the color drop down.</p> <p></p> <p>Finally, for a perfectly flat visualization, click the button above the view that says 3D. This will shift it to a 2D view. Then press the button to the right of the 2D button that says Adjust Camera and click the button with the -Z on it. This should give you a perfectly flat view of your map.</p> <p></p> <p></p> <p>And there you have a visualization of simple netCDF data. Below I will show some useful tips for visualizing more specific types of data.</p>"},{"location":"running_jobs/visualization/paraview/visualizing_netcdf_files/#three-dimensional-data","title":"Three Dimensional Data","text":"<p>For data that you would like to view in three dimensions rather than the 2D plane or sphere surface, under the properties window, change Dimensions to a property that has three separate values (often latitude, longitude, and another).</p> <p>Viewing 3D data can be a problem oftentimes on 2D screens. Likely the best ways to view this data is with ParaView's cropping tools (Green boxes on the left of the toolbar) or using the Nvidia IndeX plugin for volumetric rendering.</p>"},{"location":"running_jobs/visualization/paraview/visualizing_netcdf_files/#plotting-data","title":"Plotting Data","text":"<p>For plotting data from NetCDF files, you can reference Graphs and Exporting Data. Be sure to check the Common Issues section at the bottom of the page as NetCDF files will often have holes or null values in the data.</p>"},{"location":"running_jobs/visualization/reconstruction/gaussian_splatting/","title":"Gaussian Splatting","text":""},{"location":"running_jobs/visualization/reconstruction/gaussian_splatting/#gaussian-splatting","title":"Gaussian Splatting","text":""},{"location":"running_jobs/visualization/reconstruction/gaussian_splatting/#overview","title":"Overview","text":"<p>This page shows how to perform the Gaussian Splatting reconstruction technique on the University of Arizona's HPC. This process is different than other forms of 3d reconstruction like photogrammetry in that it doesn't attempt to construct a 3d surface of the real world scene. Instead a collection of 3d ellipsoids are generated who's color properties when viewed from a specific perspective resemble input images provided. For further background consult the original paper here.</p>"},{"location":"running_jobs/visualization/reconstruction/gaussian_splatting/#steps","title":"Steps","text":""},{"location":"running_jobs/visualization/reconstruction/gaussian_splatting/#setup","title":"Setup","text":"<p>Ensure that you are using an allocation with a GPU preferrably a interactive desktop (for the interactive viewer step).</p> <p>Make a project folder and transfer the images you have to a sub folder called <code>images</code> </p> <p><pre><code>project_folder\n|---images \n</code></pre> Navigate into that directory for the following steps</p>"},{"location":"running_jobs/visualization/reconstruction/gaussian_splatting/#structure-from-motion-sfm","title":"Structure from Motion (SfM)","text":"<pre><code>module load contrib\nmodule load chrisreidy/baylyd/colmap\n\nstart_colmap feature_extractor --database_path database.db --image_path images --ImageReader.camera_model SIMPLE_PINHOLE\nstart_colmap exhaustive_matcher --database_path database.db\n\nstart_glomap mapper --image_path images --database_path database.db --output_path sparse\n</code></pre> <p>Troubleshooting</p> <p>If this error appears</p> <pre><code>I0205 11:31:57.138674  8177 misc.cc:44] \n==============================================================================\nFeature matching\n==============================================================================\nI0205 11:31:57.320924  8179 sift.cc:1426] Creating SIFT GPU feature matcher\nI0205 11:31:57.409721  8177 pairing.cc:168] Generating exhaustive image pairs...\nI0205 11:31:57.409775  8177 pairing.cc:201] Matching block [1/8, 1/8]\nterminate called recursively\nterminate called after throwing an instance of 'std::domain_error'\n*** Aborted at 1738780317 (unix time) try \"date -d @1738780317\" if you are using GNU date ***\n  what():  Camera model does not exist\nPC: @                0x0 (unknown)\n*** SIGABRT (@0x632f00001fcb) received by PID 8139 (TID 0x7fcf367fc000) from PID 8139; stack trace: ***\n    @     0x7fcf92f74046 (unknown)\n    @     0x7fcf6f602520 (unknown)\n    @     0x7fcf6f6569fc pthread_kill\n    @     0x7fcf6f602476 raise\n    @     0x7fcf6f5e87f3 abort\n    @     0x7fcf6f89942a __gnu_cxx::__verbose_terminate_handler()\n    @     0x7fcf6f89720c (unknown)\n    @     0x7fcf6f897277 std::terminate()\n    @     0x7fcf6f8974d8 __cxa_throw\n    @     0x55b4f490586c colmap::CameraModelCamFromImg()\n    @     0x55b4f4eb1dae colmap::EstimateCalibratedTwoViewGeometry()\n    @     0x55b4f4eb2ca5 colmap::EstimateTwoViewGeometry()\n    @     0x55b4f487559b colmap::(anonymous namespace)::VerifierWorker::Run()\n    @     0x55b4f49220d6 colmap::Thread::RunFunc()\n    @     0x7fcf6f8c5253 (unknown)\n    @     0x7fcf6f654ac3 (unknown)\n    @     0x7fcf6f6e5a04 clone\n</code></pre> <p>Just attempt the same command again. </p>"},{"location":"running_jobs/visualization/reconstruction/gaussian_splatting/#resizing-images","title":"Resizing images","text":"<pre><code>module load contrib\nmodule load chrisreidy/baylyd/gaussian_splat\n\nresize_images --skip_matching --resize --skip_undistort -s $PWD\n</code></pre>"},{"location":"running_jobs/visualization/reconstruction/gaussian_splatting/#starting-the-interactive-viewer","title":"Starting the interactive Viewer","text":"<pre><code>module load contrib\nmodule load chrisreidy/baylyd/gaussian_splat\nrun_gaussian_viewer\n</code></pre>"},{"location":"running_jobs/visualization/reconstruction/gaussian_splatting/#running-gaussian-splat","title":"Running Gaussian Splat","text":"<pre><code>module load contrib\nmodule load chrisreidy/baylyd/gaussian_splat\n\n# -s pass the present working directory as source to the gaussian splat executable\n# -m pass the present working directory as output destination for trained model\nrun_gaussian_splat -s $(pwd -P) -m $(pwd -P)\n</code></pre>"},{"location":"running_jobs/visualization/visit/","title":"VisIt","text":"<ul> <li>Goal</li> <li>Using OOD</li> <li>Client/Server Configuration<ul> <li>Pre-requisite Installations</li> <li>Steps on the HPC<ul> <li>Setting up ssh keys for passwordless authentication to the HPC.</li> <li>Installing the VisIt server</li> <li>Copying the launcher script into server folder</li> </ul> </li> <li>Steps for Linux<ul> <li>Connecting to the HPC VPN.</li> <li>Installing visit<ul> <li>local machine</li> <li>Installing the HPC host profiles</li> <li>running visit</li> <li>Filling in host profile</li> </ul> </li> </ul> </li> <li>Support related to this document</li> </ul> </li> </ul>"},{"location":"running_jobs/visualization/visit/#goal","title":"Goal","text":"<p>By the end of this document you will know how to start up VisIt client server work sessions with Open On Demand or between your local machine and the compute nodes of the HPC in client server configuration.</p>"},{"location":"running_jobs/visualization/visit/#using-ood","title":"Using OOD","text":"<p>This method makes use of a remote desktop that the user can start up on the HPC. This benefits from putting the user in a familiar graphical environment so things like starting visit are the same here as on their local workstation.\u00a0 Open a browser and go to ood.hpc.arizona.edu</p> <p></p> <p>When prompted fill out the authentication</p> <p></p> <p>Now select <code>Interactive Desktop</code>\u00a0 from the <code>Interactive Apps</code> </p> <p></p> <p>Here you can see the values that were set to run a 1 hour visit session on Ocelote with a gpu. It should be noted that this workflow can also be performed with cpu only allocations too. You will need to fill out your PI where the highlighter is over <code>visteam</code> .</p> <p></p> <p>Once you have hit\u00a0<code>Launch</code>\u00a0 and your allocation is granted you will see this on the page. Now you will click on the <code>Launch Interactive Desktop</code> button.</p> <p></p> <p>A new tab will be created that shows a remote desktop in the window. Here you will right click on the background and select\u00a0<code>Open In Terminal</code> .</p> <p></p> <p>Then you will need to change directory to wherever Visit 3.2.1 was installed. If you haven't done this then follow this script to install it in the directory you are in</p> <p>Download the redhat EL7 w/ Mesa option by running these commands replacing the\u00a0<code>&lt;PIname&gt;</code>\u00a0 with your PI's name</p> <pre><code>mkdir visit_setup\ncd visit_setup\nwget \"https://github.com/visit-dav/visit/releases/download/v3.2.1/visit3_2_1.linux-x86_64-rhel7-wmesa.tar.gz\"\ntar xf visit3_2_1.linux-x86_64-rhel7-wmesa.tar.gz\ncd visit3_2_1.linux-x86_64\n</code></pre> <p>Once you are located in the <code>visit3_2_1.linux-x86_64</code>\u00a0 folder navigate to the <code>bin</code>\u00a0 folder and run <code>./visit -debug 5</code> </p> <p></p> <p>You will now see Visit starting in the remote desktop. Use the Open button to bring up a data set you want to visualize and export a movie from. Here the aneurysm dataset is selected, if you'd like to use it you may download it here https://visit-dav.github.io/largedata/datarchives/aneurysm.</p> <p></p> <p>Then choose to add a simple plot.</p> <p></p> <p>For this data one of the interesting parts is the velocity field and how it changes over time.</p> <p></p> <p>Once you have selected the velocity field make sure to draw it to the adjacent Visit window.</p> <p></p> <p>Now that we have the first frame drawn in the viewing window, let's render all the frames of the simulation out as a movie. Select <code>File</code>\u00a0 to show the drop down menu of options.</p> <p></p> <p>From the options select <code>Save Movie</code> which will open a wizard window to step you through the export configuration.</p> <p></p> <p>Select next with the default option to create a <code>New simple movie</code> </p> <p></p> <p>In the next window select the format drop down and select one of the options. A benefit to exporting individual frames as still images is it provides greater flexibility over quality options for the conversion to video which is outlined later. Here <code>PNG</code>\u00a0 will be used</p> <p></p> <p>Click the <code>-&gt;</code> arrow to queue this format to the export options. It is possible to make several outputs from a single export pass by clicking the <code>-&gt;</code> arrow multiple times for a range of formats/options.</p> <p></p> <p>Clicking next shows a window with parameters about the video to be made. These options are default set to create a frame for each time step in the video, but if a particular range is of interest you may set the <code>First Frame</code>\u00a0 and <code>Last Frame</code>\u00a0 to customize.</p> <p></p> <p>Finally we will specify where the outputs are supposed to go. Use the drop down button at the far right to open a file browser window to aid you in finding the optimal destination if it isn't the default one shown. You may change the <code>Base Name</code>\u00a0 to specify an alternate prefix for each rendered frame which is followed by a frame number padded with 4 0's.</p> <p></p> <p>Since we have already configured a visit instance with out data and plot we will <code>use the currently allocated processors</code> . More advanced workflows may require the other options but they are not documented here as of yet.</p> <p></p> <p>Once you select finish a new window opens which shows the terminal output of the process, as well as a progress bar window that helps you see how the export is unfolding.</p> <p></p> <p>Once this finishes you may return to the terminal, and navigate to the folder where the individual frames were output. Use the <code>ls</code>\u00a0 command to make sure you are in the correct location.</p> <p></p> <p>You must also load in a few modules in order to process the images into a video from the command line.</p> <pre><code>module load gnu8\nmodule load ffmpeg\n</code></pre> <p></p> <p>This is the final step. You will run ffmpeg to concatenate all the images together into a video whose format you may specify. To let ffmpeg know we are using all the images that match a pattern that starts with the word movie, then has 4 digits, then file type mp4 we use this string <code>movie%04d.png</code> . The <code>-r</code> specifies the rate in fps. The <code>-f</code>\u00a0 notifies ffmpeg that we are using the <code>image2</code>\u00a0 filter to convert an image-&gt;something else.</p> <pre><code>ffmpeg -f image2 -i movie%04d.png -r 10 output.mp4\n</code></pre> <p></p> <p>All done!</p>"},{"location":"running_jobs/visualization/visit/#clientserver-configuration","title":"Client/Server Configuration","text":"<p>This setup differs from OOD in that there is no remote rendering of the visit window contents, and allows for rapid response from interaction with the menus because the client runs natively on the local machine. The trade off is that certain capabilities are c`c</p>"},{"location":"running_jobs/visualization/visit/#pre-requisite-installations","title":"Pre-requisite Installations","text":"<p>VPN system like Cisco Anyconnect</p>"},{"location":"running_jobs/visualization/visit/#steps-on-the-hpc","title":"Steps on the HPC","text":""},{"location":"running_jobs/visualization/visit/#setting-up-ssh-keys-for-passwordless-authentication-to-the-hpc","title":"Setting up ssh keys for passwordless authentication to the HPC.","text":"<p>The documentation for this step exists on confluence at this link System Access#SSHKeys but you must complete these steps for <code>shell.hpc.arizona.edu</code>\u00a0 not the bastion <code>hpc.arizona.edu</code>\u00a0 host. Note that performing a <code>ssh &lt;netid&gt;@shell.hpc.arizona.edu</code>\u00a0 and replacing the\u00a0<code>&lt;netid&gt;</code>\u00a0 with your netid is only possible when we are on the hpc vpn.</p>"},{"location":"running_jobs/visualization/visit/#installing-the-visit-server","title":"Installing the VisIt server","text":"<p>Download the redhat EL7 w/ Mesa option by running these commands replacing the\u00a0<code>&lt;PIname&gt;</code>\u00a0 with your PI's name</p> <pre><code>cd /groups/&lt;PIname&gt;\nmkdir visit_setup\ncd visit_setup\nwget \"https://github.com/visit-dav/visit/releases/download/v3.1.4/visit3_1_4.linux-x86_64-rhel7-wmesa.tar.gz\"\ntar xf visit3_1_4.linux-x86_64-rhel7-wmesa.tar.gz\ncd visit3_1_4.linux-x86_64\n</code></pre>"},{"location":"running_jobs/visualization/visit/#copying-the-launcher-script-into-server-folder","title":"Copying the launcher script into server folder","text":"<p>The contents of the visit installation are laid out as follows.</p> <p></p> <p>Navigate to the bin folder within 3.1.4\u00a0 and copy in the customlauncher code</p> <pre><code>cd 3.1.4/bin\nwget \"https://gist.githubusercontent.com/DevinBayly/310c8689c6221fd379aad34243441dda/raw/a8f5071bb8b1e96127e1ea01b2e8667940849f1a/customlauncher\" -O customlauncher\n</code></pre> <p>If you'd like to inspect the code a copy is available here as well. Customlauncher</p>"},{"location":"running_jobs/visualization/visit/#steps-for-linux","title":"Steps for Linux","text":""},{"location":"running_jobs/visualization/visit/#connecting-to-the-hpc-vpn","title":"Connecting to the HPC VPN","text":"<p>Connect to the HPC VPN, preferably with Cisco AnyConnect. For detailed information on connecting to the HPC VPN, see VPN - Virtual Private Network.</p> <p>Note that The HPC VPN is needed to connect directly to a compute node. This differs from the standard UArizona VPN or campus network which are not sufficient. The HPC VPN is <code>vpn.hpc.arizona.edu</code>.</p>"},{"location":"running_jobs/visualization/visit/#installing-visit","title":"Installing visit","text":""},{"location":"running_jobs/visualization/visit/#local-machine","title":"local machine","text":"<p>https://visit-dav.github.io/visit-website/</p> <p>https://visit-dav.github.io/visit-website/releases-as-tables/#series-31</p>"},{"location":"running_jobs/visualization/visit/#installing-the-hpc-host-profiles","title":"Installing the HPC host profiles","text":"<p>download the <code>host_uahpc2.xml</code>\u00a0 file and save it to the visit directory in your home folder. Here's what that looks like in my case:</p> <p>host_uahpc2.xml </p> <p>Once downloaded, transfer it to `~/.visit/hosts`</p> <p></p>"},{"location":"running_jobs/visualization/visit/#running-visit","title":"running visit","text":"<p>Navigate to the visit3.1.4 folder who's contents should appear like this</p> <p></p> <p>Enter the `bin` folder and start visit in debug mode</p> <pre><code>cd bin\n\n./visit -debug 5\n</code></pre> <p>This mode will be helpful for tracking down any errors that come up when configuring the client server session.</p>"},{"location":"running_jobs/visualization/visit/#filling-in-host-profile","title":"Filling in host profile","text":"<p>There are the values that need to be changed in the host profile to support your user on the HPC. Open the host profiles from the options tab and then select UAHPC2</p> <p></p> <p>On the\u00a0<code>Host Settings</code>\u00a0 page we will update the following entries that are highlighted. The first one is the path to the visit installation on the HPC. Here you must get the absolute path to the VisIt installation on the HPC which should be <code>/groups/&lt;PIname&gt;/visit_setup/visit3_1_4.linux-x86_64</code> replacing\u00a0<code>&lt;PIname&gt;</code>\u00a0 with your PI's name. If you installed VisIt somewhere else you will use a different absolute path to that location.</p> <p>Second you must replace\u00a0<code>baylyd</code>\u00a0 with your own netid</p> <p></p> <p>Then we will change the options in the <code>Launch Profiles</code> by selecting that tab in the <code>Host Profiles</code>\u00a0 window.</p> <p></p> <p>Then select the\u00a0<code>parallel</code>\u00a0 settings tab ensuring to replace <code>visteam</code>\u00a0 with your own PI</p> <p></p> <p>Once that's complete you may attempt to open a session</p> <p></p> <p>Select the <code>UAHPC2</code>\u00a0 host</p> <p></p> <p>Then navigate to a data set you wish to visualize in the client server session</p> <p></p> <p>In my case I'll be using the sample aneurysm data which can be downloaded here https://visit-dav.github.io/largedata/datarchives/aneurysm</p> <p></p> <p>You will then have the option to select a compute launcher configuration. I'm going with <code>elgato</code>. Again check to make sure that the <code>Bank</code> field has your PI's netid in it.</p> <p></p> <p>After selecting <code>OK</code> you will see a progress bar:</p> <p></p> <p>This is displayed while the slurm batch allocation that was automatically generated for you waits in a queue. When it is approved and launched the progress window goes away. You can double check that the VisIt session is running successfully on the HPC by logging into\u00a0<code>hpc.arizona.edu</code>\u00a0 and running\u00a0<code>squeue -u &lt;netid&gt;</code>. You will see an entry like this who's name is <code>visit.&lt;netid&gt;</code> </p> <p></p> <p>At this point you can request for VisIt to perform operations and graph different aspects of your data.</p> <p></p> <p>If your data is timeseries you may also hit the play button and it will step through your data visualizing each step.</p>"},{"location":"running_jobs/visualization/visit/#support-related-to-this-document","title":"Support related to this document","text":"<p>If you are interested in this workflow but need support you can send an email to vislab-consult@list.arizona.edu for technical support a Data &amp; Visualization consultant.</p> <p>{\"serverDuration\": 18, \"requestCorrelationId\": \"79d8fd98c6fa410395673f3939c1f8ca\"}</p>"},{"location":"software/common_datasets/","title":"Common Datasets","text":"<p>We host several large community datasets.  It is beneficial to you and us.  For you, it saves all that time downloading and filling up your storage allocation.  And for us it reduces the occurrence of the same data in many places. We do not currently update them on any particular cadence. You can request updates if you feel those would be useful to the community.</p> <p>These datasets and databases are available on the compute nodes under <code>/contrib/datasets</code> in read-only mode. </p>"},{"location":"software/common_datasets/#alphafold","title":"AlphaFold","text":"<p>AlphaFold is an AI system developed by the Google DeepMind project to predict the 3D structure of a protein from its amino acid sequence. AlphaFold needs multiple datasets to for inference, the combined size of which is around several TBs. We host the datasets for both AlphaFold 2 &amp; 3. You can find these datasets under <code>/contrib/datasets/alphafold/2.3.0</code> and <code>/contrib/datasets/alphafold/3.0.0</code>. For more information on using these datasets with the provided AlphaFold modules, see AlphaFold. If you want to use these datasets with other software, you can access these paths from an interactive session or a batch script.</p>"},{"location":"software/common_datasets/#llama-2","title":"Llama 2","text":"<p>Meta has open-sourced Llama2, a group large language models, and it is available for free for research uses, under a community license. These models range from tens of GBs to hundreds of GBs in size. We have downloaded them and made them available for you at <code>/contrib/datasets/llama2</code> and <code>/contrib/datasets/llama2-hf</code>. The latter are compatible with the HuggingFace ecosystem.</p> <p>You do not need to copy these models to another location to use them. You can directly access those locations from an interactive session or an batch script. However, if you copy them to another location, then also copy the license files, present in the directories mentioned above. The following Python code shows how you can load the HuggingFace compatible models.</p> <pre><code>from transformers import LlamaForCausalLM, LlamaTokenizer\n\nllama_path = \"/contrib/datasets/llama2-hf/meta-llama_Llama-2-7b-chat-hf\"\ntokenizer = LlamaTokenizer.from_pretrained(llama_path)\nmodel = LlamaForCausalLM.from_pretrained(llama_path)\n</code></pre> <p>You can run the above code from either a Jupyter notebook (see Open OnDemand), or a batch script. If you do not provide enough memory, then the Jupyter kernel will die, or the Slurm job will terminate with an out of memory (OOM) error message. We recommend that you request at least 60 GB of memory for the models with 7 billion parameters, and progressively more for the models with 13 billion and 70 billion parameters. For more information on using Llama2, we recommend that you check Llama2's Github repository and HuggingFace documentation.</p>"},{"location":"software/common_datasets/#ncbi-blast","title":"NCBI BLAST","text":"<p>The Basic Local Alignment Search Tool (BLAST), developed by NCBI, is a program for comparing primary biological sequence information, such as amino-acid or nucleotide sequences. NCBI has several databases that you can use with BLAST. We have the <code>nr</code>, <code>nt</code>, and <code>taxdb</code> databases available locally under <code>/contrib/datasets/blast</code>. You can use these datasets with the <code>blast</code> module.</p>"},{"location":"software/containers/building_containers/","title":"Building Containers","text":""},{"location":"software/containers/building_containers/#building-containers","title":"Building Containers","text":"<p>Warning</p> <p>The Apptainer cache can fill up your home quickly. To set a different location, see our Cache Directory documentation. </p> <p>Tip</p> <p>For detailed information on Apptainer recipes, see Apptainer's official documentation.</p> <p>Apptainer Build is a tool that allows you to create containers. With Apptainer Build, you can package your application and its dependencies into a single unit, making it easier to deploy and share across different computing environments. Two useful options are to build your container by bootstrapping off a container hosted locally on HPC or bootstrapping off an existing container hosted on Dockerhub. We'll cover both cases below.</p>"},{"location":"software/containers/building_containers/#bootstrapping-off-a-local-image","title":"Bootstrapping off a Local Image","text":"<p>One common case users run into is using a Python container hosted on HPC (say, one of our Nvidia machine learning images) but finding they need additional packages installed in the image. To do this, it's possible to bootstrap off the local image and pip-install a new package in a section called <code>%post</code> which is executed during build time. </p> <p>For example, say we want to use the HPC container <code>nvidia-tensorflow-2.6.0.sif</code> located in <code>/contrib/singularity/nvidia/</code> but we need it to have the package astropy installed which is currently missing. We can create a recipe file that takes this image, bootstraps off it, and pip-installs astropy. Our recipe file would look like the following:</p> <pre><code>Bootstrap: localimage\nFrom: /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif\n\n%post\n  pip install astropy\n</code></pre> <p>We'll call this recipe file something descriptive, e.g. tf2.6-astropy.recipe. Then, to build, all we need to do is use the syntax <code>apptainer build &lt;output_image&gt; &lt;recipe_file&gt;</code>. In this case:</p> <pre><code>[netid@cpu43 build_example]$ apptainer build tf2.6-astropy.sif tf2.6-astropy.recipe \nINFO:    User not listed in /etc/subuid, trying root-mapped namespace\nINFO:    The %post section will be run under fakeroot\nINFO:    Starting build...\nINFO:    Verifying bootstrap image /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif\n. . .\nINFO:    Creating SIF file...\nINFO:    Build complete: tf2.6-astropy.sif\n[netid@cpu43 build_example]$ \n</code></pre>"},{"location":"software/containers/building_containers/#bootstrapping-off-a-docker-hub-image","title":"Bootstrapping off a Docker Hub Image","text":"<p>Tip</p> <p>Not sure how to install your own software? Check our our section on User Installations.</p> <p>Bootstrapping off Ubuntu images is a great way to create a very customizable container where you can install your own software. Instead of pulling an Ubuntu image (see: Pulling Containers for a tip on how to find an Ubuntu image), we can bootstrap directly off the image in our recipe file. </p> <p>Let's say as an example, we want to install Python 3.11 with a custom library. We can create a recipe file called <code>python3.11_astropy.recipe</code> with the following contents:</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:22.04\n\n%post \n  apt update -y\n  apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev -y \n  wget https://www.python.org/ftp/python/3.11.3/Python-3.11.3.tgz\n  tar -xf Python-3.11.3.tgz\n  cd Python-3.11.3\n  ./configure --enable-optimizations\n  make \n  make altinstall\n  python3.11 -m pip install astropy\n</code></pre> <p>Then to execute the build process and create our image, we can use: <pre><code>[netid@cpu38 pull_example]$ apptainer build python3.11_astropy.sif python3.11_astropy.recipe \nINFO:    User not listed in /etc/subuid, trying root-mapped namespace\nINFO:    The %post section will be run under fakeroot\nINFO:    Starting build...\n. . .\nINFO:    Creating SIF file...\nINFO:    Build complete: python3.11_astropy.sif\n[netid@cpu38 pull_example]$ \n</code></pre></p>"},{"location":"software/containers/containers_on_hpc/","title":"Containers on HPC","text":""},{"location":"software/containers/containers_on_hpc/#containers-on-hpc","title":"Containers on HPC","text":"<p>Tip</p> <p>Apptainer is installed on the operating systems of all HPC compute nodes, so can be easily accessed either from an interactive session or batch script without worrying about software modules. </p>"},{"location":"software/containers/containers_on_hpc/#available-containers","title":"Available Containers","text":"<p>We support the use of HPC and ML/DL containers available on NVIDIA GPU Cloud (NGC). Many of the popular HPC applications including NAMD, LAMMPS and GROMACS containers are optimized for performance and available to run in Apptainer on Ocelote or Puma. The containers and respective README files can be found in <code>/contrib/singularity/nvidia</code>. They are only available from compute nodes, so start an interactive session if you want to view them.</p>  Container  Description <code>nvidia-caffe.20.01-py3.simg</code> Caffe is a deep learning framework made with expression, speed, and modularity in mind. It was originally developed by the Berkeley Vision and Learning Center (BVLC). <code>nvidia-gromacs.2018.2.simg</code> GROMACS is designed to simulate biochemical molecules like proteins, lipids, and nucleic acids <code>nvidia-julia.1.2.0.simg</code> The Julia programming language is a flexible dynamic language, appropriate for scientific and numerical computing, with performance comparable to traditional statically-typed languages. <code>nvidia-lammps.24Oct2018.sif</code> The main use case of the Large-scale Atomic / Molecular Massively Parallel Simulator is atom scale particle modeling or, more generically, as a parallel particle simulator at the atomic, meson, or continuum scale <code>nvidia-namd_2.13-multinode.sif</code> NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR. <code>nvidia-pytorch.20.01-py3.simg</code> PyTorch is a Python package that provides two high-level features:- Tensor computation (like numpy) with strong GPU acceleration- Deep Neural Networks built on a tape-based autograd system <code>nvidia-rapidsai.sif</code> RAPIDS provides unmatched speed with familiar APIs that match the most popular PyData libraries. Built on state-of-the-art foundations like NVIDIA CUDA and Apache Arrow. <code>nvidia-relion_2.1.b1.simg</code> RELION (REgularized LIkelihood OptimizatioN) implements an empirical Bayesian approach for analysis of electron cryo-microscopy (Cryo-EM). Specifically, RELION provides refinement methods of singular or multiple 3D reconstructions as well as 2D class averages. <code>nvidia-tensorflow_2.0.0-py3.sif</code> TensorFlow is an open source software library for numerical computation using data flow graphs. TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research. <code>nvidia-theano.18.08.simg</code> Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently."},{"location":"software/containers/containers_on_hpc/#sharing-your-containers","title":"Sharing Your Containers","text":"<p>If you have containers that you would like to share with your research group or broader HPC community, you may do so by creating a directory in the space <code>/contrib/singularity/shared</code>.</p>"},{"location":"software/containers/containers_on_hpc/#cache-directory","title":"Cache Directory","text":"<p>To speed up image downloads for faster, less redundant builds and pulls, Apptainer sets a cache directory in your home under <code>~/.apptainer</code>. This directory stores images, metadata, and docker layers that can wind up being reasonably large. If you're struggling with space usage and your home's 50 GB quota, one option is to set a new Apptainer cache directory. You can do this by setting the environment variable <code>APPTAINER_CACHEDIR</code> to a new location. From Apptainer's documentation:</p> <p>If you change the value of <code>APPTAINER_CACHEDIR</code> be sure to choose a location that is:</p> <ol> <li>Unique to you. Permissions are set on the cache so that private images cached for one user are not exposed to another. This means that <code>APPTAINER_CACHEDIR</code> cannot be shared.</li> <li>Located on a filesystem with sufficient space for the number and size of container images anticipated.</li> <li>Located on a filesystem that supports atomic rename, if possible.</li> </ol> <p>For example, if you wanted to set your cache directory to your PI's <code>/groups</code> directory under a directory you own, you could use:</p> <pre><code>export APPTAINER_CACHEDIR=/groups/pi/your_netid/.apptainer\n</code></pre> <p>To make the change permanent, add this line to the hidden file in your home directory <code>~/.bashrc</code>. If you are unfamiliar with this file and would like more information, see our Linux cheat sheet guide.</p>"},{"location":"software/containers/pulling_containers/","title":"Pulling Containers","text":""},{"location":"software/containers/pulling_containers/#pulling-containers","title":"Pulling Containers","text":"<p>Warning</p> <p>The Apptainer cache can fill up your home quickly. To set a different location, see our Cache Directory documentation. </p>"},{"location":"software/containers/pulling_containers/#pulling-docker-containers","title":"Pulling Docker Containers","text":"<p>Apptainer has the ability to convert available docker images into sif format allowing them to be run on HPC. If you find an image on Docker Hub that you would like to use, you can pull it using the <code>apptainer pull &lt;local_image_name&gt;.sif docker://docker_image</code>. </p> <p>As an example, we could pull an Ubuntu image from Docker Hub with OS 22.04 by searching for Ubuntu, opening the Tags tab, and copying their <code>docker pull</code> command:</p> <p></p> <p>Then, on HPC, we can run:</p> <pre><code>[netid@cpu37 pull_example]$ apptainer pull ubuntu_22.04.sif docker://ubuntu:22.04\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 01007420e9b0 done  \nCopying config 3db8720ecb done  \nWriting manifest to image destination\nStoring signatures\n2024/02/20 09:02:02  info unpack layer: sha256:01007420e9b005dc14a8c8b0f996a2ad8e0d4af6c3d01e62f123be14fe48eec7\nINFO:    Creating SIF file...\n[netid@cpu37 pull_example]$ ls\nubuntu_22.04.sif\n</code></pre>"},{"location":"software/containers/pulling_containers/#pulling-nvidia-images","title":"Pulling Nvidia Images","text":"<p>The NVIDIA GPU Cloud (NGC) provides GPU-accelerated HPC and deep learning containers for scientific computing.  NVIDIA tests HPC container compatibility with the Apptainer runtime through a rigorous QA process. Application-specific information may vary so it is recommended that you follow the container-specific documentation before running with Apptainer. If the container documentation does not include Apptainer information, then the container has not yet been tested under Apptainer. Apptainer can be used to pull, execute, and bootstrap off Docker images. </p> <p>To pull images, you'll need to register with Nvidia. Once you have an account, you can view their images from their catalogue. Click on the name of the software you're interested in to view available versions</p> <p></p> <p>If you click on the Tags tab at the top of the screen, you'll find the different versions that are available for download. For example, if we click on TensorFlow, we can get the pull statement for the latest tag of TensorFlow 2 by clicking the ellipses and selecting Pull Tag.</p> <p></p> <p>This will copy a <code>docker pull</code> statement to your clipboard, in this case:</p> <pre><code>$ docker pull nvcr.io/nvidia/tensorflow:22.02-tf2-py3\n</code></pre> <p>To pull and convert this NGC image to a local Apptainer image file, we'll convert this to:</p> <pre><code>$ apptainer build ~/tensorflow2-22.02-py3.sif docker://nvcr.io/nvidia/tensorflow:22.02-tf2-py3\n</code></pre> <p>The general format for any pull you want to do is:</p> <pre><code>$ apptainer build &lt;local_image_name&gt; docker://nvcr.io/&lt;registry&gt;/&lt;app:tag&gt;\n</code></pre> <p>This Apptainer build command will download the <code>app:tag</code> NGC Docker image, convert it to Apptainer format, and save it to the local filename <code>&lt;local_image_name&gt;</code>. </p>"},{"location":"software/containers/using_containers/","title":"Using Containers","text":""},{"location":"software/containers/using_containers/#using-containers","title":"Using Containers","text":"<p>If you've gone through the previous sections, you have an idea of how to pull and create containers, but how do you actually use one? Apptainer can be used to run your analyses and applications in various ways: running a prepackaged workflow embedded in the image with <code>apptainer run</code>, executing commands within the container's environment using <code>apptainer exec</code>, or starting an interactive instance to work directly within the container using <code>apptainer shell</code>.</p>"},{"location":"software/containers/using_containers/#running-prepackaged-apptainer-workflows","title":"Running Prepackaged Apptainer Workflows","text":"<p>Note: Not every container may include a predefined workflow</p> <p><code>apptainer run</code> is used without any arguments and executes a predefined workflow embedded in the image. The general syntax is: </p> <pre><code>apptainer run &lt;img&gt;.sif\n</code></pre> <p>For example:</p> <pre><code>[netid@cpu38 run_example]$ apptainer run lolcow.sif \n _______________________________________\n/ You get along very well with everyone \\\n\\ except animals and people.            /\n ---------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"software/containers/using_containers/#executing-commands-within-apptainer-images","title":"Executing Commands within Apptainer Images","text":"<p><code>apptainer exec</code> allows you to execute commands within an Apptainer image without entering it interactively. It simplifies running tasks by providing a command-line interface to interact with containerized applications, ensuring consistency and reproducibility in your workflow. The general syntax is</p> <pre><code>apptainer exec app.sif &lt;commands&gt; \n</code></pre> <p>For example:</p> <pre><code>apptainer exec /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 script.py\n</code></pre>"},{"location":"software/containers/using_containers/#using-apptainer-applications-interactively","title":"Using Apptainer Applications Interactively","text":"<p>Apptainer shell starts an interactive session within the container's environment. This is optimal for testing, debugging, or using any sort of graphical interface installed in your image. The general syntax is</p> <pre><code>apptainer shell app.sif\n</code></pre> <p>For example:</p> <pre><code>[netid@cpu38 run_example]$ apptainer shell /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif\nSingularity&gt; python3\nPython 3.8.10 (default, Sep 28 2021, 16:10:42) \n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.6.2'\n</code></pre>"},{"location":"software/containers/using_containers/#apptainer-and-gpus","title":"Apptainer and GPUs","text":"<p>If you're using Apptainer to execute workflows that require a GPU, you'll need to add the additional flag <code>--nv</code> to your Apptainer command. For example:</p> <pre><code>apptainer exec --nv &lt;container&gt; &lt;commands&gt;\n</code></pre>"},{"location":"software/containers/what_are_containers/","title":"What are Containers","text":""},{"location":"software/containers/what_are_containers/#what-are-containers","title":"What are Containers?","text":"<p>Tip</p> <p>Want to learn more about containers? Check out our Intro to Containers workshop. </p> <p></p> <p>A container is a packaged unit of software that contains code and all its dependencies including, but not limited to: system tools, libraries, settings, and data. This makes applications and pipelines portable and reproducible, allowing for a consistent environment that can run on multiple platforms.</p> <p>Shipping containers have frequently been used as an analogy because the container is standard, does not care what is put inside, and will be carried on any ship; or in the case of computing containers, it can run on many different systems.</p> <p>Docker is widely used by researchers, however, Docker images require root privileges which means they cannot be run in an HPC environment.</p> <p>Apptainer (formerly Singularity) addresses this by completely containing the authority so that all privileges needed at runtime stay inside the container. This makes it ideal for the shared environment of a supercomputer. Even better, a Docker image can be encapsulated inside an Apptainer image. Some ideal use cases that can be supported by Apptainer on HPC include:</p> <ul> <li>You already use Docker and want to run your jobs on HPC.</li> <li>You want to preserve your environment so a system change will not affect your work.</li> <li>You need newer or different libraries than are offered on the system.</li> <li>Someone else developed a workflow using a different version of Linux.</li> <li>You prefer to use a Linux distribution other than CentOS (e.g. Ubuntu).</li> <li>You want a container with a database server like MariaDB.</li> </ul> <p>The documentation in this section provides instructions on how to either take a Docker image and run it from Apptainer, or create and run an image using Apptainer only.</p> <p></p>"},{"location":"software/modules/","title":"Software Modules","text":"<p>Availability</p> <p>Software modules are not available on the login nodes. You will need to be on a compute node to access them.</p> <p>Cluster Differences</p> <p>Ocelote and El Gato both run CentOS 7 as their operating system and share the same system libraries and modules. These two clusters can generally be used interchangably. Puma uses Rocky Linux 9 as its operating system and has different system libraries and modules than Ocelote and El Gato. This means workflows may not be transferrable between Puma and the other two clusters. Take note of which cluster you're using to prevent software failures.</p> <p>Software packages are available as modules and are accessible from the compute nodes of any of our three clusters. A software module is a tool used to manage software environments and dependencies. It allows you to easily load and unload different software packages, libraries, and compilers needed for computational tasks without conflicts. This ensures access to many specific tools, and even different versions of the same tools, without affecting the overall system configuration.</p>"},{"location":"software/modules/#module-commands","title":"Module Commands","text":"<p>Default Versions</p> <p>If multiple versions of software are available on the system, the newest is made the default. This means loading a module without specifying the version will select the most recent. We strongly recommend including version information in your module statements. This ensures that you maintain a consistent environment for your analyses in the event of a software upgrade.</p> Command Description <pre><code>module avail</code></pre> Display all the software and versions installed on the system <pre><code>module avail &lt;search_term&gt;</code></pre> Display all installed modules matching <code>&lt;search_term&gt;</code> <pre><code>module list</code></pre> Display the software you have loaded in your environment <pre><code>module whatis &lt;module_name&gt;</code></pre> Displays some descriptive information about a specific module <pre><code>module show &lt;module_name&gt;</code></pre> Displays system variables that are set/modified when loading module <code>&lt;module_name&gt;</code> <pre><code>module load &lt;module_name&gt;</code></pre> Load a software module in your environment <pre><code>module unload &lt;module_name&gt;</code></pre> Unload a specific software package from your environment <pre><code>module swap &lt;module_name&gt;/&lt;version1&gt; &lt;module_name&gt;/&lt;version2&gt;</code></pre> Switch versions of a software module <pre><code>module purge</code></pre> Unload all the software modules from your environment <pre><code>module help</code></pre> Display a help menu for the module command"},{"location":"software/modules/#examples","title":"Examples","text":""},{"location":"software/modules/#loading-modules","title":"Loading Modules","text":"<pre><code>[netid@cpu39 ~]$ module avail python\n\n------------------- /opt/ohpc/pub/modulefiles --------------------\n   python/3.6/3.6.5     python/3.9/3.9.10\n   python/3.8/3.8.2     python/3.11/3.11.4 (D)\n   python/3.8/3.8.12\n[netid@cpu39 ~]$ module load python/3.9\n[netid@cpu39 ~]$ python3 --version\nPython 3.9.10\n[netid@cpu39 ~]$ module swap python/3.9 python/3.11\n\nThe following have been reloaded with a version change:\n  1) python/3.9/3.9.10 =&gt; python/3.11/3.11.4\n\n[netid@cpu39 ~]$ python3 --version\nPython 3.11.4\n</code></pre>"},{"location":"software/modules/#finding-executables-and-libraries","title":"Finding Executables and Libraries","text":"<p>If you're looking for the specific paths added to your environment when loading a module, you can use the command <code>module show</code>. For example:</p> <p><pre><code>[netid@cpu38 ~]$ module show gromacs\n---------------------------------------------------------\n   /opt/ohpc/pub/moduledeps/gnu8-openmpi3/gromacs/2021.5:\n---------------------------------------------------------\nwhatis(\"Name: gromacs \")\nwhatis(\"Version: 2021.5 \")\nwhatis(\"Molecular dynamics for biophysical chemistry \")\nsetenv(\"GROMACS_BASE\",\"/opt/ohpc/pub/apps/gromacs/2021.5\")\nprepend_path(\"PATH\",\"/opt/ohpc/pub/apps/gromacs/2021.5/bin\")\nprepend_path{\"CPPFLAGS\",\"-I/opt/ohpc/pub/apps/gromacs/2021.5/include\",delim=\" \"}\nprepend_path(\"MANPATH\",\"/opt/ohpc/pub/apps/gromacs/2021.5/share/man\")\nprepend_path(\"PKG_CONFIG_PATH\",\"/opt/ohpc/pub/apps/gromacs/2021.5/lib64/pkgconfig\")\nprepend_path{\"LDFLAGS\",\"-L/opt/ohpc/pub/apps/gromacs/2021.5/lib64\",delim=\" \"}\nprepend_path(\"LD_LIBRARY_PATH\",\"/opt/ohpc/pub/apps/gromacs/2021.5/lib64\")\nunload(\"gnu8\")\nload(\"gnu8\")\nhelp([[ Adds gromacs to your environment\n]])\n</code></pre> These paths will allow you to determine the locations of the executables, libraries, header files, etc. available to you after loading the software.</p>"},{"location":"software/modules/#compilers","title":"Compilers","text":""},{"location":"software/modules/#puma","title":"Puma","text":"Compiler Version Module Command gcc 12.2.0 <pre><code>module load gnu12/12.2.0</code></pre> gcc 13.2.0 <pre><code>module load gnu13/13.2.0 # Recommended. Loaded by default.</code></pre> intel 2023.2.1 <pre><code>module load intel/2023.2.1</code></pre> intel 2024.0.0 <pre><code>module load intel/2024.0.0 # Default</code></pre> intel 2024.1.2 <pre><code>module load intel/2024.1.2</code></pre>"},{"location":"software/modules/#ocelote-and-el-gato","title":"Ocelote and El Gato","text":"Compiler Version Module Command gcc 5.4.0 <pre><code>module load gnu/5.4.0</code></pre> gcc 7.3.0 <pre><code>module load gnu7/7.3.0</code></pre> gcc 8.3.0 <pre><code>module load gnu8/8.3.0 # Recommended. Loaded by default</code></pre> Intel 2020.1 <pre><code>module load intel/2020.1</code></pre> Intel 2020.4 <pre><code>module load intel/2020.4</code></pre>"},{"location":"software/modules/#software-install-requests","title":"Software Install Requests","text":"<p>If you need to use a software package and it is not installed on the system, we can install it for you, provided it meets the criteria outlined in our software policies. Software can be requested by using our HPC software install request Form. There is no expected time frame for how long it takes to install software, as there are many variables that determine this. If you haven't heard back within a week, it may be appropriate to follow up.</p> <p>For software that doesn't meet the criteria outlined in our policies and doesn't fall into the unsupported software category, it may be possible for you to install it locally. We have instructions documented in our user installations section.</p>"},{"location":"software/overview/","title":"Software Overview","text":""},{"location":"software/overview/#overview","title":"Overview","text":"Module availability <p>Software modules are not available on the login nodes. To access them, you will need to connect to a compute node either via an interactive session or batch job.</p> <p>Our HPC systems support over 100 software applications.  The first thing to know is that the compute nodes on each cluster are where you use and manage software.  </p> <p>The software is available in three different ways:</p> <ul> <li>Libraries in the operating system (like <code>fftw</code> or <code>screen</code>);</li> <li>Personal software that you build or download and place in your own directory space;</li> <li>Modules, which are external packages built and maintained by HPC team for system-wide usage.</li> </ul>"},{"location":"software/overview/#cluster-differences","title":"Cluster Differences","text":"<p>It's important to note which cluster you're using for your analyses as all three do not share the same operating system, software, and libraries.</p> <p>Ocelote and ElGato both run on CentOS 7 and share the same filesystem. This means identical software modules, compilers, and system libraries are available on both. </p> <p>Puma runs on Rocky Linux 9 with different software modules, compilers, and system libraries than the other two systems.</p> <p>When compiling your own software or installing packages (e.g., with Python, R, Julia, etc), take note of which cluster you're using as migrating between Puma and the other clusters may result in failures. </p>"},{"location":"software/overview/#policies","title":"Policies","text":""},{"location":"software/overview/#academicfree-software","title":"Academic/Free Software","text":"<p>There is a plethora of software generally available for scientific and research usage.  We will install that software as a module if it meets the following requirements:</p> Requirements Compatible with our module environment Some software is not written with clusters in mind and tries to install into system directories, or needs a custom environment on every compute node. Generally useful Some software has to be configured to the specific compute environment of the user. You are encouraged to use our \"contrib\" environment to install your own. Public license We do not install software if that would be a violation of its licensing. Reasonably well written Some software takes days of effort and still does not work right.  We have limited resources and reserve the right to \"give up\". Sometimes software is written for workstations and does not function correctly in a shared environment. Downloadable Some software requires additional steps to download installation files, such as registering on a website or accepting a license agreement. In these cases we ask researchers to download files and put them in a directory on the HPC storage. When you submit a software installation request, let us know that you have already downloaded the files and provide path to the directory where they are located."},{"location":"software/overview/#commercialfee-based-software","title":"Commercial/Fee-based Software","text":"<p>The University of Arizona Research Computing facility has many commercial and freeware packages installed on our supercomputers. Our approach to acquisition of additional software depends upon its cost, licensing restrictions, and user interest.   </p> Audience Single user interest The license for the software is purchased by the user and his/her department or sponsor.  This software is best installed by the user.  There are two main options; the first and easiest, is to install the software locally in the relevant user's account using the example procedure. The second is to use the \"contrib\" environment.  The advantage is that you can share the software built here with other users. This is created through a support ticket where a consultant will create a \"contrib\" group in which you can build software and add users. Group interest If a package is of interest to a group of several users, the best approach at first is for one user to act as a primary sponsor and arrange to split the procurement/licensing costs among the group. We can install the software and manage the user access according to requests from the group. Broad interest The High Performance Computing team will consider acquiring and supporting software packages that have broad interest among our users. Full facility support will depend on the cost of the package and our ability to comply with any restrictive licensing conditions."},{"location":"software/overview/#unsupported-software","title":"Unsupported Software","text":"<p>Unfortunately, our HPC system is not configured to support all software use cases. We have summarized the main scenarios which cause software to be unsupported by our system below. Prior to submitting an installation request, double-check that your software requirements don't fall into one of these categories. While the HPC may not be able to support these cases, it may be possible that other campus resources are able to. We encourage you to contact services listed in our Community Resources page to see if they are able to support your workflow's requirements.</p> <p>The below list is not exhaustive and may be expanded as new scenarios are encountered. If you are unsure whether your desired software is supported, feel free to contact our consultants.</p> Software Requirements Examples Non-SSH External Connections Software requiring external communications that utilize protocols other than SSH are not supported. Job Management/Scheduling The UArizona HPC uses Slurm as its task scheduler and resource manager. Software requiring a different scheduler is unsupported. Apache SparkSun Grid Engine Workstation Software Software that is designed to be run on a local Linux workstation often requires root privileges and display management that may not be compatible with a shared, remote system like HPC Persistent Databases/Servers The UArizona HPC is not configured to support databases, server instances, or other persistent software-specific daemons. SQL databasesApplication deploymentsSAS server Windows Applications While there are plenty of excellent Windows software suites available for scientific computing, they unfortunately cannot be run on HPC. There are, however, national resources available that may support your application. One example is JetStream2 available through an ACCESS Allocation. See our Community Resources page linked at the top of this page for more details. ArcGIS"},{"location":"software/overview/#federal-regulations","title":"Federal Regulations","text":"<p>By policy, it is prohibited to use any of the facility's resources in any manner that violates the US Export Administration Regulations (EAR) or the International Trafficking in Arms Regulations (ITAR). It is relevant in this regard to be aware that the facility employs analysts who are foreign, nonresident, nationals and who have root-access privileges to all files and data. Specifically, you must agree not to use any software or data on facility systems that are restricted under EAR and/or ITAR.</p>"},{"location":"software/popular_software/R/","title":"R","text":"<p>R is a popular language for data analysis and visualization. Different versions are available as software modules and we provide the graphical interface RStudio for R through our Open OnDemand web interface.</p> <p>Similar to other languages that use package managers to install libraries contributed by the user community, we recommend you create and manage your own local libraries in your account. This ensures a stable global environment for all users and that you have the most control over your packages' versions and dependencies.</p> <p>We provide instructions below for how to create, use, and switch between libraries as well as some debugging techniques for when package installations fail.</p> <p>RStudio is a popular method for running analyses (and for good reason!), but for longer-running jobs (say, many hours or days) or workflows that need more flexibility in their environment (e.g., need access to software installed as system modules such as gdal), we recommend batch submissions.</p>"},{"location":"software/popular_software/R/#creating-a-custom-library","title":"Creating a Custom Library","text":"<p>R Package Debugging</p> <p>R packages can be finicky. See Switching Between Custom Libraries and Common Problems below to help with frequent user issues. Alternatively, you can consider using Mamba to manage your R packages.</p> <p>Creating your first library</p> <ol> <li> <p>Make a local directory to store your packages. It's recommended to include information about your library in the name, e.g., which version of R you're using. It's important to maintain a consistent version of R with your libraries since installing packages into one library with multiple versions or R will often cause trouble. Need to switch versions of R? Creating a new library will help.    <pre><code>mkdir -p ~/R/library_4.2\n</code></pre></p> </li> <li> <p>Tell R where your library is by creating an environment file     <pre><code>echo 'R_LIBS=~/R/library_4.2/' &gt;&gt; ~/.Renviron\n</code></pre>     The file <code>~/.Renviron</code> is a \"dot\" file which means it does not show up when you run a standard <code>ls</code>. This particular file can be used to control your R environment for each subsequent time you start a session. All the <code>echo</code> command does is append the line <code>R_LIBS=~/R/library_4.2/</code> to this file. </p> </li> <li> <p>That's it! Now you can install packages and they will be stored in the directory you just created. For example, to install and load the package <code>ggplot2</code>:     <pre><code>module load R/4.2\nR\ninstall.packages(\"ggplot2\")\n</code></pre></p> </li> </ol>"},{"location":"software/popular_software/R/#switching-between-custom-libraries","title":"Switching Between Custom Libraries","text":"<p>If you're switching versions of R, we recommend you use a different library. See Common Problems below for more information. When creating a library, consider including pertinent information in the name such as R version. For example, if you were previously using R 4.2 and wanted to switch to using R 4.1, you could create a directory called <code>library_4.1</code> using: <pre><code>mkdir -p ~/R/library_4.1\n</code></pre> Then, to use your new library, edit your <code>~/.Renviron</code> file using a text editor such as <code>nano</code>: <pre><code>nano ~/.Renviron\n</code></pre> Once your text editor opens, set the <code>R_LIBS</code> previously defined in your file to the name and location of your new library. In this case, this would look like: <pre><code>R_LIBS=~/R/library_4.1\n</code></pre> To exit <code>nano</code>, use Ctrl+X and save at the prompt. Once your file is saved, you're ready to start installing files into your new library. </p>"},{"location":"software/popular_software/R/#common-problems-and-how-to-debug-them","title":"Common Problems and How to Debug Them","text":"<p>Working on a cluster without root privileges can lead to complications. For general information on package installations, see the r-bloggers documentation. For information on common installation problems on our clusters, see the section below with with suggested solutions:</p> AnacondaA Corrupted EnvironmentLibrary IssuesMixing R VersionsOpen OnDemand RStudio Issues <p>One common reason R packages won't install is an altered environment. This can frequently be caused by the presence of Anaconda (or Miniconda) installed locally or initialized in your account from our system module.</p> <p>We have instructions on how to remove Anaconda from your environment in our Anaconda documentation. </p> <p>If Anaconda is not initialized in your account, there might be other culprits that are corrupting your environment.</p> <p>Look for any of the file types listed below on your account. If you find them, try removing them (make a backup if you need them) and try the installation again.</p> <ul> <li>Saved R sessions. If this is the case, after starting a session, you will get the message <code>[Previously saved workspace restored]</code>. Old sessions may be saved as a hidden file <code>.RData</code> in your home directory. Alternatively, they may be stored under <code>~/.local/share/rstudio</code>. Where old sessions are stored is dependent on the version of R you are using.</li> <li>Gnu compilers</li> <li>Windows files</li> </ul> <p>Have you set up a custom library? Are you switching between custom libraries? You may want to check that everything is being loaded from the correct location and that there are not multiple or unwanted libraries being used.</p> <p>Double-check that you have an <code>.Renviron</code> file. This is a hidden file located in your home directory and should set the path to your custom R library. If you do not have a custom library name set up, R will create one for you saved as something like: <pre><code>~/R/x86_64-pc-linux-gnu-library\n</code></pre> This directory can lead to unwanted behavior. For example, if you're trying to use a new custom library (such as when switching R version), R will still search x86_64-pc-linux-gnu-library for package dependencies and may cause installs to fail. To fix this, rename these types of folders something unique and descriptive.</p> <p>To set up/switch custom libraries, follow the instructions in the Creating a Custom R Library section above.</p> <p>Because HPC is a cluster where multiple versions of R are available, users should take care to avoid mixing and matching. Because packages often depend on one another, libraries using different versions of R can turn into a tangled mess.  Common errors that can crop up include: <code>Error: package or namespace load failed.</code></p> <p>If you're switching R versions and have a custom library defined in your <code>~/.Renviron</code> file, we recommend creating a new library.</p> <p>RStudio is a great tool! Sometimes though, because it's a different environment than working directly from the terminal, you may run into problems. Specifically, these typically arise for installs or when using packages that rely on software modules.  </p> <p>Package Installations</p> <p>If you're trying to install a package in an OOD RStudio session and you've tried all the troubleshooting advice in the other tabs without luck, try starting R in the terminal and give the installation another try. You can access an R session in the terminal by first starting an interactive session, then using:</p> <pre><code>$ module load R/&lt;version&gt;\n$ R\n&gt; install.packages(\"package_name\")\n</code></pre> <p>Accessing Modules</p> <p>RStudio does not natively have access to <code>module load</code> commands. This means that if you have a package that relies on a system module, the easiest option is to work through an interactive terminal session or to submit a batch script.</p> <p>The alternative is to create a hidden directory in your account called <code>~/.UAz_ood/rstudio.sh</code> with the module load commands you need. More information on this can be found in the section Loading Modules in RStudio below.</p> <p>Font Issues</p> <p>RStudio uses Apptainer under the hood. As a result, there are some environment differences that may affect correct font formatting in images generated in RStudio. If you are experiencing this, add the following line to the hidden file <code>~/.Renviron</code> in your account (you can create this file if it does not exist):</p> <pre><code>FONTCONFIG_PATH=/opt/ohpc/pub/apps/fontconfig/2.14.2/etc/fonts\n</code></pre>"},{"location":"software/popular_software/R/#using-rstudio","title":"Using RStudio","text":"<p>Jupyter</p> <p>R is one of the core languages that Jupyter supports. You can also use Jupyter as an alternative GUI to RStudio. For more information, see Mamba.</p> Open OnDemandApptainer/Singularity <p>We provide access to the popular development environment RStudio through our Open OnDemand web interface. This is a very handy tool, though it should be noted that it is a less flexible environment than using R from the command line. This is because RStudio sets its own environment which prevents easy access to third party software installed as system modules. These issues can sometimes worked around by following the guide in the debugging section above.</p> <p>In some circumstances, you may want to run RStudio using your own Apptainer (rebranded from Singularity) image. For example, this allows access to different versions of R not provided when using our OOD application. We have some instructions on one way to do this below.</p> <p>First, log into HPC using an Open OnDemand Desktop session and open a terminal. A Desktop session is the easiest solution to access RStudio since it eliminates the need for port forwarding.</p> <p>In the terminal, make an RStudio directory where all of the necessary files will be stored. In this example, we'll be working in our home directory and will pull an RStudio image from Dockerhub to use as a test. If you're interested, you can find different RStudio images under rocker in Dockerhub.</p> <pre><code>mkdir $HOME/RStudio\ncd $HOME/RStudio\napptainer pull ./geospatial.sif docker://rocker/geospatial.sif\n</code></pre> <p>Next, create the necessary directories RStudio will use to generate temporary files. You will also generate a secure cookie key.</p> <pre><code>TMPDIR=$HOME/RStudio/rstudio-tmp\nmkdir -p $TMPDIR/tmp/rstudio-server\nuuidgen &gt; $TMPDIR/tmp/rstudio-server/secure-cookie-key\nchmod 600 $TMPDIR/tmp/rstudio-server/secure-cookie-key\nmkdir -p $TMPDIR/var/{lib,run}  \n</code></pre> <p>Next, create a file in your RStudio directory called rserver.sh and make it an executable:</p> <p><pre><code>touch rserver.sh\nchmod u+x rserver.sh\n</code></pre> Open the file in your favorite editor and enter the content below. Modify the variables under <code>USER OPTIONS</code> to match your account if necessary. You can change <code>PASSWORD</code> to any password you'd like to use. Once you've entered the contents, save and exit:</p> <pre><code>#!/bin/bash\n\n# --- USER OPTIONS --- #\nWD=$HOME/RStudio\nSIFNAME=geospatial.sif\nPASSWORD=\"PASSWORD\"\n\n# --- SERVER STARTUP EXECUTED BELOW --- #\nNETID=$(whoami)\nTMPDIR=$WD/rstudio-tmp\nSIF=$WD/$SIFNAME\nPASSWORD=$PASSWORD apptainer exec -B $TMPDIR/var/lib:/var/lib/rstudio-server -B $TMPDIR/var/run:/var/run/rstudio-server  -B $TMPDIR/tmp:/tmp $SIF rserver --auth-none=0 --auth-pam-helper-path=pam-helper --server-user=$NETID --www-address=127.0.0.1\n</code></pre> <p>Now, in your desktop session's terminal, execute the rserver.sh script using <code>./rserver.sh</code></p> <p></p> <p>Next, open a Firefox window and enter <code>localhost:8787</code> for the URL. In your browser, you will be prompted to log into your RStudio server. Enter your NetID under Username. Under Password, enter the password you defined in the script server.sh.</p> <p></p> <p>This will open your RStudio session:</p> <p></p>"},{"location":"software/popular_software/R/#loading-modules-in-rstudio-new","title":"Loading Modules in RStudio (New!)","text":"<p>Library installations still must be performed on the command line</p> <p>The method detailed below is used to faciliate loading R libraries in RStudio when they depend on system software modules. R library installations will still need to take place in an interactive session on the command line. (1)</p> <ol> <li>This is because the container that is necessary to start RStudio overrides environment variables set by modules that are necessary for successful library compilations.</li> </ol> <p>If you are using the RStudio application in Open OnDemand, it is now possible to load additional software modules into your environment. You might want to do this if your R libraries depend on modules. An example of this might be the Seurat package which depends on the modules gdal, proj, sqlite3, and geos. </p> <p>Method to Load Modules</p> <p>To load modules, you will need to create a hidden directory in your home called <code>~/.UAz_ood</code>. Inside that directory, create a file called <code>rstudio.sh</code>. You can then add any <code>module load</code> statements you need to this file. The file <code>rstudio.sh</code> is sourced when your RStudio session initiates, so if you modify your the file, you will need to start a new RStudio session for the changes to take effect.</p> <p>An example of how to create this file with example contents is shown below. </p> <p>Example</p> <p>This example assumes you are working on the command line. Start by first creating the necessary directory and file:</p> <p><pre><code>mkdir -p ~/.UAz_ood\ntouch ~/.UAz_ood/rstudio.sh\n</code></pre> Next, open <code>rstudio.sh</code> in your favorite text editor. If you are not familiar with command line text editors, a beginner-friendly tool is nano. For example:</p> <p><pre><code>nano ~/.UAz_ood/rstudio.sh\n</code></pre> Next, in the file add the modules that you would like to load in RStudio. For example:</p> <pre><code>module load gdal/3.8.5 geos/3.9.5 proj/9.4.0 sqlite/3.45\n</code></pre> <p>Now, save and exit. If you're using nano, you can do this with Ctrl+X, then select Y to save your changes and exit. Once your file exists with the desired contents, start an new OnDemand RStudio session. </p>"},{"location":"software/popular_software/R/#setting-a-new-user-state-directory","title":"Setting a New User State Directory","text":"<p>When working on a large project in RStudio, it is possible for your R session's data to fill up your home directory resulting in out-of-space errors (e.g. when trying to edit files, create new OOD sessions, etc). With the newest version of RStudio, you can find these saved session files under <code>~/.local/share/rstudio</code>.</p> <p>To preserve space in your home, you can specify a different directory by setting the environment variable <code>RSTUDIO_DATA_HOME</code>. To do this, open the hidden file <code>~/.bashrc</code> and add:</p> <pre><code>export RSTUDIO_DATA_HOME=&lt;/path/to/new/directory&gt;\n</code></pre> <p>where <code>&lt;/path/to/new/directory&gt;</code> is the path to a different location where you have a larger space quota. For example, <code>/groups/&lt;YOUR_PI&gt;/&lt;YOUR_NETID&gt;/rstudio_sessions</code>.</p>"},{"location":"software/popular_software/R/#setting-your-working-directory","title":"Setting Your Working Directory","text":"Current SessionAll Non-Project Sessions <p>If you'd like to change your working directory in an RStudio session, one option is to use <code>setwd(\"/path/to/directory\")</code> in your terminal. Alternatively, if you'd like to see the contents of your new workspace in your file browser, you can navigate to the Session dropdown tab, navigate to Set Working Directory, and click Choose Directory...</p> <p></p> <p>From there, either navigate to the desired subdirectory, or click the ellipsis <code>...</code> in the upper right to enter the full path to a directory.</p> <p></p> <p>Once you click OK and then Choose in the main file navigation window, R will change its working directory and you should see the contents of your new space under the Files browser in the lower right.</p> <p></p> <p>If you'd like to permanently set a different default working directory for all non-project RStudio sessions, navigate to the Tools dropdown tab and select Global Options...</p> <p></p> <p>This will open a menu where you can set your default working directory under General. Click the Browse... button to open a file navigator</p> <p></p> <p>To select a new working directory, either navigate to the subdirectory of your current working space, or select the ellipsis <code>...</code> in the upper right to allow you to enter the full path. </p> <p></p> <p>The ellipsis option allows for more flexibility such as pointing to an <code>/xdisk</code> or <code>/groups</code> space.</p> <p></p> <p>Next, click OK, then Choose in the Choose Directory window, then Apply in the Global Options menu. This will set your working directory for your current session as well as all future sessions.</p> <p></p>"},{"location":"software/popular_software/R/#popular-packages","title":"Popular Packages","text":"<p>Below, we document some installation instructions for common R packages. We attempt to keep these instructions reasonably up-to-date. However, given the nature of ongoing software and package updates, there may be discrepancies due to version changes. If you notice any instructions that don't work, contact our consultants and they can help. </p> <p>Alternative installation</p> <p>The instructions below show how you can install these packages with the R modules described above. An alternative is to install these packages in a Conda environment with a package manager like Mamba. For more information, see Mamba.</p> Seurat and SeuratDiskMonocle3Terra <p>R Studio Version</p> <p>If you use RStudio for your analyses, make sure that you load the same version of R when working with modules on the command line.</p> <p>Anaconda must be removed from your environment</p> <p>You will need to make sure Anaconda is completely removed from your environment prior to the install. If you have Anaconda initialized in your account, see the code block on our Anaconda page under Removing Anaconda From Your Environment --&gt; Temporary Removal.</p> <p>Installs must be done in a terminal</p> <p>To install Seurat and SeuratDisk, you'll need to be in an interactive terminal session and not in an RStudio session. Once your installation is successful, it is possible to load these modules in RStudio. More details are provided below. </p> SeuratSeuratDisk <pre><code>(elgato) [netid@junonia ~]$ interactive -a &lt;your_group&gt;\n[netid@cpu38 ~]$ module load R/&lt;version&gt;\n[netid@cpu38 ~]$ module load gdal/3.3.2 glpk/5.0 libpng/1.6.37 # software modules that are needed for Seurat's dependencies\n[netid@cpu38 ~]$ R\n&gt; install.packages(\"Seurat\")\n</code></pre> <p>If you want to load this software in an RStudio session, you will need to create the file <code>~/.UAz_ood/rstudio.sh</code>. See the Loading Modules in RStudio section above for more information. </p> <p>SeuratDisk is similar to Seurat with a few more dependencies. It also includes the line <code>unset CPPFLAGS</code> due to a reported issue with the dependency hdf5r:</p> <pre><code>(elgato) [netid@junonia ~]$ interactive -a &lt;your_group&gt;\n[netid@cpu1 ~]$ module load R/&lt;version&gt; gdal/3.3.2 geos/3.9.1 hdf5/1.10.5 libpng/1.6.37 glpk/5.0 libgit2/1.8.1\n[netid@cpu1 ~]$ unset CPPFLAGS\n[netid@cpu1 ~]$ R\n&gt; install.packages(\"Seurat\")\n&gt; install.packages(\"remotes\")\n&gt; remotes::install_github(\"mojaveazure/seurat-disk\")\n</code></pre> <p>If you want to load this software in an RStudio session, you will need to create the file <code>~/.UAz_ood/rstudio.sh</code>. See the Loading Modules in RStudio section above for more information. </p> <p>R Studio Version</p> <p>If you use RStudio for your analyses, make sure that you load the same version of R when working with modules on the command line.</p> <p>Installs must be done in a terminal</p> <p>To install Monocle3, you'll need to be in an interactive terminal session and not in an RStudio session. Once your installation is successful, it is possible to load these modules in RStudio. More details are provided below. </p> <p>Anaconda must be removed from your environment</p> <p>You will need to make sure Anaconda is completely removed from your environment prior to the install. If you have Anaconda initialized in your account, see the code block on our Anaconda page under Removing Anaconda From Your Environment --&gt; Temporary Removal.</p> <p>Monocle3's documentation includes steps that you can use for a successful installation.</p> <pre><code>(elgato) [netid@junonia ~]$ interactive -a your_group\n[netid@cpu1 ~]$ module load R/&lt;version&gt; gdal/3.3.2\n[netid@cpu1 ~]$ R\n&gt; install.packages(\"BiocManager\")\n&gt; BiocManager::install(c('BiocGenerics', 'DelayedArray', 'DelayedMatrixStats',\n                       'limma', 'lme4', 'S4Vectors', 'SingleCellExperiment',\n                       'SummarizedExperiment', 'batchelor', 'HDF5Array',\n                       'terra', 'ggrastr'))\n&gt; install.packages(\"remotes\")\n&gt; remotes::install_github('cole-trapnell-lab/monocle3')\n</code></pre> <p>Then, to load Monocle3 in RStudio, you will need to create the file <code>~/.UAz_ood/rstudio.sh</code> as detailed in the Loading Modules in RStudio section above. </p> <p>R Studio Version</p> <p>If you use RStudio for your analyses, make sure that you load the same version of R when working with modules on the command line.</p> <p>Installs must be done in a terminal</p> <p>To install Monocle3, you'll need to be in an interactive terminal session and not in an RStudio session. Once your installation is successful, it is possible to load these modules in RStudio. More details are provided below. </p> <p>Anaconda must be removed from your environment</p> <p>You will need to make sure Anaconda is completely removed from your environment prior to the install. If you have Anaconda initialized in your account, see the code block on our Anaconda page under Removing Anaconda From Your Environment --&gt; Temporary Removal.</p> <p>To install the R package <code>terra</code>, you will need to load the module <code>gdal</code> which will pull in other dependencies (<code>geos</code>, <code>proj</code>, and <code>sqlite</code>). In this example, we'll use the modules <code>R/4.3</code> and <code>gdal/3.8.5</code></p> <p><pre><code>(elgato) [netid@junonia ~]$ interactive -a &lt;your_group&gt;\n[netid@cpu1 ~]$ module load R/&lt;version&gt; gdal/3.8.5\n[netid@cpu1 ~]$ R\n&gt; install.packages(\"terra\")\n</code></pre> Then, to load Terra in RStudio, you will need to create the file <code>~/.UAz_ood/rstudio.sh</code> as detailed in the Loading Modules in RStudio section above. </p>"},{"location":"software/popular_software/R/#example-jobs","title":"Example Jobs","text":"<p>Specify your R version</p> <p>Note that it's always a good idea to explicitly specify the version of R you want to use when loading an R module in a batch script. When modules are updated, the default changes to the most recent version. This may create conflicts and library issues. Always specifying your R version is a good way to ensure a consistent environment. </p> <p>Batch job basics</p> <p>The examples below assume a basic familiarity with batch scripts. If you've never submitted a batch script before, check out our Introduction to Batch Jobs for a comprehensive walkthrough. </p> <p>Below are some examples on how to submit R analyses as batch jobs. R jobs may also be run using interactive terminal sessions or using RStudio through Open OnDemand. </p> Basic R ExamplePlotting in a Batch ScriptR Array Jobs <p>Download Example</p> <p>To run an R job as a batch submission, you can include all of your R commands in a single file and execute it with the command <code>Rscript</code>.</p> <p>R Script</p> <p>To start, we'll create a simple R script that will print the line <code>Hello World!</code>. We'll call this script <code>hello_world.r</code> and can create it using the command <code>touch</code>: <pre><code>[netid@wentletrap ~]$ touch hello_world.r\n</code></pre> Now, open the file in your favorite text editor and add the R commands that you want to run in your job. In this case, we'll use: <pre><code>#!/usr/bin/env Rscript\n\nhello_string &lt;- \"Hello World! \"\nprint (hello_string, quote=FALSE)\n</code></pre></p> <p>Slurm Script</p> <p>To call this R script in a batch submission, we'll load the R version we'd like to use and then execute our workflow with the command <code>Rscript hello_world.r</code>. Create a batch submission file using the same method as before,<code>touch submit_r_script.slurm</code>, and open it in your favorite text editor. Then, in your Slurm submission file, add the following:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=R-Plotting-Job\n#SBATCH --ntasks=1\n#SBATCH --nodes=1 \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n\nmodule load R/4.0.0\nRscript hello_world.r\n</code></pre> <p>Submitting your job</p> <p>To run the batch job, submit it to the scheduler using the command <code>sbatch</code> followed by the Slurm file\u2019s name. This will return a job ID that you can use to track the status of your job. In this case:</p> <pre><code>[netid@wentletrap ~]$ sbatch submit_r_script.slurm \nSubmitted batch job 53341\n</code></pre> <p>Output</p> <p>An output file will be generated by the scheduler containing any output that would have been printed to the terminal had you run your batch script interactively. <pre><code>[netid@wentletrap ~]$ cat slurm-53341.out \n[1] Hello World! \n</code></pre></p> <p>Download Example</p> <p>Creating and saving figures as a part of your workflow is possible in a batch script. You can save your figure to a specified directory (the default is your working directory), give it a custom name, control your image quality and dimensions, and choose your output format (e.g., pdf, png, jpg, etc.). An example is included below.</p> <p>RScript</p> <p>To start, we'll write an R script that will create a sinusoidal plot. We'll call this script <code>example.r</code> and can create it using the command <code>touch</code>:</p> <pre><code>[netid@wentletrap ~]$ touch example.r\n</code></pre> <p>Now open the text file in your favorite editor and add the following:</p> <pre><code>print (\"In R Script. Plotting...\")\nx &lt;- seq(-pi,pi,0.1)\npng(\"rplot.png\") \nplot(x, sin(x))\ndev.off()\n</code></pre> <p>In the example above, we're using <code>png(\"rplot.png\")</code> to save our output figure to png format rather than displaying the image interactively and the <code>dev.off()</code> closes the file after it is generated. </p> <p>Batch Script</p> <p>Next, we'll create our submission file using <code>touch submit_r_script.slurm</code> and add the contents: <pre><code>#!/bin/bash\n#SBATCH --job-name=R-Plotting-Job\n#SBATCH --ntasks=1\n#SBATCH --nodes=1 \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n\nmodule load R/4.0.0\nRscript example.r\n</code></pre></p> <p>The command <code>RScript</code> is used to execute our R script in batch mode allowing it to be run non-interactively.</p> <p>Job Submission</p> <p>Next, we'll submit our job using <code>sbatch</code>:</p> <pre><code>[netid@wentletrap ~]$ sbatch submit_r_script.slurm \nSubmitted batch job 53337\n</code></pre> <p>Output</p> <p>Once our job has completed, we should see both a Slurm output file which contains any text that would have been printed to the terminal had we run our commands interactively, as well as the image file we specified.  <pre><code>[netid@wentletrap ~]$ ls\nslurm-53337.out  rplot.png  example.r  submit_r_script.slurm\n[netid@wentletrap ~]$ cat slurm-53337.out \n[1] \"In R Script. Plotting...\"\nnull device \n          1    \n</code></pre> The contents of <code>rplot.png</code> should be:</p> <p></p> <p>Array Jobs Intro</p> <p>Unsure what an array job is? See our Array Jobs documentation which provides in-depth information on their function and implementation. </p> <p>Download Example</p> <p>In this example, we\u2019ll create an R script that generates 1000 randomized 1s and 0s, stores them as a dataframe, then saves the dataframe to an output file. We\u2019ll run this R script as an array job to simulate what a researcher might do if they were performing multiple independent simulations using the same R script.</p> <p>R Script</p> <p>This script is designed to accept command line input using the following syntax:</p> <p><pre><code>Rscript r_array_example.R $SLURM_ARRAY_TASK_ID\n</code></pre> Since we\u2019re using the same file for each simulation, if we were to create a single static output filename for our <code>save()</code> command, each simulation would overwrite the one that came before it. To deal with this issue, we\u2019ll make use of the Slurm environment variable <code>$SLURM_ARRAY_TASK_ID</code> to differentiate them.</p> <p>The use of <code>commandArgs()</code> is to pull in that task ID to our R script so that we can use it in our output filenames.</p> <p>Start by creating an R script by using the <code>touch</code> command</p> <p><pre><code>[netid@cpu4 ~]$ touch save_example.R\n</code></pre> Next, open the file in your favorite text editor and add the contents:</p> <pre><code># We'll pull in any command line arguments used in executing this script.\n# This is to grab the SLURM_ARRAY_TASK_ID that's associated with this particular\n# array subjob. We'll use this integer to differentiate save files so multiple\n# simulations don't overwrite one another.\nargs&lt;-commandArgs(TRUE)\n\n# An example of a workflow is included below\n\n# Generate sample of 1000 random 0s and 1s\na &lt;-sample(0:1, 1000, rep = TRUE)\n# Save as data.fame\ndf &lt;- data.frame(replicate(10,sample(0:1,1000,rep=TRUE)))\n\n# Now we'll save our dataframe to a unique filename. If we run our job with:\n# Rscript r_array_example.R $SLURM_ARRAY_TASK_ID\n# then args[1] will be set to the job's unique integer. Using paste, we can\n# add this into our save filename.\nfilename &lt;- paste(\"random_sample_df_run\", args[1], \".rda\", sep = \"\", collapse = NULL)\n\n# For demonstration purposes, we'll print out the expected filename\nsprintf(\"Simulation complete. Saving dataframe to filename: %s\", filename)\n\nsave(df, file = filename)\n</code></pre> <p>Submission Script</p> <p>Our submission script includes the usual directives as well as the additional <code>--array=1-2</code> directive. This tells the scheduler to submit two jobs with array indices 1 and 2. </p> <pre><code>#!/bin/bash\n#SBATCH --account=your_group_here\n#SBATCH --partition=standard\n#SBATCH --time=00:01:00\n#SBATCH --ntasks=1\n#SBATCH --nodes=1\n#SBATCH --array=1-2\n\nmodule load R/4.1.0\n\nRscript save_example.R $SLURM_ARRAY_TASK_ID\n</code></pre> <p>Job Submission</p> <pre><code>[netid@cpu4 ~]$ sbatch r_array_example.slurm \nSubmitted batch job 260764\n</code></pre> <p>Output</p> <p>We can now see that two jobs were submitted, each with their own Slurm output file as well as their own distinct R output.</p> <pre><code>[netid@cpu4 ~]$ cat slurm-260764_* | grep Simulation\n[1] \"Simulation complete. Saving dataframe to filename: random_sample_df_run1.rda\"\n[1] \"Simulation complete. Saving dataframe to filename: random_sample_df_run2.rda\"\n[netid@cpu4 ~]$ ls random_sample_df_run*\nrandom_sample_df_run1.rda  random_sample_df_run2.rda\n</code></pre>"},{"location":"software/popular_software/alphafold/","title":"AlphaFold","text":"<p>AlphaFold is an AI system developed by the Google DeepMind project to predict the 3D structure of a protein from its amino acid sequence. AlphaFold needs multiple datasets for inference, the combined size of which is around several TBs. We currently host the datasets for AlphaFold 2 &amp; 3, and the corresponding modules. The datasets are available under <code>/contrib/datasets/alphafold</code>.</p> <p>Each AlphaFold module loads an Apptainer container containing the corresponding AlphaFold. You can use the containers directly, if you want. They are available under <code>/contrib/singularity/alphafold</code>. The modules however provide some useful shortcuts, and you can use them without knowing anything about Apptainer containers.</p>"},{"location":"software/popular_software/alphafold/#alphafold-2","title":"AlphaFold 2","text":"<p>We currently host AlphaFold version 2.3.0. For versions 2.3.1 or 2.3.2, you will have to build your own Apptainer containers. You can use corresponding Docker images for that purpose. For more information on building your own Apptainer containers from Docker images, see Containers. These use the same datasets as version 2.3.0.</p> <p>When you load the <code>alphafold/2.3.0</code> module it adds the following variables to your environment:</p> <ul> <li><code>ALPHAFOLD_DIR</code> which points to <code>/contrib/singularity/alphafold</code></li> <li><code>ALPHAFOLD_DATADIR</code> which points to <code>/contrib/datasets/alphafold/2.3.0</code></li> </ul> <p>You can use these variables to easily access the container and the datasets. You can use the the module from either a batch script or an interactive session, the invocations are the same. For optimum performance, you should use no more than 8 CPU cores. While you can run this without a GPU, we do not recommend that. The following batch script demonstrates how you can use this module:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=alphafold2-run\n#SBATCH --time=04:00:00\n#SBATCH --gres=gpu:volta:1\n#SBATCH --cpus-per-task=8\n#SBATCH --ntasks-per-node=1\n#SBATCH --partition=gpu_standard\n#SBATCH --account=&lt;pi-account&gt;\n\n# Uncomment the following two lines to make predictions on proteins that would be too long to fit into GPU memory.\n# export APPTAINERENV_TF_FORCE_UNIFIED_MEMORY=1 \n# export APPTAINERENV_XLA_PYTHON_CLIENT_MEM_FRACTION=4.0\n\nmodule load alphafold/2.3.0\n\nalphafold --nv \\\n          --use_gpu_relax \\\n          --uniref90_database_path=/data/uniref90/uniref90.fasta  \\\n          --uniref30_database_path=/data/uniref30/UniRef30_2021_03 \\\n          --mgnify_database_path=/data/mgnify/mgy_clusters_2022_05.fa  \\\n          --bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt  \\\n          --pdb70_database_path=/data/pdb70/pdb70  \\\n          --template_mmcif_dir=/data/pdb_mmcif/mmcif_files  \\\n          --obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\\n          --model_preset=monomer \\\n          --max_template_date=2023-08-02 \\\n          --db_preset=full_dbs \\\n          --output_dir=&lt;output_dir&gt; \\\n          --fasta_paths=&lt;input.fasta&gt;\n</code></pre> <p>You can find out the full list of options that you can specify by loading the module in an interactive session and running <code>alphafold --helpfull</code>. </p>"},{"location":"software/popular_software/alphafold/#alphafold-3","title":"AlphaFold 3","text":"<p>We host AlphaFold version 3.0.0. Unlike AlphaFold 2, AlphaFold 3 is not completely free. We host the datasets it needs for inference. However, you need to obtain the model parameters from Google directly. Fill and submit this form to request access to the model parameters from Google. Please ensure that you have read the terms and conditions mentioned in the form and that you can comply with them.</p> <p>After Google approves your request, they will send you an email with a link to download a file containing the model parameters. You will likely download the file on your local computer, it is little less than 1 GB in size. Use one of our file transfer options to transfer this file to your HPC storage. The model parameters file is in a compressed format. You will need to decompress it to use it. From an interactive session, run:</p> <pre><code>module load zstd\nunzstd af3.bin.zstd\n</code></pre> <p>We recommend that you put the decompressed <code>af3.bin</code> file in a directory named <code>models</code>. </p> <p>When you load the <code>alphafold/3.0.0</code> module it adds the following variables to your environment:</p> <ul> <li><code>ALPHAFOLD_DIR</code> which points to <code>/contrib/singularity/alphafold</code></li> <li><code>ALPHAFOLD_DATADIR</code> which points to <code>/contrib/datasets/alphafold/3.0.0</code></li> </ul> <p>AlphaFold 3 requires 1 GPU for inference. Officially only NVIDIA A100 and H100 GPUs, with 80 GB of GPU RAM, are supported. However, you can use older GPUs, such as V100 and P100, or GPUs with less memory, such as one of our A100 slices with 20 GB of GPU RAM. The input size will be limited in these cases. The AlphaFold documentation provides strategies that you can adopt to make runs with larger inputs. To run it on node with P100 or V100 GPUs, add <code>--flash_attention_implementation=xla</code> to the <code>alphafold</code> command in the example below.</p> <p>For optimum performance do not use more than eight CPU cores. The following batch script demonstrates how you can use this module:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=alphafold3-run\n#SBATCH --time=04:00:00\n#SBATCH --gres=gpu:nvidia_a100_80gb_pcie_2g.20gb\n#SBATCH --cpus-per-task=8\n#SBATCH --ntasks-per-node=1\n#SBATCH --partition=gpu_standard\n#SBATCH --account=&lt;pi-account&gt;\n\nmodule load cuda12\nmodule load alphafold/3.0.0\n\nalphafold --db_dir=/data --output_dir=&lt;output_dir&gt; --model_dir=models --json_path=&lt;input.json&gt;\n</code></pre> <p>You can find out the full list of options that you can specify by loading the module in an interactive session and running <code>alphafold --helpfull</code>. </p>"},{"location":"software/popular_software/anaconda/","title":"Anaconda","text":"<p>Anaconda modules are deprecated</p> <p>Anaconda on UA HPC is now deprecated, which means no new version of the Anaconda distribution will be installed, and the existing modules will be removed some time in the future. We recommend users to modify their workflow to replace Anaconda with Mamba. For more information on how to use Mamba, check our Mamba guide.</p> <p>Known Issues with Anaconda</p> <p>Anaconda is known to cause potential issues on HPC Clusters. If it is possible to design your workflow around the native Python package manager Pip, we highly encourage you to do so. </p> <p>If you decide to use Anaconda on our HPC system, please read this page carefully and make yourself aware of the common problems and how to best avoid them.</p>"},{"location":"software/popular_software/anaconda/#overview","title":"Overview","text":"<p>We have several versions of Anaconda installed as system modules for use. You can initialize these in your home directory for access and package management. </p> Version Accessibility 2020.02 <code>module load anaconda/2020.02</code> 2020.11 <code>module load anaconda/2020.11</code> 2022.05 <code>module load anaconda/2022.05</code>"},{"location":"software/popular_software/anaconda/#initializing-anaconda","title":"Initializing Anaconda","text":"<p>Initializing Anaconda in your account only needs to be performed once and is what makes Anaconda available and ready for customization (e.g., installing custom packages) in your account. </p> <p>Faster Reloading</p> <p>Conda will direct you to close and reopen your shell to complete the initialization process. You can skip this by running the command <code>source ~/.bashrc</code> listed in the instructions below. </p> <p>Turn Off Auto-Activate</p> <p>To ensure proper functioning of built-in system functions, turning off auto-activation is highly recommended. Do this by running <code>conda config --set auto_activate_base false</code> in a terminal following initialization.</p> <p>In an interactive session, replacing <code>&lt;version&gt;</code> with your desired Anaconda module version:</p> <pre><code>module load anaconda/&lt;version&gt;\nconda init bash                  \nsource ~/.bashrc  \nconda config --set auto_activate_base false\n</code></pre>"},{"location":"software/popular_software/anaconda/#creating-a-conda-environment","title":"Creating a Conda Environment","text":"<p>Once conda has been configured following the steps above, you can create a local environment. This allows you to control the version of Python you want to use, install your own software, and even create custom Juypyter kernels (making your environment accessible in an OnDemand notebook). To do this, you can use the command <code>conda create</code>. For example, in an interactive session:</p> <pre><code>conda activate\nconda create --name py37 python=3.7 # Build a local environment with a specific version of python\nconda activate py37\n</code></pre> <p>To view the environments available to you, use the command</p> <pre><code>conda env list\n</code></pre>"},{"location":"software/popular_software/anaconda/#installing-conda-packages-and-other-software","title":"Installing Conda Packages and Other Software","text":"<p>Once you have created a conda environment, you can install the packages you need. To do this, follow the software-specific installation instructions. This may be as simple as running <code>conda install &lt;my_package&gt;</code>, or it may involve installing a handful of dependencies. If the installation instructions ask you to create a new environment, you do not have to repeat this step. </p> <p>Once you have performed the install, you should then be able to access your software within this environment. If you are unable to load your software, check your active environment with</p> <pre><code>conda info\n</code></pre> <p>and the installed packages with </p> <pre><code>conda list\n</code></pre>"},{"location":"software/popular_software/anaconda/#conda-in-batch-jobs","title":"Conda in Batch Jobs","text":"<p>If you've turned off Conda's auto-activate feature (recommended), add the following as your first bash command in your batch script:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>This will allow you to run standard <code>conda activate</code> and <code>conda activate &lt;environment name&gt;</code> commands. Without the <code>source</code> command, you may get errors indicating that conda has not been initialized.</p>"},{"location":"software/popular_software/anaconda/#custom-jupyter-kernel","title":"Custom Jupyter Kernel","text":"<p>If you want to make one of your conda environments available for use in one of our Open OnDemand Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment:</p> <pre><code>conda activate\nconda activate &lt;your_environment&gt;\n</code></pre> <p>Next, pip-install Jupyter and use it to create a custom kernel using the command <code>ipython</code> and replacing <code>&lt;your_environment&gt;</code> with your own environment's name:</p> <pre><code>pip install jupyter\nipython kernel install --name &lt;your_environment&gt; --user\n</code></pre> <p>Once you've configured your kernel, go to Open OnDemand and start a Jupyter notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your custom name. For example, if you created a conda environment with the kernel name py38, you should see the following:</p> <p></p> <p>Once you've selected your environment, try checking the Python version in your notebook using the <code>sys</code> module. Additionally, for demonstration purposes, we'll check that a custom package installed in py38 (emoji) can be imported and is working. </p> <p></p>"},{"location":"software/popular_software/anaconda/#loading-modules-in-jupyter","title":"Loading Modules in Jupyter","text":"<p>In OnDemand Jupyter sessions, accessing HPC software modules directly from within a notebook can be challenging due to system configurations. However, it's still possible to access these modules when needed. For instance, machine learning packages like TensorFlow or PyTorch often require additional software modules such as CUDA for GPU utilization.</p> <p>To access software modules in your Jupyter notebooks, follow the steps below:</p> <p>Step 1: If you haven't already done so, create a custom kernel for your Jupyter notebook environment.</p> <p>Step 2: You will then need to edit your kernel configuration file <code>kernel.json</code> which is what sets up your environment at runtime. This file can be found in the following location, where <code>&lt;kernel_name&gt;</code> is a placeholder for the name you gave your kernel when it was created:</p> <pre><code>$HOME/.local/share/jupyter/kernels/&lt;kernel_name&gt;/kernel.json\n</code></pre> <p>Step 3: Next, you will need to modify your kernel's configuration by editing this file. Start by opening it with a text editor, for example <code>nano $HOME/.local/share/jupyter/kernels/&lt;kernel_name&gt;/kernel.json</code>. The contents of this file should look something like the following:</p> <pre><code>{\n \"argv\": [\n  \"&lt;/path/to/your/environment&gt;/bin/python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"&lt;kernel_name&gt;\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\n</code></pre> <p>The part you need to change is the section under <code>argv</code>. We will change this from executing a Python command to a Bash command with a module load statement. Make a note of the path <code>&lt;/path/to/your/environment&gt;/bin/python</code> to use in the edited file. The edited file should look like the following:</p> <pre><code>{\n \"argv\": [\n \"bash\",\n \"-c\",\n \"module load &lt;your_modules_here&gt; ; &lt;/path/to/your/environment&gt;/bin/python -m ipykernel_launcher -f {connection_file}\"\n ],\n \"display_name\": \"&lt;kernel_name&gt;\",\n \"language\": \"python\",\n \"metadata\": {\n \"debugger\": true\n }\n}\n</code></pre> <p>Replace <code>&lt;your_modules_here&gt;</code> with the modules you would like to load and <code>&lt;/path/to/your/environment&gt;/bin/python</code> with the path to your environment's python. </p> <p>Step 4: Save the <code>kernel.json</code> file and restart your Jupyter notebook session.</p>"},{"location":"software/popular_software/anaconda/#removing-anaconda-from-your-environment","title":"Removing Anaconda From Your Environment","text":"<p>When Anaconda is initialized, your <code>.bashrc</code> file is edited so that conda becomes the first thing in your <code>PATH</code> variable any time you're logged into HPC. This can cause all sorts of mayhem when using other system functions or software. </p> <p>If you are running into issues (particularly when using other package managers or compiling), we recommend removing Anaconda from your environment as part of your debugging process. Sometimes it can be as simple as turning off Anaconda's auto-activation, other times it becomes necessary to modify your environment and its variables. </p> <p>Below are three methods for removing Anaconda from your environment:</p> Turn off Auto-activationTemporary RemovalPermanent Removal <p>Removing Auto-activation may not always be sufficient</p> <p>Sometimes turning off auto-activation won't be enough because Anaconda will still be present in your <code>PATH</code>. In this case, follow the instructions in the tab Temporary Removal or Permanent Removal</p> <p>By default, Anaconda's initialization will tell it to automatically activate itself when you log in (when Anaconda is active, you will see a <code>(base)</code> preceding your command prompt). This is known to cause issues, for example, this behavior breaks OnDemand Desktop sessions preventing you from making a connection.</p> <p>If you have not already done so, disable this behavior by running the following from the command line in an interactive terminal session:</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> <p>This will suppress Anaconda's activation until you explicitly call <code>conda activate</code> and is a handy way to have more control over your environment. Once you run this, you will need to log out and log back in again.</p> <p>If you have already turned off Anaconda's auto-activation feature and are still running into issues, it may be necessary to modify your environment variables to fully remove Anaconda. This is because <code>conda deactivate</code> is insufficient and Anaconda binaries and libraries will still be accessible. </p> <p>To fully remove Anaconda, you can either use the command <code>conda deactivate</code> and then manually edit your <code>PATH</code> variable to remove all paths where <code>conda</code> is present, or you can copy the following code block and run it in your terminal: <pre><code>conda deactivate &gt; /dev/null 2&gt;&amp;1\nIFS=':' read -ra PATHAR &lt;&lt;&lt; \"$PATH\"\nfor i in \"${PATHAR[@]}\"\n    do if [[ $i == *\"conda\"* ]]\n        then echo \"removing $i from PATH\"\n    else NEWPATH=$i:$NEWPATH\n    fi\ndone\nexport PATH=$NEWPATH\nmodule unload gnu8 &amp;&amp; module load gnu8\nunset NEWPATH\necho \"Successfully removed conda\"\n</code></pre></p> <p>This is a temporary solution and will only modify your current working environment. Anaconda will still be present for all future HPC sessions. If you would like to make the change permanent, follow the instructions under the Permanent Removal tab. </p> <p>Be careful when editing your <code>~/.bashrc</code></p> <p>Your <code>~/.bashrc</code> file configures your environment each time you start a new session. Be careful when editing it. You may consider making a backup in case of unwanted changes. Not sure what a <code>~/.bashrc</code> is and want more information? Check out our Linux cheat sheet guide.</p> <p>The most permanent solution for removing Anaconda from your environment is to edit your <code>~/.bashrc</code> to manually remove its initialization. This change will remove Anaconda from all future terminal sessions.</p> <p>Start by opening the file <code>~/.bashrc</code>. This can be done using the command <code>nano</code></p> <pre><code>nano ~/.bashrc # opens your bashrc file to edit\n</code></pre> <p>Then comment out or delete the following lines and the text in between:</p> <p><pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n...\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre> To exit use Ctrl+X, select Y to save, and hit Enter to confirm your filename.</p> <p>This change will not take effect right away. To make the changes live, log out of HPC and then log back in again.</p> <p>If you need Anaconda again in the future, you can either uncomment the initialization lines (if you commented them out), or you can initialize Anaconda again.</p>"},{"location":"software/popular_software/gaussian/","title":"Gaussian","text":""},{"location":"software/popular_software/gaussian/#access","title":"Access","text":"<p>In order to access Gaussian and Gaussview, you will need to belong to a special group called g03.  You can request to be added by submitting a help ticket. This is a constraint specific to Gaussian that other modules do not have.</p>"},{"location":"software/popular_software/gaussian/#gpu-notes","title":"GPU Notes","text":"<p>When reading these notes, keep in mind that the GPU nodes on Ocelote have one P100 GPU, 28 cores and 256 GB RAM.</p> <ol> <li> <p>Gaussian 16 can use the NVIDIA P100 GPUs installed on Ocelote.  Earlier GPUs do not have the computational capabilities or memory size to run the algorithms in G16.  Allowing larger amounts of memory is even more important when using GPUs than for CPUs, since larger batches of work must be done at the same time in order to use the GPUs efficiently (see below).</p> </li> <li> <p>When using GPUs it is essential to have the GPU controlled by a specific CPU and is much more efficient if the CPU is physically close to the GPU it is controlling. The hardware arrangement can be checked using the <code>nvidia-smi</code> utility. For example, this output is for a machine with 2 16-core Haswell CPU chips and 4 K80 boards, each of which has two GPUs:</p> <pre><code>GPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 CPU Affinity \nGPU0  X    PIX  SOC  SOC  SOC  SOC  SOC  SOC    0-15\nGPU1  PIX   X   SOC  SOC  SOC  SOC  SOC  SOC    0-15\nGPU2  SOC  SOC   X   PIX  PHB  PHB  PHB  PHB   16-31\nGPU3  SOC  SOC  PIX   X   PHB  PHB  PHB  PHB   16-31\nGPU4  SOC  SOC  PHB  PHB   X   PIX  PXB  PXB   16-31\nGPU5  SOC  SOC  PHB  PHB  PIX   X   PXB  PXB   16-31\nGPU6  SOC  SOC  PHB  PHB  PXB  PXB   X   PIX   16-31\nGPU7  SOC  SOC  PHB  PHB  PXB  PXB  PIX   X    16-31\n</code></pre> <p>The important part is the CPU affinity. This shows that GPUs 0 and 1 (on the first K80 card) are connected to the CPUs on chip 0 while GPUs 2-7 (on the other three K80 cards) are connected to the CPUs on chip 1.  So a job which uses all the CPUs (24 CPUs doing parts of the computation and 8 controlling GPUs) would use input</p> <pre><code>%cpu=0-31\n%gpucpu=0-7=0-1,16-21\n</code></pre> <p>or equivalently but more verbosely</p> <pre><code>%cpu=0-31\n%gpucpu=0,1,2,3,4,5,6,7=0,1,16,17,18,19,20,21\n</code></pre> <p>This pins threads 0-31 to CPUs 0-31 and then uses GPU0 controlled by CPU 0, GPU1 controlled by CPU 1, GPU2 controlled by CPU 16, etc.</p> <p>Normally one uses consecutive numbering in the obvious way, but things can be associated differently in special cases. For example, suppose on the other machine one already had one job using 6 CPUs running with <code>%cpu=16-21</code>.  Then if one wanted to use the other 26 CPUs with 8 controlling GPUs one would specify:</p> <pre><code>%cpu=0-15,22-31\n%gpucpu=0-7=0-1,22-27\n</code></pre> <p>This would create 26 threads with GPUs controlled by the threads on CPUs 0,1,22,23,24,25,26, and 27.</p> </li> <li> <p>GPUs are not helpful for small jobs but are effective for larger molecules when doing DFT energies, gradients and frequencies (for both ground and excited states).  They are not used effectively by post-SCF calculations such as MP2 or CCSD.</p> <p>Each GPU is several times faster than a CPU but since on modern machines there are typically many more CPUs than GPUs, it is important to use all the CPUs as well as the GPUs and the speedup from GPUs is reduced because many CPUs are also used effectively (i.e., in a job which uses all the CPUs and all the GPUs).  For example, if the GPU is 5x faster than a CPU, then the speedup from going to 1 CPU to 1 CPU + 1 GPU would be 5x, but the speedup going from 32 CPUs to 32 CPUs + 8 GPUs would be 32 CPUs -&gt; 24 CPUs + 8 GPUs, which would be equivalent to 24 + 5x8 = 64 CPUs, for a speedup of 64/32 or 2x.</p> </li> <li> <p>The GPUs can have up to 16 GB of memory and one typically tries to have most of this available for Gaussian, which requires at least an equal amount of memory for the CPU thread which is running each GPU.  8 or 9 GB works well if there is 12 GB total on each GPU, or 11-12 GB for a 16 GB GPU.  Since the program gives equal shares of memory to each thread, this means that the total memory allowed should be the number of threads times the memory required to use a GPU efficiently.  For example, when using 4 CPUs and two GPUs each of which has 12 GB of memory, one should use 4 x 12 GB of total memory, i.e. </p> <pre><code>%mem=48gb\n%cpu=0-3\n%gpucpu=0-1=0,2\n</code></pre> <p>(or whatever specific CPU and GPU numbers are appropriate to the machine).</p> </li> <li> <p>GPUs on nodes in a cluster can be used. Since the <code>%cpu</code> and <code>%gpucpu</code> specifications are applied to each node in the cluster, the nodes must have identical configurations (number of GPUs and their affinity to CPUs); since most clusters are collections of identical nodes, this is not usually a problem.</p> </li> </ol>"},{"location":"software/popular_software/mamba/","title":"Mamba","text":"<p>Mamba is a package manager to create and manage Conda environments. Following definitions will help you to understand the relevant terms that you might come across when using Mamba:</p> Conda Conda is a package management system and environment management system for installing multiple versions of software packages and their dependencies and switching between them. Anaconda Anaconda is a distribution built around Conda. It includes hundreds of packages, and access to the Anaconda repository, besides the package manager itself. The use of Anaconda on our HPC systems is now deprecated, due to recent developments in how the company behind Anaconda licenses the Anaconda repository. You can read more on that in the Anaconda FAQs. Mamba Mamba is a package manager which was initially developed as a faster alternative to Conda. It can create and manage Conda environments. For most cases, Mamba is a drop-in replacement for Conda. Mamba comes in multiple flavors, see the Mamba documentation for more information. The most relevant to us is the binary <code>micromamba</code>. For the rest of the document we will specifically focus on <code>micromamba</code>. Environment In the context of this document, an environment is a directory that contains a specific collection of packages that you have installed. We will only consider Conda environments in this document, i.e. environments that can be created and managed with Conda. All flavors of Mamba, including <code>micromamba</code>, can create and manage Conda environments. <p>TL;DR</p> <p>If you were using Conda/Anaconda, use <code>micromamba</code> instead.</p>"},{"location":"software/popular_software/mamba/#initializing-micromamba","title":"Initializing <code>micromamba</code>","text":"<p>You can check the available versions of <code>micromamba</code> with <code>module avail micromamba</code>. Once you have decided which version you want, you can load it and initialize it. You have to initialize it only once.</p> <p>In an interactive session, replacing  with your desired <code>micromamba</code> module version: <pre><code>module load micromamba/&lt;version&gt;\nmicromamba shell init -s bash -r ~/micromamba\nsource ~/.bashrc\n</code></pre> This will create a directory called <code>micromamba</code> in your home folder in which your Conda environments and associated packages will be installed, and all that information to your <code>.bashrc</code>.  <p>Tip</p> <p>Your home directory can fill up pretty fast with environments and packages installed with <code>micromamba</code>. You can change the location where your environments and packages are installed to avoid that. Your PI's group directory can be a good choice for this. Run the following commands to change the location: <pre><code>micromamba config append envs_dirs &lt;new_location&gt;/envs\nmicromamba config append pkgs_dirs &lt;new_location&gt;/pkgs\n</code></pre></p>"},{"location":"software/popular_software/mamba/#creating-a-conda-environment","title":"Creating a Conda Environment","text":"<p>Warning</p> <ol> <li>Always create a local Conda environment with <code>micromamba</code> before installing packages with in.</li> <li>Do not mix Conda environments with any other type of virtual environments.</li> </ol> <p>Tip</p> <p>For more information on environments, check out our Software and Environments workshop.</p> <p>After initializing <code>micromamba</code>, you can create a local Conda environment with it to install packages. If you are used to using Conda to create local environments, then <code>micromamba</code> is very similar. For example, in an interactive session, after loading the <code>micromamba</code> module: <pre><code>micromamba create -n myenv\nmicromamba activate myenv\n</code></pre></p> <p>This will create and activate a Conda environment called <code>myenv</code>. To view the environments available to you, run: <pre><code>micromamba env list\n</code></pre></p> <p>Unlike some versions of Conda, <code>micromamba</code> will always create an empty environment with no packages installed. If you want to install packages, you can do so after activating the environment. You can also mention the packages you want to install when you create the environment. For example, the following command will create an environment named <code>py312</code> with Python 3.12 installed in it: <pre><code>micromamba create -n py312 python=3.12\n</code></pre></p>"},{"location":"software/popular_software/mamba/#installing-packages-in-conda-environments","title":"Installing packages in Conda environments","text":"<p>After activating a Conda environment, you can install packages with:</p> <ol> <li><code>micromamba install &lt;package-name&gt;</code></li> <li>Language-specific package installers (such as <code>pip</code> for Python, or <code>install.packages</code> for R)</li> <li>You can also mix both approaches</li> </ol> <p>Which option you choose will depend on the package, and how the package maintainers make it available.</p> <p>By default <code>micromamba</code> downloads packages from the <code>conda-forge</code> repository or channel. <code>conda-forge</code> is a community maintained repository containing a large number of packages typically used in scientific computing, data science, and others. However, if you want to install packages from other repositories you can do so. For example, the <code>bioconda</code> repository provides many packages used in bioinformatics. </p> <p>To install a package from the <code>bioconda</code> repository, run the following in an interactive environment, replacing <code>&lt;package-name&gt;</code> with the name of your desired package: <pre><code>micromamba install &lt;package-name&gt; -c bioconda\n</code></pre> If you want to install packages from any other repositories, simply replace <code>bioconda</code> with the name of the repository in the above command.</p> <p>Tip</p> <p>By default, <code>micromamba</code> does not access the Anaconda repository, and we recommend not changing that. To ensure that you do not install from the Anaconda repository, you can run the following: <pre><code>micromamba config append channels conda-forge\nmicromamba config append channels nodefaults\nmicromamba config set channel_priority strict\n</code></pre></p>"},{"location":"software/popular_software/mamba/#micromamba-in-batch-jobs","title":"<code>micromamba</code> in Batch Jobs","text":"<p>Assuming that you have initialized <code>micromamba</code> (recommended), you will need to load the <code>micromamba</code> module and activate a Conda environment before you can access the packages installed in that environment. Add the following, replacing <code>&lt;env-name&gt;</code> with the name of your desired Conda environment, as your first bash commands in your batch script:</p> <pre><code>module load micromamba\nmicromamba activate &lt;env-name&gt;\n</code></pre> <p>Tip</p> <p>You should install packages in your Conda environment from an interactive session. Do not put those instructions in a batch script.</p> <p>Initialization Error</p> <p>Sometimes batch jobs with <code>micromamba</code> might fail with the following error message:</p> <pre><code>critical libmamba Shell not initialized\n\n'micromamba' is running as a subprocess and can't modify the parent shell.\nThus you must initialize your shell before using activate and deactivate.\n\n...\n</code></pre> <p>If you have already correctly initialized <code>micromamba</code> as mentioned above and you are still getting this error message, you can do one of the following to ensure that Slurm sources your <code>.bashrc</code> (where the relevant <code>micromamba</code> information is stored):</p> <ul> <li>Replace <code>#!/bin/bash</code> at the top of your batch script with <code>#!/bin/bash --login</code></li> <li>Add <code>source ~/.bashrc</code> to your batch script before you activate the virtual environment</li> </ul> <p>Alternatively, instead of activating a Conda environment with <code>micromamba</code>, you can use <code>micromamba run -n &lt;env-name&gt; &lt;command&gt;</code> to run your command in that Conda environment. For example, if you have the following in your batch script <pre><code>module load micromamba\nmicromamba activate myenv\npython myscript.py\n</code></pre> you can replace it with <pre><code>module load micromamba\nmicromamba run -n myenv python myscript.py\n</code></pre> In this case you will not have to source your <code>.bashrc</code> from your batch script.</p>"},{"location":"software/popular_software/mamba/#language-specific-suggestions","title":"Language specific suggestions","text":"<p>You can use <code>micromamba</code> for some Python or R workflows which would otherwise be non-trivial to do with the corresponding language modules.</p> <p>Warning</p> <p>Do not use the <code>micromamba</code> module along with the Python or R modules, such as <code>python/3.8</code> or <code>R/4.4</code>. Choose one or the other.</p>"},{"location":"software/popular_software/mamba/#python","title":"Python","text":"<p>One of the typical Python workflows on the HPC involves Jupyter through the Open OnDemand (OOD) interface. One issue of this is that if you use the Python modules then you are tied to the specific version of Python that Jupyter on OOD uses. If Jupyter on OOD uses Python 3.8, then you can only use the <code>python/3.8</code> module to install your packages.</p> <p>With <code>micromamba</code> you do not have that limitation. You can use your desired version of Python. All you have to do is the following:</p> <ol> <li>Create and activate a Conda environment with your desired Python version</li> <li>Install Jupyter in it</li> <li>Create a Jupyter kernel</li> </ol> <p>For example, if you want to use Python 3.11, you can try running the following commands from an interactive environment: <pre><code>micromamba create -n &lt;env_name&gt; python=3.11\nmicromamba activate &lt;env_name&gt;\nmicromamba install jupyter # alternatively you can use pip install jupyter\nipython kernel install --name &lt;env_name&gt; --user\n</code></pre> Once you've configured your kernel, go to OOD and start a Jupyter notebook. Once the session starts, open it and click the New dropdown menu in the upper right. If everything is working correctly, you should see your kernel. For example if the kernel's name is <code>torch311</code>:</p> <p></p>"},{"location":"software/popular_software/mamba/#r","title":"R","text":"<p>Some of the most widely used R packages on the HPC have non-trivial installation processes when installed with the R modules, see the Popular Packages for more information. Here we show how you can install these packages with <code>micromamba</code>.</p> <p>Updates and Version Changes</p> <p>We attempt to keep these instructions reasonably up-to-date. However, given the nature of ongoing software and package updates, there may be discrepancies due to version changes. If you notice any instructions that don't work, contact us and we will help. </p> <p>You have to be in an interactive terminal session and not in an RStudio session to run the commands below. To install any of the R packages, first create a Conda environment with R installed in it: <pre><code>micromamba create -n &lt;env_name&gt; r=4.4\nmicromamba activate &lt;env_name&gt;\n</code></pre> If you want to install any other version of R, then replace 4.4 with that version number in the above command. You might have to choose different versions of the packages in the examples below if you use a version of R below 4.4.</p> Seurat &amp; SeuratDiskTerra &amp; Monocle3 <p>To install Seurat, run: <pre><code>micromamba install r-seurat\n</code></pre> Seurat can also be installed with R's built-in package manager <code>install.packages</code>. However installing it with <code>micromamba</code> is way faster, since it just downloads the relevant binaries, and does not have to do any local compilation.</p> <p>For SeuratDisk, assuming you have already installed Seurat:</p> <ol> <li>Install dependencies: <pre><code>micromamba install r-hdf5r r-remotes\n</code></pre></li> <li>Start an R prompt: <pre><code>R\n</code></pre></li> <li>Install SeuratDisk with the <code>install_github</code> function from the <code>remotes</code> R package: <pre><code>remotes::install_github(\"mojaveazure/seurat-disk\")\n</code></pre></li> </ol> <p>To install Terra, run: <pre><code>micromamba install r-terra\n</code></pre> Terra can also be installed with R's built-in package manager <code>install.packages</code>. However installing it with <code>micromamba</code> is way faster, since it just downloads the relevant binaries, and does not have to do any local compilation.</p> <p>For Monocle3, assuming you have already installed Terra:</p> <ol> <li>Install dependencies: <pre><code>micromamba install r-biocmanager r-remotes r-ggrastr\n</code></pre></li> <li>Start an R prompt: <pre><code>R\n</code></pre></li> <li>Install more dependencies with BiocManager: <pre><code>BiocManager::install(c('BiocGenerics', 'DelayedArray', 'DelayedMatrixStats',\n               'limma', 'lme4', 'S4Vectors', 'SingleCellExperiment',\n               'SummarizedExperiment', 'batchelor', 'HDF5Array'))\n</code></pre></li> <li>Install Monocle3 with <code>install_github</code> function from the <code>remotes</code> R package: <pre><code>remotes::install_github('cole-trapnell-lab/monocle3')\n</code></pre></li> </ol>"},{"location":"software/popular_software/mamba/#jupyter","title":"Jupyter","text":"<p>Jupyter is typically thought of as belonging to the Python ecosystem, but R is one of the core languages that Jupyter supports (1). While R practitioners tend to gravitate towards RStudio, you might find that Jupyter is an equally capable, in not more, IDE for programming in R. Particularly, if you are using the Open OnDemand (OOD) interfaces of Jupyter and RStudio, you might find the Jupyter experience to be smoother. With <code>micromamba</code> you can easily set up Jupyter for your R workflow. </p> <ol> <li>It is in the name: Ju[lia]Pyt[hon]eR. The three core languages that Jupyter supports.</li> </ol> <p>To use your R packages from Jupyter, you have to install Jupyter in your Conda environment and then create a with the IRkernel package:</p> <ol> <li>Install Jupyter and IRkernel: <pre><code>micromamba install jupyter r-irkernel\n</code></pre></li> <li>Start an R prompt: <pre><code>R\n</code></pre></li> <li>Create a R kernel for Jupyter, replacing <code>&lt;kernel_name&gt;</code> and <code>&lt;display_name&gt;</code> with your desired kernel and display names: <pre><code>IRkernel::installspec(name = \"&lt;kernel_name&gt;\", displayname = \"&lt;display_name&gt;\")\n</code></pre></li> </ol> <p>Once you've configured your kernel, go to OOD and start a Jupyter notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your kernel. For example, if you had given a display name <code>R 4.4</code> (the default kernel and display names are <code>ir</code> and <code>R</code>, respectively):</p> <p></p>"},{"location":"software/popular_software/mamba/#rstudio","title":"RStudio","text":"<p>To use the Open OnDemand (OOD) interface of RStudio with your Conda environment, do the following:</p> <ol> <li>Create a directory <code>.UAz_ood</code> under your home directory. </li> <li>Create a file <code>rstudio.sh</code> under <code>.UAz_ood</code> with the following contents, replacing <code>&lt;env_name&gt;</code> with the name of your environment:    <pre><code>#!/bin/bash\n\nsource ~/.bashrc\nmicromamba activate &lt;env_name&gt;\n</code></pre></li> </ol> <p>Version Issues</p> <p>Loading some packages such as <code>terra</code> in the RStudio Console might fail with an error message like the following: <pre><code>Error: package or namespace load failed for 'terra' in dyn.load(file, DLLpath = DLLpath, ...):\n unable to load shared object '/groups/sohampal/micromamba/envs/r-test/lib/R/library/terra/libs/terra.so':\n  /lib64/libssl.so.3: version `OPENSSL_3.2.0' not found (required by /groups/sohampal/micromamba/envs/r-test/lib/R/library/terra/libs/../../../.././libcurl.so.4)\n</code></pre> The likely solution in such a case will be to install the correct version of the dependency, in this case, OpenSSL 3.2.0. However, doing so might require you to downgrade the version of R installed in your Conda environment. If you do not want to downgrade, then you might want to consider using Jupyter instead of RStudio as your IDE.</p>"},{"location":"software/popular_software/matlab/","title":"Matlab","text":""},{"location":"software/popular_software/matlab/#overview","title":"Overview","text":"<p>There are three common ways to run Matlab:</p> <ol> <li>Using the Matlab graphical application through Open OnDemand.</li> <li>Using a module to start a graphical mode in an Open OnDemand virtual desktop.</li> <li>The command line version using modules. This is the most common as you will typically submit a job using Slurm.</li> </ol> <p>Like any other application, Matlab has to be loaded as a module before you can use it. To see all the installed versions of the Matlab, use the command <code>module avail matlab</code>.</p>"},{"location":"software/popular_software/matlab/#logging-into-mathworks","title":"Logging Into Mathworks","text":"<p>With some of the latest versions, it's necessary to interactively log into Mathworks to access Matlab. This only needs to be done once using your university credentials. After your credentials are validated, they will be stored, allowing you to run batch jobs</p> <p>To start, open an interactive session, load Matlab, and start an interactive instance:</p> Load and start Matlab<pre><code>[netid@cpu37 ~]$ module load matlab/r2023b\n[netid@cpu37 ~]$ matlab\nMATLAB is selecting SOFTWARE OPENGL rendering.\nPlease enter your MathWorks Account email address and press Enter: \n</code></pre> <p>at the prompt, enter your university email address. For example:</p> Enter your university password and go to URL<pre><code>Please enter your MathWorks Account email address and press Enter: netid@arizona.edu\nYou need a one-time password to sign in. To get a one-time password, follow these steps:\n    1. Go to https://www.mathworks.com/mw_account/otp\n    2. Enter your MathWorks Account email address.\n    3. Copy the generated one-time password.\n    4. Return here and enter the password.\nEnter the one-time password:\n</code></pre> <p>Next, go to the URL they provide (highlighted above) and enter your university email address again</p> <p></p> <p>This will take you through the usual university WebAuth process. Once this is completed, you will be given a temporary code:</p> <p></p> <p>Copy this code to your clipboard and paste it into your terminal:</p> Enter one time password<pre><code>Enter the one-time password:\n12345\n\n\n                                                          &lt; M A T L A B (R) &gt;\n                                                Copyright 1984-2023 The MathWorks, Inc.\n                                           R2023b Update 6 (23.2.0.2485118) 64-bit (glnxa64)\n                                                           December 28, 2023\n\nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n\nUsing 1 thread(s) on compute node.\n&gt;&gt;\n</code></pre> <p>Once the process is complete, you should be able to use Matlab as usual. </p>"},{"location":"software/popular_software/matlab/#running-matlab-analyses-in-batch","title":"Running Matlab Analyses in Batch","text":"<p>The typical procedure for performing calculations on UArizona HPC systems is to run your program non-interactively on compute nodes using a batch submission. The easiest way to run Matlab non-interactively is to use input/output redirection. This method uses Linux operators <code>&lt;</code> and <code>&gt;</code> to point Matlab to the input file and tell where to write the output (see the example script below). The other method is to execute a statement using the <code>-r</code> flag. For details, refer to the manual page for the matlab command.</p> <p>An example batch script might look like the following:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=matlab\n#SBATCH --account=group_name\n#SBATCH --partition=standard\n#SBATCH --ntasks=20\n#SBATCH --nodes\n#SBATCH --mem-per-cpu=5gb\n#SBATCH --time=01:00:00\n\nmodule load matlab/&lt;version&gt;\n\nmatlab -nodisplay -nosplash &lt; script_name.m &gt; output.txt\n</code></pre> <p>The options <code>-nodisplay</code> and <code>-nosplash</code> in the example prevent Matlab from opening graphical elements. To view the full list of options for the <code>matlab</code> command, load the Matlab module and type <code>matlab -h</code> at the prompt. Alternatively, use the link above to see the manual page on the MathWorks website.</p>"},{"location":"software/popular_software/matlab/#parallel-computing-toolbox","title":"Parallel Computing Toolbox","text":""},{"location":"software/popular_software/matlab/#temporary-files","title":"Temporary Files","text":"<p>By default, Matlab PCT will dump files to <code>~/.matlab/&lt;MATLAB_VERSION&gt;</code>. This causes problems when multiple Matlab PCT jobs are running simultaneously. Users should always define the environment variable <code>MATLAB_PREFDIR</code> so each job uses a unique temporary folder. Files there will be cleaned after the job finishes. For example: <pre><code>export MATLAB_PREFDIR=$(mktemp -d ${SLURM_JOBTMP}/matlab-XXXX)\n</code></pre></p>"},{"location":"software/popular_software/matlab/#matlab-and-slurm-resource-requests","title":"Matlab and Slurm Resource Requests","text":"<p>If you are trying to run Matlab in parallel interactively, you may encounter the following error:</p> <pre><code>&gt;&gt; Starting parallel pool (parpool) using the 'local' profile ...\nError using parpool (line 149)\nYou requested a minimum of &lt;n&gt; workers, but the cluster \"local\" has the NumWorkers property set to allow a maximum of 1 workers. To run a communicating job on more workers than this\n(up to a maximum of 512 for the Local cluster), increase the value of the NumWorkers property for the cluster. The default value of NumWorkers for a Local cluster is the number of\nphysical cores on the local machine.\n</code></pre> <p>This is caused by an interaction between Slurm and Matlab. To resolve this issue, when requesting <code>&lt;n&gt;</code> cores for your interactive job, you will need to set Slurm's <code>--ntasks</code> directive to 1 and <code>--cpus-per-task</code> to the number of cores you need. For example:</p> <pre><code>$ salloc --nodes=1 --ntasks=1 --cpus-per-task=6 --mem-per-cpu=5GB --time=01:00:00 --job-name=interactive --account=&lt;GROUP&gt; --partition=standard\n</code></pre>"},{"location":"software/popular_software/matlab/#external-resources","title":"External Resources","text":"<p>If you are getting started with Matlab or think there might be a better way, check out the training resources.</p> Resource Link Self-Paced Online Courses https://matlabacademy.mathworks.com/ Matlab Parallel Server https://www.mathworks.com/products/matlab-parallel-server.html#resources Natural Language Processing https://www.mathworks.com/discovery/natural-language-processing.html Matlab Videos https://www.mathworks.com/videos.html"},{"location":"software/popular_software/perl/","title":"Perl","text":""},{"location":"software/popular_software/perl/#accessibility","title":"Accessibility","text":"<p>Perl is installed on the operating system of each compute node: </p> <pre><code>[netid@compute_hostname ~]$ perl --version\n\nThis is perl 5, version 16, subversion 3 (v5.16.3) built for x86_64-linux-thread-multi\n(with 44 registered patches, see perl -V for more detail)\n...\n</code></pre>"},{"location":"software/popular_software/perl/#perl-module-policy","title":"Perl Module Policy","text":"<p>We provide a version of Perl through modules or the operating system. Installation of additional user libraries can be done in a Perl environment using <code>perl-virtualenv</code>.</p> <p>For a helpful Perl tutorial, see: http://www.tutorialspoint.com/perl/perl_modules.htm. Additionally, O'Reilly Media is a well regarded source for Perl </p>"},{"location":"software/popular_software/perl/#installing-perl-packages-using-perl-virtualenv","title":"Installing Perl Packages Using perl-virtualenv","text":"<p>One of the best things about Perl is the number of packages provided by the user community. Installing packages generally requires root access but that is not a viable solution in the HPC environment.</p> <p>An easy solution is to use <code>perl-virtualenv</code> on a compute node to create a consistent personal Perl environment. An example of usage:</p> <pre><code>[netid@i0n1 ~]$ perl-virtualenv my_project    # Create virtual environment\nperl path: /usr/bin/perl\nvenv path: /home/uxx/netid/my_project\n[netid@i0n1 ~]$ source my_project/bin/activate # Activate virtual environment\n(my_project)[netid@i0n1 ~]$ cpanm -i Config::Trivial\n--&gt; Working on Config::Trivial\nFetching http://www.cpan.org/authors/id/A/AT/ATRICKETT/Config-Trivial-0.81.tar.gz ... OK\nConfiguring Config-Trivial-0.81 ... OK\n...\n4 distributions installed\n(my_project)[netid@i0n1 ~]$\n</code></pre> <p>Once your environment is created, it can be activated for any future jobs or interactive sessions using the <code>source &lt;/path/to/environment&gt;</code> command.</p>"},{"location":"software/popular_software/python/","title":"Python Modules","text":""},{"location":"software/popular_software/python/#overview","title":"Overview","text":"<p>Different versions of Python are available on HPC both as system modules as well as system software on each compute node.</p>"},{"location":"software/popular_software/python/#installation-and-package-policy","title":"Installation and Package Policy","text":"<p>We maintain a two tiered approach to Python packages:</p> <ul> <li> <p>Tier 1: We install the basic Python packages that are required by most users (these are mostly libraries rather than packages, such as numpy and scipy). This is done for the versions of Python that we install as modules. </p> </li> <li> <p>Tier 2: For packages that we do not provide, or updates to the versions we do, we STRONGLY recommend the use of virtual environments, which is detailed below and provides a custom and easy to use personal Python environment.</p> </li> </ul>"},{"location":"software/popular_software/python/#available-python-versions","title":"Available Python Versions","text":"<p>Python 2 is no longer officially supported by the Python Software Foundation.</p> <p>Using the <code>python</code> command</p> <p>The command <code>python</code> defaults to the system 2.7.5 version. To use Python 3, use the command <code>python3</code>.</p> <p>Multiple versions of Python are available on HPC. They are only available on compute nodes and are accessible either using a batch submission or interactive session. </p> Version Accessibility Python 2.7.5 system version (no module) Python 3.6.8 system version (no module) Python 3.6.5 module load python/3.6/3.6.5 Python 3.8.2 module load python/3.8/3.8.2 Python 3.9.10 module load python/3.9/3.9.10 Python 3.11.4 module load python/3.11/3.11.4"},{"location":"software/popular_software/python/#installing-python-packages-using-a-virtual-environment","title":"Installing Python Packages Using a Virtual Environment","text":"Virtual environment tips <ul> <li>Useful overview of virtualenv and venv: InfoWorld Article: Python virtualenv and venv do's and don'ts</li> <li>In the following instructions any module commands have to be run from an interactive session on a compute node</li> </ul> <p>One of the best things about Python is the number of packages provided by the user community. On a personal machine, the most popular method today for managing these packages is the use of a package manager, like <code>pip</code>. Unfortunately, these may require root access preventing you from being able to successfully install the libraries you need.</p> <p>There is an easy solution, however. You can use a virtual environment to create a personal Python environment that will persist each time you log in. There is no risk of packages being updated under you for another user and allows greater control over your environment.</p> <p></p> <ol> <li> <p>Set up your virtual environment in your account. This step is done one time only and will be good for all future uses of your Python environment. You will need to be in an interactive session to follow along. </p> <p>Note: In the commands below, <code>&lt;/path/to/virtual/env&gt;</code> is the path to the directory where all of your environment's executables and packages will be saved. For example, if you use the path <code>~/mypyenv</code>, this will create a directory in your home called <code>mypyenv</code>. Inside will be directories <code>bin</code>, <code>lib</code>, <code>lib64</code>, and <code>include</code>. </p> Python Version \\(&lt;\\) 3.8Python Version \\(\\geq\\) 3.8 <pre><code>module load python/&lt;version&gt;\nvirtualenv --system-site-packages &lt;/path/to/virtual/env&gt;\n</code></pre> <pre><code>module load python/&lt;version&gt;\npython3 -m venv --system-site-packages &lt;/path/to/virtual/env&gt;\n</code></pre> </li> <li> <p>To use your new environment, you'll need to activate it. Inside your virtual environment, there's a directory called <code>bin</code> that has a file called <code>activate</code>. Sourcing this will add all of the paths needed to your working environment. To activate, run the following, replacing <code>&lt;/path/to/virtual/env&gt;</code> with the path specific to your account:</p> <p><pre><code>source &lt;/path/to/virtual/env&gt;/bin/activate\n</code></pre> 3. Once your environment is active, you can use <code>pip</code> to install your Python packages. You should first upgrade to the latest version of pip. For example, to add the pycurl package to the virtual environment:</p> <p><pre><code>pip install --upgrade pip\npip install pycurl\n</code></pre> 4. That's it! As long as your virtual environment is active, you will have access to the packages you have installed. Virtual environments deactivate when you log out, so for each subsequent session or in batch jobs, you will just need to reactivate the environment to get access to your packages: <pre><code>module load python/&lt;version&gt;\nsource &lt;/path/to/virtual/env&gt;/bin/activate\n</code></pre> Note: 1. Always use the same <code>&lt;version&gt;</code> as the one you used to create your environment 2. Use the <code>module load</code> command before running your <code>source</code> command. If you activate your environment first, you will get a library error. </p> </li> </ol>"},{"location":"software/popular_software/python/#custom-jupyter-kernel","title":"Custom Jupyter Kernel","text":"<p>Warning</p> <p>The default version of Python available in an OnDemand Jupyter Notebook is 3.8.2. If you would like to create a virtual environment using a standard Python module, you will need to use Python version 3.8.2. If you want to use a different version of python, you can use Mamba.</p> <p>If you want to make one of your virtual environments available for use in one of our Open OnDemand Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment (if you do not have an environment, refer to the sections above on how to do so):</p> <pre><code>module load python/3.8/3.8.2                     \nsource &lt;/path/to/your/virtual/environment&gt;/bin/activate\n</code></pre> <p>Once your environment is ready to go, pip-install <code>jupyter</code> and create your own custom kernel. The <code>--force-reinstall</code> flag will allow you to install the <code>jupyter</code> package in your local environment and will not affect the system version. This will create the directory <code>~/.local/share/jupyter/kernels/</code> in your account. In the following commands, replace <code>&lt;your_environment&gt;</code> with the name of your own environment: </p> <pre><code>pip install jupyter --force-reinstall\nipython kernel install --name &lt;your_environment&gt; --user \n</code></pre> <p>Once you've successfully created your kernel, go to Open OnDemand and start a Jupyter Notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your custom kernel's name. For example, if the custom kernel's name was <code>py38-env</code>:</p> <p></p> <p>Once you've selected your environment, try loading a custom package you've installed to check that everything is working as expected. In this example, we'll check with the non-standard package <code>emoji</code> which has been installed in this environment:</p> <p></p>"},{"location":"software/popular_software/python/#loading-modules-in-jupyter","title":"Loading Modules in Jupyter","text":"<p>In OnDemand Jupyter sessions, accessing HPC software modules directly from within a notebook can be challenging due to system configurations. However, it's still possible to access these modules when needed. For instance, machine learning packages like TensorFlow or PyTorch often require additional software modules such as CUDA for GPU utilization.</p> <p>To access software modules in your Jupyter notebooks, follow the steps below:</p> <p>Step 1: If you haven't already done so, create a custom kernel for your Jupyter notebook environment.</p> <p>Step 2: You will then need to edit your kernel configuration file <code>kernel.json</code> which is what sets up your environment at runtime. This file can be found in the following location, where <code>&lt;kernel_name&gt;</code> is a placeholder for the name you gave your kernel when it was created:</p> <pre><code>$HOME/.local/share/jupyter/kernels/&lt;kernel_name&gt;/kernel.json\n</code></pre> <p>Step 3: Next, you will need to modify your kernel's configuration by editing this file. Start by opening it with a text editor, for example <code>nano $HOME/.local/share/jupyter/kernels/&lt;kernel_name&gt;/kernel.json</code>. The contents of this file should look something like the following:</p> <pre><code>{\n \"argv\": [\n  \"&lt;/path/to/your/environment&gt;/bin/python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"&lt;kernel_name&gt;\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\n</code></pre> <p>The part you need to change is the section under <code>argv</code>. We will change this from executing a Python command to a Bash command with a module load statement. Make a note of the path <code>&lt;/path/to/your/environment&gt;/bin/python</code> to use in the edited file. The edited file should look like the following:</p> <pre><code>{\n \"argv\": [\n \"bash\",\n \"-c\",\n \"module load &lt;your_modules_here&gt; ; &lt;/path/to/your/environment&gt;/bin/python -m ipykernel_launcher -f {connection_file}\"\n ],\n \"display_name\": \"&lt;kernel_name&gt;\",\n \"language\": \"python\",\n \"metadata\": {\n \"debugger\": true\n }\n}\n</code></pre> <p>Replace <code>&lt;your_modules_here&gt;</code> with the modules you would like to load and <code>&lt;/path/to/your/environment&gt;/bin/python</code> with the path to your environment's python. </p> <p>Step 4: Save the <code>kernel.json</code> file and restart your Jupyter notebook session.</p>"},{"location":"software/popular_software/vscode_remote_connection/","title":"VSCode Remote Connection","text":""},{"location":"software/popular_software/vscode_remote_connection/#overview","title":"Overview","text":"<p>Visual Studio Code (VS Code) can be used to edit source code and other files on the HPC systems.  VS Code is available to run directly on HPC through the Open OnDemand system. </p> <p>VS Code can also be run locally on laptop or desktop computers and used to make a remote connection to the HPC systems. This documentation is intended to detail the steps that must be taken to allow such a connection. For more detailed information on establishing SSH connections, refer to the VS Code documentation here: https://code.visualstudio.com/docs/remote/ssh-tutorial. There is also a general example shown in the section Connection Example below.</p>"},{"location":"software/popular_software/vscode_remote_connection/#general-method","title":"General Method","text":"<p>Remote VSCode sessions should connect to a compute node. The reasons for this are:</p> <ol> <li> <p>Connecting to the bastion host (hostname: <code>hpc.arizona.edu</code>) will generate files that may overfill your 10 MB quota. This can cause unpredictable connection issues for any future SSH sessions. Additionally, the bastion host is not connected to the shared storage array which means your HPC files will not be accessible.</p> </li> <li> <p>Connecting directly to the filexfer nodes is possible and will allow you to edit files. However, running applications for testing or debugging is not permitted on these machines, so anything beyond editing will need to be done with dedicated compute resources. </p> </li> </ol> <p>Briefly, the general procedure is as follows:</p> <pre><code>graph LR\nsubgraph Only needed once\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    A[Set up SSH keys&lt;br&gt;on the filexfer nodes.]\nend\nA -.-&gt; B[Connect to the HPC VPN]\nB --&gt; C[Request resources&lt;br&gt;on a compute node]\nC --&gt; D[Connect VSCode to the&lt;br&gt;allocated compute node]</code></pre>"},{"location":"software/popular_software/vscode_remote_connection/#specifics-with-example","title":"Specifics with Example","text":"<p>Step 1: Set up SSH key authentication</p> <p>This step needs to be performed one time only. Once you've set up SSH keys, they will persist in your environment for subsequent sessions. The hostname for the file transfer nodes is <code>filexfer.hpc.arizona.edu</code>. Once this process is complete, it will allow VS Code to directly connect to the HPC systems without using passwords or Duo authentication (which may cause connection issues). </p> <p>We have detailed documentation for setting up SSH keys on the bastion host here: SSH Keys. Follow the procedure documented on that page, but replace <code>hpc.arizona.edu</code> in any commands with <code>filexfer.hpc.arizona.edu</code>.</p> <p>Step 2: Connect to the HPC VPN (<code>vpn.hpc.arizona.edu</code>)</p> <p>Connect to the HPC VPN, preferably with Cisco AnyConnect. For detailed information on connecting to the HPC VPN, see VPN - Virtual Private Network.</p> <p>Note that The HPC VPN is needed to connect directly to a compute node. This differs from the standard UArizona VPN or campus network which are not sufficient. The HPC VPN is <code>vpn.hpc.arizona.edu</code>.</p> <p>Step 3: Start an HPC job for the length of time that you\u2019d like to connect VS Code</p> <p>This can either be done directly on the command line, or you can start an Open OnDemand graphical job so you don't have to worry about timing out due to inactivity</p> <p>Note that starting a job either on Ocelote or ElGato will likely get you though the queue faster. As an example, from the command line we could request an eight-hour session with:</p> Interactive session on the command line<pre><code>(ocelote) [netid@wentletrap ~]$ interactive -a hpcteam -n 4 -t 8:00:00\nRun \"interactive -h for help customizing interactive use\"\nSubmitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1    --ntasks=4 --time=8:00:00 --account=hpcteam --partition=standard\nsalloc: Pending job allocation 3293757\nsalloc: job 3293757 queued and waiting for resources\nsalloc: job 3293757 has been allocated resources\nsalloc: Granted job allocation 3293757\n[netid@i16n10 ~]$ hostname\ni16n10.ocelote.hpc.arizona.edu\n</code></pre> <p>After the interactive session starts, type <code>hostname</code>, which will give something like <code>i16n10.ocelote.hpc.arizona.edu</code> (shown highlighted in the code above). This is the name that you will enter in your local VS Code as the remote computer to connect to.</p> <p>Different jobs are allocated different nodes</p> <p>Note that each time you start an interactive session you will likely get a different node, and will therefore need to tell VS Code the specific host to connect to for each unique connection.</p> <p>Step 4: Open a remote connection in VS Code</p> <p>Once your session is running and you have the hostname of your compute node, you can connect VS Code directly to that machine. </p> <p>As an example, open a new window in VS Code on your local computer and select <code>&gt;&lt;</code> from the bottom-left side</p> <p></p> <p>Select Connect to Host..., </p> <p></p> <p>Select +Add New SSH Host...</p> <p></p> <p>Then enter the ssh connection information (replacing <code>netid</code> with your own NetID). In this case, the command with the specific hostname would be <code>ssh netid@i16n10.ocelote.hpc.arizona.edu</code>. </p> <p></p> <p>You will be prompted to select the configuration file to update, this will typically be something like <code>~/.ssh/config</code>. </p> <p></p> <p>If prompted, allow the connection by selecting Allow in the next window that pops up. In the bottom right, you should now see a tile with a button prompting you to connect. </p> <p></p> <p>If a connection has been established, you should now be able to select Open Folder, then can enter the full path to your desired working directory. This may be under your <code>/home</code>, <code>/groups</code>, or <code>/xdisk</code>.</p> <p></p> <p>If everything has gone well, you should now be able to work with your files</p> <p></p>"},{"location":"software/user_installations/","title":"User Installations","text":""},{"location":"software/user_installations/#package-managers","title":"Package managers","text":"<p>Many popular programs, in particular Python and R have built-in package managers that can be used to collect new software from the internet and easily install them to your environment. See the section corresponding to your preferred language on the left under the \"Popular Software\" heading for more detailed instructions.</p>"},{"location":"software/user_installations/#manual-installations","title":"Manual installations","text":"<p>You are encouraged to download and compile(1) any desired software in your own account. Commands like <code>git clone</code> and <code>wget</code> are available to pull repositories from the internet. The installation process may involve changing install locations from system folders (e.g. <code>/usr/bin/xyz</code>) to user folders (e.g. <code>/home/ux/netid/</code> or <code>/groups/pi_netid/netid/</code>), and will vary substantially from software to software, so providing instructions for all cases is beyond the scope of this page. If you encounter difficulties while installing your own software, the HPC Consult team is available for assistance. </p> <ol> <li>on a compute node!</li> </ol> <p>While you cannot add or update system software or libraries using tools that require root privileges such as <code>yum</code>, many software packages can be installed locally without needing to be a superuser. Frequently linux packages make use of the \"<code>configure</code>, <code>make</code>, <code>make install</code>\" method which allows you to customize your installation location. An example of how to do this is shown below. </p> <p>Where to compile</p> <p>Software is not available on the login nodes. To install custom software, log into an interactive session.</p> <p>Installation destination</p> <p>For a typical Linux installation, the default settings may attempt to install files in system locations. This is not permitted, so the installation process (specifically, the <code>./configure</code> step) needs to be changed so that files are installed somewhere you have write access.  There is frequently done with the syntax <code>--prefix=/path/to/software</code>.</p> <p>Compiling on different clusters</p> <p>Be sure to note which cluster you are compiling your software on. El Gato and Ocelote run the operating system CentOS 7 and share the same system libraries and modules. Puma runs Rocky Linux 9 and has its own newer system libraries and modules. Compiling software on one operating system and trying to use it on another may result in failures.</p> configure/make/make install examplecmake <p>Here is a typical example of installing software on a Linux cluster with the <code>configure</code>, <code>make</code>, <code>make install</code> method. We'll use a simple hello world example downloaded from https://ftp.gnu.org/gnu/hello/</p> <ol> <li> <p>Download and unpack the software</p> <pre><code>[user@cpu39 make_example]$ wget https://ftp.gnu.org/gnu/hello/hello-2.10.tar.gz\n[user@cpu39 make_example]$ tar xzvf hello-2.10.tar.gz \n[user@cpu39 make_example]$ cd hello-2.10\n</code></pre> </li> <li> <p>Configure your software</p> <p>The <code>./configure</code> command generates a Makefile tailored to the specific environment and requirements of the system where the software is being installed. The use of the <code>--prefix</code> option allows users to install the software to a custom directory, circumventing the standard root locations which users do not have permission to modify. In this example, we'll install the software to a directory called hello_world in our home. </p> <pre><code>[user@cpu39 hello-2.10]$ ./configure --prefix=$HOME/hello_install\n</code></pre> </li> <li> <p>Compile and install your software</p> <p>Tip</p> <p>Often, there is an additional option to test your software after compiling it and before installing it. Usually, this is something like <code>make test</code> or <code>make check</code></p> <p>The <code>make</code> command compiles the software according to the instructions provided in the Makefile generated by <code>./configure</code>. Once the software is compiled, <code>make install</code> will install the software in the directory you specified with the <code>--prefix</code> option.</p> <pre><code>[user@cpu39 hello-2.10]$ make\n[user@cpu39 hello-2.10]$ make install\n[user@cpu39 hello-2.10]$ ls $HOME/hello_install \nbin  share\n</code></pre> </li> <li> <p>Modify your environment</p> <p>Different environment variables control where the system looks for executables, libraries, header files, etc. Modifying your environment variables will allow you to use your new software without specifying the full paths. These variables can either be set manually on the command line for each new session, or can be added to your bashrc to make the changes permanent. For more information, see the sections Environment Variables and Hidden Files and Directories in our Bash cheat sheet.</p> <pre><code>[user@cpu39 hello-2.10]$ export PATH=$HOME/hello_install/bin:$PATH\n</code></pre> </li> <li> <p>Use your software</p> <pre><code>[user@cpu39 hello-2.10]$ hello\nHello, world!\n</code></pre> </li> </ol> <p>Here is a typical example of installing software on a Linux cluster with the <code>cmake</code> method. We'll use the software library Eigen as an example. </p> <ol> <li> <p>Download the software</p> <p>In this example, we'll <code>git clone</code> the source code from the Gitlab repository https://gitlab.com/libeigen/eigen</p> <pre><code>[user@cpu39 cmake_example]$ git clone https://gitlab.com/libeigen/eigen.git\n[user@cpu39 cmake_example]$ cd eigen/\n</code></pre> </li> <li> <p>Create and set your build environment</p> <p>Next, create a subdirectory called build where build files will be generated and stored. </p> <pre><code>[user@cpu39 eigen]$ mkdir build\n[user@cpu39 eigen]$ cd build\n</code></pre> <p>Additionally, you'll often need to set some environment variables to point to compilers and libraries. In this case, we'll set <code>CC</code> (which sets your C compiler), <code>CXX</code> (which sets your C++ compiler), and <code>FC</code> (which sets your Fortran compiler).</p> <pre><code>[user@cpu39 build]$ export CC=$(which gcc)\n[user@cpu39 build]$ export CXX=$(which g++)\n[user@cpu39 build]$ export FC=$(which gfortran)\n</code></pre> <p>Tip</p> <p>The command <code>which</code> returns a full filepath to an executable. Running something like <code>export FOO=$(which foo)</code> will take the output of <code>which foo</code> and store it as the environment variable <code>FOO</code></p> </li> <li> <p>Configure your build</p> <p>Use <code>cmake</code> to configure your build. Use <code>-DCMAKE_INSTALL_PREFIX</code> to set a custom installation directory that you have access to. This will prevent the installation process from trying to access default root-owned locations which users don't have permission to modify. The <code>..</code> is a shortcut for the directory one level above which is where the CMakeLists.txt file lives.</p> <pre><code>[user@cpu39 build]$ cmake -DCMAKE_INSTALL_PREFIX=$HOME/eigen_install ..\n</code></pre> </li> <li> <p>Compile and install your software</p> <pre><code>[user@cpu39 build]$ make\n[user@cpu39 build]$ make install\n[user@cpu39 build]$ ls $HOME/eigen_install\ninclude  share\n</code></pre> </li> <li> <p>Modify your environment</p> <p>Different environment variables control where the system looks for executables, libraries, header files, etc. Modifying your environment variables will allow you to use your new software without specifying the full paths. These variables can either be set manually on the command line for each new session, or can be added to your bashrc to make the changes permanent. For more information, see the sections Environment Variables and Hidden Files and Directories in our Bash cheat sheet.</p> <p>In this case, we'll set <code>INCLUDE</code> and <code>CPPFLAGS</code></p> <pre><code>[user@cpu39 build]$ export INCLUDE=$HOME/eigen_install:$INSTALL\n[user@cpu39 build]$ export CPPFLAGS=\"-I$HOME/eigen_install $CPPFLAGS\"\n</code></pre> </li> </ol>"},{"location":"storage_and_transfers/storage/hpc_storage/","title":"HPC Storage","text":""},{"location":"storage_and_transfers/storage/hpc_storage/#overview","title":"Overview","text":"<p>The University\u2019s Research Data Center provides data storage for active analysis on the high-performance computers (HPCs). Using central computing storage services and resources, researchers are able to:</p> <ul> <li>Share research data in a collaborative environment with other UArizona affiliates on the HPC system.</li> <li>Store large-scale computational research data.</li> <li>Request additional storage for further data analysis.</li> </ul> <p>All clusters share access to the same mounted HPC storage, so your files are available regardless of which cluster you\u2019re using.</p> <p>Every user has access to individual and shared storage on the system where they can host data for active analyses. A summary of these locations is shown below:</p> Path Description Quota Duration <code>/home/uxx/netid</code> An individual storage allocation provided for every HPC user 50 GB Accessible for the duration of user's account <code>/groups/pi_netid</code> A communal storage allocation provided for every research group 500 GB Accessible for the duration of a PI's account <code>/xdisk/pi_netid</code> Temporary communal storage available for every group on request. See xdisk section below for details. 200 GB to 20  TB Up to 300 days <code>/tmp</code> Local storage available on individual compute nodes. \\(&lt;\\) 800 GB to 1.4 TB Only accessible for the duration of a job's run <p>Managing permissions</p> <p>If you're working with other members of your group and need to make your files more accessible, see our bash cheat sheet. This offers an overview on Linux file permissions, ownership, and some tips for working with collaborators. </p>"},{"location":"storage_and_transfers/storage/hpc_storage/#best-practices","title":"Best Practices","text":"<p>The shared file system on HPC is the location for everything in <code>/home</code>, <code>/groups</code>, and <code>/xdisk</code>. The <code>/tmp</code> directory is also available to users, and refers to the local disk on each node. Your I/O activity can have dramatic activity on other users. Extreme read/write activity can cause bottlenecks and may be cancelled without warning. It is generally best to limit I/O whenever possible to avoid straining the system. The HPC consult team is available to help optimize workflows that may be impacted by I/O. </p> <ul> <li> <p> Be aware of I/O load.</p> <p>Running multiple instances of jobs performing significant I/O activity may be detrimental to the system, especially if these occur within the same subdirectories. It may be best to read in data at the beginning of a workflow, perform the entire analysis, then write at the very end. Reconfiguring your workflow to limit I/O may cost some time up front, but will most likely be made back through faster job completion. </p> <p>If you are running array jobs, please be cognizant of your I/O activity.</p> </li> <li> <p> Use /tmp for working space</p> <p>If you have multiple jobs that will use the same data, consider copying it to <code>/tmp</code> and run multiple jobs. This can increase performance and reduce I/O load.</p> </li> <li> <p> Avoid storing many files in a single directory</p> <p>Hundreds of files is probably ok; tens of thousands is not.    </p> </li> <li> <p> Avoid opening and closing files repeatedly in tight loops</p> <p>If possible, open files once at the beginning of your workflow/program, then close them at the end.</p> </li> <li> <p> Watch your quotas</p> <p>You are limited in capacity and exceeding your storage quotas may have unintended side effects (e.g., login issues, data loss, or failed jobs). See the section below on checking your storage usage.</p> </li> <li> <p> Avoid frequent snapshot files</p> <p>This can stress the storage.</p> </li> <li> <p> Use parallel I/O</p> <p>Some modules enable parallelized file operations, such as <code>phdf5</code>.</p> </li> </ul>"},{"location":"storage_and_transfers/storage/hpc_storage/#checking-your-storage-usage","title":"Checking Your Storage Usage","text":"Command LineUser Portal <p>To check your storage usage, on a compute node, file transfer node, or login node, use the command <code>uquota</code>. This will show you all the spaces you have access to, their quotas, and current usage. <pre><code>(puma) [netid@junonia ~]$ uquota\n                                        used  soft limit  hard limit\n/groups/pi_netid                            6.6G      500.0G      500.0G\n/home                                      37.1G       50.0G       50.0G\n/xdisk/pi_netid                            12.9G        9.8T        9.8T\n</code></pre></p> <p>You can check your storage allocation through our online user portal by navigating to the Storage tab and clicking Check Disk Quotas:</p> <p></p>"},{"location":"storage_and_transfers/storage/hpc_storage/#xdisk","title":"xdisk","text":""},{"location":"storage_and_transfers/storage/hpc_storage/#what-is-xdisk","title":"What is xdisk?","text":"<p>xdisk is a temporary storage allocation available to all faculty members (PIs) and offers up to 20 TB of usable space for their group for up to 300 days. PIs may only have one active xdisk at a time.</p> <p>A PI can request an allocation either via the command line or through our web portal (no paperwork necessary!). Only PIs may request, alter, or delete an allocation from the command line. However, members of their research group may be delegated management rights allowing them to manage a group's xdisk on their PI's behalf through our web portal.</p> <p>Once an xdisk allocation is created, it is immediately available for use. Groups can find their allocations under <code>/xdisk/pi_netid</code>. By default, a subdirectory is created for each group member under <code>/xdisk/pi_netid/netid</code>. If a group member is added after the allocation is created, a directory is not automatically created for them. To add one, reach out to our consultants.</p> <p>Because xdisk allocations are temporary, they will be removed as soon as their time limit is reached. Warnings will be sent to every group member at their netid@arizona.edu addresses beginning one month before the expiration. It is the group's responsibility to renew xdisk allocations or copy files to an alternate storage location prior to the expiration date. Once an xdisk allocation expires, the contents are deleted. </p> <p>PIs may request a new xdisk allocation immediately after their previous one has expired. This ensures groups will always have access to increased storage on HPC on a rolling basis with the requirement that housekeeping be done once per academic year. </p>"},{"location":"storage_and_transfers/storage/hpc_storage/#requesting-modifying-and-deleting-an-allocation","title":"Requesting, Modifying, and Deleting an Allocation","text":"<p>XDisk management is limited to PIs and delegates</p> <p>Only PIs and trusted delegates can create, delete, and modify xdisk allocations. The CLI interface is restricted to PIs only. Delegates may manage their groups allocation through the user portal. For more information on adding group delegates and how they can use the portal on their PI's behalf, see: Delegating Group Management Rights. </p>"},{"location":"storage_and_transfers/storage/hpc_storage/#user-portal","title":"User Portal","text":"Requesting an AllocationModifying an AllocationDeleting an Allocation <p>Warning</p> <p>If a group has an active xdisk allocation, a new one cannot be created until the active allocation expires or is deleted.</p> <p>PIs or delegates can request an xdisk allocation at any time through the user portal. Under the Storage tab, select Create XDisk</p> <p></p> <p>This will open a web form where you can enter your size and duration requirements in GB and days, respectively. The maximum size that can be requested is 20000 GB and the maximum duration is 300 days. In addition, specify the desired group ownership for the allocation from the Group dropdown menu This will determine file permissions and who has access. Once you click , your allocation should immediately be available.</p> <p></p> <p>PIs or delegates may manage their xdisk allocation at any time through the user portal. Under the Storage tab, either select Update XDisk Size or Update XDisk Duration, depending on the property you would like to update.</p> <p></p> <p>This will open a form which will allow you to modify the size and duration of your xdisk. Xdisk allocations cannot be increased beyond 20000 GB and the maximum duration of 300 days. Note: the Group field may only be modified at the time of the allocation's creation.</p> <p>PIs or delegates may delete their xdisk allocation at any time through the user portal. Under the Storage tab, select Delete XDisk</p> <p></p> <p>Clicking this link will open a window with a prompt. Type confirm and then select  to complete the process.</p> <p></p> <p>If you would like to request a new xdisk, you may do so as soon as the request is processed. Note: sometimes processing the request can take a few minutes, depending on the number of files and the size of the allocation.</p>"},{"location":"storage_and_transfers/storage/hpc_storage/#cli-commands-pis-only","title":"CLI Commands (PIs only)","text":"<p>Warning</p> <p>The xdisk CLI commands are usable by PIs only. Group delegates can manage allocations via the user portal after switching to their PI's account.</p> <p><code>xdisk</code> is a locally written utility for PI's to create, delete, resize, and extend xdisk allocations. Any PIs who wish to utilize the CLI to manage their allocations can do so using the syntax shown below:</p> xdisk Function Command Examples Display xdisk help <pre><code>xdisk -c help</code></pre> <pre><code>$ xdisk -c help</code></pre> View Current Information <pre><code>xdisk -c query</code></pre> <pre><code>$ xdisk -c queryXDISK on host: ericidle.hpc.arizona.eduCurrent xdisk allocation for &lt;pi_netid&gt;:Disk location: /xdisk/&lt;pi_netid&gt;Allocated size: 200GBCreation date: 3/10/2020 Expiration date: 6/8/2020Max days: 45    Max size: 1000GB</code></pre> Create an xdisk <pre><code>xdisk -c create -m [size in gb] -d [days]</code></pre> <pre><code>$ xdisk -c create -m 300 -d 30Your create request of 300 GB for 30 days was successful.Your space is in /xdisk/&lt;pi_netid&gt;</code></pre> Extend xdisk Expiration Date <pre><code>xdisk -c expire -d [days]</code></pre> <pre><code>$ xdisk -c expire -d 15Your extension of 15 days was successfully processed</code></pre> Resize an xdisk Allocation <pre><code>xdisk -c size -m [size in gb]</code></pre> <pre><code>$ # Assuming an initial xdisk allocation size of 200 gb$ xdisk -c size -m 200XDISK on host: ericidle.hpc.arizona.eduYour resize to 400GB was successful$ xdisk -c size -m -100XDISK on host: ericidle.hpc.arizona.eduYour resize to 300GB was successful</code></pre> Delete an xdisk Allocation <pre><code>xdisk -c delete</code></pre> <pre><code>$ xdisk -c delete`Your delete request has been processed</code></pre>"},{"location":"storage_and_transfers/storage/overview/","title":"Overview","text":""},{"location":"storage_and_transfers/storage/overview/#storage-overview","title":"Storage Overview","text":""},{"location":"storage_and_transfers/storage/overview/#where-should-i-store-my-data","title":"Where Should I Store My Data?","text":"<ol> <li>Data undergoing active analyses should be stored in HPC's local High Performance Storage.</li> <li>Large amounts of data not requiring immediate access from our HPC compute nodes can be stored at reasonable rates on our Rental Storage. </li> <li>RDAS is a research data service which supports the mounting of SMB shares. The supported operating systems are MacOS, Linux, and Windows. It provides 5 TB of free storage. </li> <li>Research data not requiring immediate access should be stored in General Research Data Storage (Tier 2). For example:<ol> <li>Large datasets where only subsets are actively being analyzed.</li> <li>Results no longer requiring immediate access.</li> <li>Backups (highly encouraged!).</li> </ol> </li> <li>Data that require HIPAA-compliance can be stored on Soteria (currently in the pilot phase).</li> </ol> <p> <pre><code>graph LR\n  A[My data are...] --&gt; B{Controlled?}\n  B--&gt;|Yes| C{HIPAA?};\n  C--&gt;|Yes| D[&lt;a href=\"../../../resources/secure_hpc/\"&gt;Soteria&lt;/a&gt;];\n  C--&gt;|No| E[Unsupported];\n  B--&gt;|No| F{Archival?}\n  F--&gt;|Yes| G[&lt;a href=\"../../storage/tier2_storage/\"&gt;AWS Tier 2&lt;br&gt;Storage&lt;/a&gt;]\n  F--&gt;|No| H{Need&lt;br&gt;HPC&lt;br&gt;compute?}\n  H--&gt;|Yes| I{Under&lt;br&gt;20 TB?}\n  I--&gt;|Yes| J[&lt;a href=\"../../storage/hpc_storage/\"&gt;HPC Storage&lt;/a&gt;]\n  I--&gt;|No| K[&lt;a href=\"../../storage/rental_storage/\"&gt;Rental Storage&lt;/a&gt;]\n  H--&gt;|No| L{Under&lt;br&gt;5 TB?}\n  L--&gt;|No| K\n  L--&gt;|Yes| M[&lt;a href=\"../../storage/rdas_storage/\"&gt;R-DAS Storage&lt;/a&gt;]</code></pre> </p>"},{"location":"storage_and_transfers/storage/overview/#storage-option-summary","title":"Storage Option Summary","text":"Purpose Capacity Cost Restricted Data Access Duration Backup Primary HPC Storage Research data. Supports compute. Directly attached to HPC <code>/home</code>: 50 GB<code>/groups</code>: 500 GB<code>/xdisk</code>: 20 TB Free \u274c Directly mounted on HPC. Also uses Globus and DTNs. Long term. Aligns with HPC purchase cycle. No R-DAS Research Desktop Attached Storage - SMB shares 5 TB Free \u274c Mounted to workstations as shares Long term No Rental Storage Research data. Large datasets. Typically for staging to HPC Rented per TB per year $47.35 per TB per year \u274c Uses Globus and DTNs. Copy data to Primary Long term. Aligns with HPC purchase cycle No Tier 2 Typically research data. Unused data is archived 15 GB to TBs Tier-based system. First 1 TB of active data and archival data are free. Active data &gt; 1 TB is paid. \u274c Uses Globus and AWS command line interface Typically long term since use of Glacier is free and slow Archival ReData Research data. Managed by UArizona Libraries Quota system Free \u274c Log in and fill out fields, then upload Longer than 10 years No Soteria HIPAA Secure data enclave Individual requests Free upon qualification Restricted data; HIPAA, ePHI HIPAA training required, followed by request process Long term No Box General Data 50 GB Free \u274c Browser Long term Cloud Google Drive General data 15 GB Free. Google rates for amounts &gt; 15 GB \u274c Browser Unlimited usage expires March 1, 2023 Cloud"},{"location":"storage_and_transfers/storage/overview/#nih-data-management-and-sharing-policy","title":"NIH Data Management and Sharing Policy","text":"<p>The NIH has issued a new data management and sharing policy, effective January 25, 2023. The University Libraries now offers a comprehensive guide for how to navigate these policies and what they mean for you.</p> <p>What's new about the 2023 NIH Data Management and Sharing Policy?   Previously, the NIH only required grants with $500,000 per year or more in direct costs to provide a brief explanation of how and when data resulting from the grant would be shared.   The 2023 policy is entirely new. Beginning in 2023, ALL grant applications or renewals that generate Scientific Data must now include a robust and detailed plan for how you will manage and share data during the entire funded period. This includes information on data storage, access policies/procedures, preservation, metadata standards, distribution approaches, and more. You must provide this information in a data management and sharing plan (DMSP). The DMSP is similar to what other funders call a data management plan (DMP).   The DMSP will be assessed by NIH Program Staff (though peer reviewers will be able to comment on the proposed data management budget). The Institute, Center, or Office (ICO)-approved plan becomes a Term and Condition of the Notice of Award.</p>"},{"location":"storage_and_transfers/storage/rdas_storage/","title":"Research Desktop Attached Storage (R-DAS)","text":""},{"location":"storage_and_transfers/storage/rdas_storage/#overview","title":"Overview","text":"<p>R-DAS not an HPC filesystem</p> <p>R-DAS storage is not mounted on HPC compute or login nodes. Data stored in R-DAS will need to be copied over to the HPC filesystem in order to be accessible to jobs. Instructions on how to access R-DAS from HPC are included below.</p> <p>Group Sharing</p> <p>Faculty members/PIs can share their allocations with group members. To do so, in step 6 in the Accessing Your R-DAS Allocation section below, group members will choose the allocation with their faculty member's/PI's NetID.</p> <p>No Controlled Data</p> <p>This service is not intended for HIPAA or otherwise controlled data. Please see Secure HPC for more information. </p> <p>On October 16, 2023, we went live with the Research Desktop Attached Storage Array (R-DAS). R-DAS provides up to 5 TB of no-cost storage capacity for each PI group. Our requirement was to enable our users to easily share data with other research group members. You can treat the allocation as a drive mounted on your local computer. R-DAS is intended for storing open research data, but not controlled or regulated data.</p>"},{"location":"storage_and_transfers/storage/rdas_storage/#technical-requirements","title":"Technical Requirements","text":"<p>R-DAS is a storage service backed by a Qumulo branded storage array. It supports the mounting of SMB shares for SMB 3.1. The supported operating systems are MacOS (Monterey or higher), Linux (kernel 3.7 or higher), and Windows (Windows 10 or 11).</p>"},{"location":"storage_and_transfers/storage/rdas_storage/#performance","title":"Performance","text":"<p>The storage array is located in the Research Data Center to benefit from the network infrastructure in the Computer Center. The performance you experience will depend on your network connectivity. The best case is likely wired ethernet in a newer building. Off campus usage requires connection to the VPN, and so performance can be variable. Our testing off campus regularly reached 3 MB/s.</p>"},{"location":"storage_and_transfers/storage/rdas_storage/#requesting-an-allocation","title":"Requesting an Allocation","text":"<p>PIs can request an allocation on R-DAS from https://portal.hpc.arizona.edu/portal</p> <ol> <li>Go to the Storage tab</li> <li> <p>Select Create Shared Desktop Storage under Research Desktop Storage</p> <p></p> </li> <li> <p>Select Open Submission Form  from the window that opens. This will open the MOU agreement. Review it and, if it is acceptable to you, select Submit Request . Note: you must scroll to the bottom of the MOU agreement to be able to submit the request. </p> <p></p> </li> <li> <p>You can now select the View Shared Desktop Storage option from the main Storage page in the user portal</p> <p></p> </li> </ol>"},{"location":"storage_and_transfers/storage/rdas_storage/#accessing-your-r-das-allocation","title":"Accessing Your R-DAS Allocation","text":"<p>Tip</p> <p>UArizona IP Address Required: To access your R-DAS allocation you need to be connected to either the UArizona campus network, or the UArizona SSL VPN. For information about connecting to a VPN, see VPN - Virtual Private Network. If you are accessing your R-DAS allocation from an HPC cluster, then you are already on the UArizona campus network and do not need to connect to the UArizona SSL VPN. </p> <p>R-DAS can be accessed from Linux, MacOS, or Windows. The screenshots are intended to be visual aids, but they include information from the consulting team. When you proceed, please enter your own information.</p> <p>Choose your operating system</p> LinuxMac OSWindows <p>No <code>sudo</code> on HPC</p> <p>Do not attempt to run <code>sudo</code> commands on HPC, these are only meant for your personal Linux machines. To transfer data between R-DAS and HPC see Transfer data between R-DAS and HPC storage.</p> <p>First, install the necessary software packages to access your allocation</p> <p>Choose your distribution</p> Debian/UbuntuFedora/CentOSOther Linux Distributions <pre><code>sudo apt install samba gvfs-backends smbclient\n</code></pre> <pre><code>sudo yum install samba gvfs-samba samba-client \n</code></pre> <p>Please check the documentation of your distribution.</p> <p>Next, access your allocation</p> <p>Choose your connection method</p> GUICLI <p>On a desktop environment, such as MATE, GNOME, KDE, you can mount your R-DAS allocation as a local drive with the corresponding file manager (Caja on MATE, GNOME Files, Dolphin on KDE). On HPC, you can use a virtual desktop.</p> <ol> <li> <p>Open the file manager (Caja, GNOME Files, Dolphin)</p> </li> <li> <p>Press Ctrl+L. This makes the location bar editable.</p> </li> <li> <p>Enter <code>smb://rdas.hpc.arizona.edu</code> in the location bar, and press Enter.</p> <p></p> </li> <li> <p>A few moments later a window opens, prompting for your Username (<code>BLUECAT\\</code> followed by your UArizona NetID) and Password (UA NetID password). After entering the details, select Connect (on other file managers this may be OK). Some file managers, such as Caja and GNOME Files, also have a Domain field, whereas others, like Dolphin, do not. Either way, you do not need to modify its default value.</p> <p></p> </li> <li> <p>Select the allocation named after your group from the list of allocations displayed.</p> <p></p> </li> <li> <p>On some file managers, such as Dolphin, you can right away access your allocation by double clicking on it. On others, such as Caja and GNOME Files, double clicking on it will open another window prompting for your Username (<code>BLUECAT\\</code> followed by your UArizona NetID) and Password (UA NetID password). Select Connect as user, enter the details, and select Connect. Your allocation will be mounted as a local drive.</p> <p></p> </li> </ol> <p>You can interactively browse your R-DAS allocation with <code>smbclient</code>: <pre><code>smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\&lt;share&gt; -U BLUECAT\\\\&lt;username&gt;\n</code></pre></p> <p>The <code>&lt;share&gt;</code> is the PI group that you belong to, and <code>&lt;username&gt;</code> is your UArizona NetID. The command will prompt for a password where you will enter your UArizona NetID password. This will start an <code>smb</code> shell. For example:</p> <pre><code>~ $ smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\sohampal -U BLUECAT\\\\sohampal\nPassword for [BLUECAT\\sohampal]:\nTry \"help\" to get a list of possible commands.\nsmb: \\&gt;\n</code></pre> <p>Try <code>help</code> to get a list of possible commands:</p> <pre><code>smb: \\&gt; help\n?              allinfo        altname        archive        backup        \nblocksize      cancel         case_sensitive cd             chmod         \nchown          close          del            deltree        dir           \ndu             echo           exit           get            getfacl  \n. . .\n</code></pre> <p>Use the <code>-L</code> flag to get the list of shares on the Array. For example:</p> <pre><code>smbclient -L \\\\\\\\rdas.hpc.arizona.edu -U BLUECAT\\\\sohampal\nPassword for [BLUECAT\\sohampal]:\n\nSharename       Type      Comment\n---------       ----      -------\nQ$              Disk      Default root share for SRVSVC.\nipc$            IPC       Named Pipes\nupgrade         Disk      for qumulo upgrades\ntmerritt        Disk      Desktop share for tmerritt created on 09/12/2023 12:24 PM\n. . .\n</code></pre> <p>Any command that you can run interactively from the smb shell, you can also run non-interactively with the <code>-c</code> flag. For example, to list the files and directories in your share, run: <pre><code>smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\&lt;share&gt; -U BLUECAT\\\\&lt;username&gt; -c 'ls'\n</code></pre> You can also combine multiple commands with <code>;</code>. For example to list the contents in a directory in your share, run: <pre><code>smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\&lt;share&gt; -U BLUECAT\\\\&lt;username&gt; -c 'cd &lt;directory&gt;;ls'\n</code></pre> To copy a file from your local system to your R-DAS share use <code>put</code>, and from your R-DAS share to your local system use <code>get</code>: <pre><code>smbclient \\\\\\\\rdas.hpc.arizona.edu\\\\&lt;share&gt; -U BLUECAT\\\\&lt;username&gt; -c 'put &lt;file&gt;'\n</code></pre> To learn more about smbclient, run <code>man smbclient</code>.</p> <p>If you are on a Mac, then you can mount your R-DAS allocation as a local drive with the following steps:</p> <ol> <li>Go to Finder</li> <li>Select Go from the top menu bar.</li> <li>From the drop-down menu, select Connect to Server.</li> <li> <p>In the window that opens, enter <code>smb://rdas.hpc.arizona.edu</code> in the address bar, and select Connect.</p> <p></p> </li> <li> <p>After a few moments a window opens prompting for your Name (UA NetID) and Password (UA NetID password). After entering the details, select Connect.</p> <p></p> </li> <li> <p>A window will open with the list of allocations on the array. Select the allocation named after your group, and then select OK.</p> <p></p> </li> </ol> <p>If you are on Windows, you can mount your R-DAS allocation as a local drive with the following steps:</p> <ol> <li>Open Windows Explorer.</li> <li> <p>Enter <code>\\\\rdas.hpc.arizona.edu</code> in the location bar, and press Enter.</p> <p></p> </li> <li> <p>A few moments later a window will open, prompting for your Username (<code>BLUECAT\\</code> followed by your UArizona NetID) and Password (UA NetID password). After entering the details, select OK.</p> <p></p> </li> <li> <p>Select the allocation named after your group from the list of allocations displayed. You can directly open the allocation by double-clicking on it, or mount it by right clicking on it and selecting Map network drive.</p> <p></p> </li> </ol>"},{"location":"storage_and_transfers/storage/rdas_storage/#transfer-data-between-r-das-and-hpc-storage","title":"Transfer data between R-DAS and HPC storage","text":"<p>The simplest way to transfer data between your R-DAS share and HPC storage is to first transfer data to your local machine, and then from local machine to the destination. For more information on transferring data from local machine, see Transfers. However if you do not want to store the data to your local machine as an intermediate step, then you can transfer data between R-DAS and HPC storage with the following steps:</p> <ol> <li>Mount the R-DAS share as a local drive following the steps above.</li> <li>Transfer the data using <code>rsync</code>, see rsync for more information. For example, if your local machine is a Mac, then you can transfer the data from R-DAS to HPC storage with the following:    <pre><code>rsync -ravz /Volumes/&lt;share-name&gt;/&lt;path-to-source&gt; &lt;netid&gt;@filexfer.hpc.arizona.edu:&lt;path-to-destination&gt;\n</code></pre></li> </ol> <p>The above steps assumes that you know the mount point of the R-DAS share on your local machines:</p> <ul> <li>On Linux, it might take some amount of sleuthing to find out where it is mounted. File managers dependent on <code>gvfs</code> will typically mount it under <code>/run/user/&lt;uid&gt;/gvfs</code>.</li> <li>On a Mac, it will typically be mounted at <code>/Volumes/&lt;share-name&gt;</code>.</li> <li>On Window, you will have to map it to a drive.</li> </ul> <p>We recommend that you use <code>rsync</code> to transfer the data from your R-DAS share to HPC storage. However, if you do not know the mount point of the R-DAS share, or if you do not want to use <code>rsync</code>, then the other alternative to transfer data between R-DAS and HPC storage is:</p> <ol> <li>Start an virtual desktop on Open OnDemand. See Virtual Desktop for more information.</li> <li>Mount the R-DAS share following the Linux GUI steps. (1)</li> <li>Transfer the data graphically, or using your favorite command line tool from the virtual desktop terminal. </li> </ol> <ol> <li>On the HPC virtual desktop's MATE desktop environment, you can launch the file manager, Caja, by clicking the file drawer like icon in the top bar, or by selecting Applications &gt; System Tools &gt; Caja.</li> </ol>"},{"location":"storage_and_transfers/storage/rental_storage/","title":"Rental Storage","text":""},{"location":"storage_and_transfers/storage/rental_storage/#overview","title":"Overview","text":"<p>Accessibility</p> <p>Your <code>/rental</code> allocation is only mounted on our Data Transfer Nodes (hostname <code>filexfer.hpc.arizona.edu</code>) and is not directly accessible from the HPC login or compute nodes. </p> <p>No Controlled Data</p> <p>This service is not intended for HIPAA or otherwise controlled data. Please see Secure HPC for more information. </p> <p>We offer a rental storage solution that has less performance than our primary SSD array making it affordable for researchers to rent. This storage array is located in the Research Data Center and is mounted on our data transfer nodes which makes it more accessible than most other options. Data in your rental space will be accessible via the command line and the graphical transfer application Globus. </p>"},{"location":"storage_and_transfers/storage/rental_storage/#pricing","title":"Pricing","text":""},{"location":"storage_and_transfers/storage/rental_storage/#cost-per-year","title":"Cost per Year","text":"<p>The first-year rate is $94.50 per TB, and RII will provide matching funds for first-year allocations to make the actual first-year cost to researchers $47.35. These matching funds will be applied automatically, so in practice you will see the $47.35 rate. The ongoing rate after year one is $47.35 per TB per year.</p>"},{"location":"storage_and_transfers/storage/rental_storage/#billing","title":"Billing","text":"<p>Researchers must provide a KFS account for this service. Charges will be applied at the end of the academic year (June).</p>"},{"location":"storage_and_transfers/storage/rental_storage/#size-modifications","title":"Size Modifications","text":"<p>If the size of your allocation is modified, you will be billed for the maximum amount of space reserved during that fiscal year. </p>"},{"location":"storage_and_transfers/storage/rental_storage/#data-location","title":"Data Location","text":"<p>Danger</p> <p>Your <code>/rental</code> allocation is only mounted on our Data Transfer Nodes and is not directly accessible from the HPC login or compute nodes. </p> <p>Your rental space will be on a storage array in our Research Data Center and mounted on our data transfer nodes (hostname: <code>filexfer.hpc.arizona.edu</code>). Your space will be findable under </p> <pre><code>/rental/&lt;pi_netid&gt;\n</code></pre> <p>Where <code>&lt;pi_netid&gt;</code> is the NetID of the faculty member who requested the allocation.</p>"},{"location":"storage_and_transfers/storage/rental_storage/#data-transfers","title":"Data Transfers","text":"<p>A few data transfer options are Globus, <code>sftp</code>, and <code>scp</code> which will allow you to move data external to the data center to your allocation.</p> <p>For data transfers between HPC storage (<code>/home</code>, <code>/groups</code>, or <code>/xdisk</code>) and your rental allocation, you may also <code>ssh</code> into <code>filexfer.hpc.arizona.edu</code> and use <code>mv</code> or <code>cp</code>. For large copies done using this method, we recommend using a <code>screen</code> session to prevent timeouts. For example: <pre><code>[netid@home ~]$ ssh netid@filexfer.hpc.arizona.edu\nAuthorized uses only. All activity may be monitored and reported.\nLast login: Fri Sep 15 10:53:27 2023\n[netid@sdmz-dtn-3 ~]$ cd /rental/pi/netid/example\n[netid@sdmz-dtn-3 example]$ screen\n[netid@sdmz-dtn-3 example]$ cp -r /xdisk/pi/CONTAINERS/ $PWD/CONTAINERS\n[netid@sdmz-dtn-3 example]$ ls\nCONTAINERS\n[netid@sdmz-dtn-3 example]$ exit # exits screen session\n[netid@sdmz-dtn-3 example]$ exit # exits filexfer node\nlogout\nCome again soon!\nConnection to filexfer.hpc.arizona.edu closed.\n[netid@home ~]$\n</code></pre></p>"},{"location":"storage_and_transfers/storage/rental_storage/#how-to-request-rental-storage","title":"How to Request Rental Storage","text":"<p>Warning</p> <p>Allocations up to 20TB in size can be requested through the user portal. For allocations larger than 20TB, contact our consulting team for help.</p> <p>Tip</p> <p>It can take a few days to process the request as it has to route through the Financial Services Office (FSO). You will receive an email confirmation once it is complete.</p> <ol> <li> <p>PIs or Group Delegates can request rental storage on behalf of their group. To do so, navigate to the User Portal in your browser, choose the Storage tab, then select Submit Rental Storage Request under the Rental Storage heading.</p> <p></p> </li> <li> <p>This will open a web form. Add your KFS number under KFS Number(1) and the email address for the Department's financial contact under Business contact email. There will also be two optional fields: Subaccount and Project. These are used for tagging/reporting purposes in KFS billing. You can safely leave these entries blank if you're not sure what they are. Once you have completed the form, click . </p> <ol> <li>A KFS number is used for accounting purposes and used by your Department's finance specialist. If you do not know your KFS number, contact your department's financial office. </li> </ol> <p></p> </li> <li> <p>Once your space has been created, you will receive an email notification that it is ready for use.</p> </li> </ol>"},{"location":"storage_and_transfers/storage/rental_storage/#resizing-your-allocation","title":"Resizing Your Allocation","text":"<p>Warning</p> <p>Resizing allocations up to 20TB can be done the user portal. For allocations larger than 20TB, contact our consulting team for help.</p> <p>Your rental allocation can be resized through the user portal by navigating to the Storage tab and selecting Modify Rental Quota under the Rental Storage heading.</p> <p></p>"},{"location":"storage_and_transfers/storage/rental_storage/#checking-your-usage","title":"Checking Your Usage","text":"<p>You can check your allocation's size and current usage either through the user portal or on the command line.</p> User PortalCommand Line <p>In the user portal, navigate to the Storage tab and select Check Rental Quota from under the Rental Storage heading. This option is only available to PIs and group delegates.</p> <p></p> <p>From an HPC login node, enter the command <code>uquota</code>, for example: <pre><code>[user@local_machine ~]$$ ssh netid@hpc.arizona.edu\n[netid@gatekeeper ~]$ shell\n(puma) [netid@wentletrap ~]$ uquota\n                                        used  soft limit  hard limit\n/groups/pi                                163.4G      500.0G      500.0G\n/home                                      13.2G       50.0G       50.0G\n/rental/pi                                 11.8G      931.3G      931.3G\n/xdisk/pi                                   9.0T        9.9T        9.9T\n</code></pre></p>"},{"location":"storage_and_transfers/storage/tier2_storage/","title":"Tier 2 AWS Storage","text":""},{"location":"storage_and_transfers/storage/tier2_storage/#overview","title":"Overview","text":"<p>No Controlled Data</p> <p>This service is not intended for HIPAA or otherwise controlled data. Please see Secure HPC for more information. </p> <p>Research Technologies in partnership with UITS has implemented an AWS rental storage solution. This AWS option is called Tier 2 which differs from Tier 1, the primary storage that is directly connected to the HPC clusters. Tier 1 is very fast, very expensive, and immediately available for active analyses. Tier 2 is intended for archival data not immediately undergoing active analyses and for backups (highly encouraged!). Researchers can use the software Globus to move data to Tier 2, and can also move data from other sources (called endpoints). The data in Tier 2 will not be mounted on HPC, and so Globus will be used to move it back to Tier 1 if needed.</p> <p>AWS storage is organized in \"buckets.\" One S3 intelligent tiering bucket is supported per KFS account. A PI could sponsor multiple buckets by submitting separate requests each with a unique KFS number, and then provide permissions as they see fit. </p>"},{"location":"storage_and_transfers/storage/tier2_storage/#data-lifecycle","title":"Data Lifecycle","text":"<p>Avoid large numbers of files</p> <p>Because AWS is set up for automatic archiving, files are moved into tiers where restore requests need to be submitted for each individual file that needs to be downloaded after a period of time. We strongly recommend archiving directories (.zip, .tar.gz files, etc) prior to moving them to AWS. This will significantly speed up your data transfers as well as reduce the complexity of file restorations. If you transfer hundreds or thousands of files to AWS, restore requests may take days or weeks to process. </p> <p>Small files</p> <p>Warning: Very small files (less than 128KB ) are not subject to intelligent tiering and are not migrated to Glacier/Deep Glacier. This means they are permanently stored in the paid storage class. If you have many small files, we recommend making archives of your directories (.tar.gz, .zip, etc) prior to uploading them to AWS. This will also reduce transfer times significantly. </p> <p>Tier 2 AWS buckets use intelligent tiering to determine the archival status of files. When data are first uploaded to a group's bucket, they are in the standard access class. This essentially means they are stored on higher performant storage and are available for immediate download. After three months of inactivity(1), data are automatically migrated to Glacier storage. This is less performant and data are no longer instantly downloadable. Users will need to request a restore before downloading their files. Restore requests can be submitted either in the user portal or using a command line tool available on our compute nodes (more details below).</p> <ol> <li>Activity in this context means the user has interacted with the file in some way, e.g. by downloading. </li> </ol> <p>After three months of inactivity in the Glacier access tier, data are automatically migrated to Deep Glacier. Deep Glacier utilizes very slow storage technology and requires a restore request to be submitted prior to downloading files, similar to Glacier. Deep Glacier restore requests typically take more time than Glacier files. </p> <pre><code>graph LR\n  A[Data uploaded&lt;br&gt;to AWS bucket] --&gt; B[Data stored in&lt;br&gt;standard access tier];\n  B --&gt;|Data downloaded| C[Standard access&lt;br&gt;tier reset];\n  C --&gt; B;\n  B --&gt;|Three months inactivity| D[Glacier access&lt;br&gt;storage tier];\n  D --&gt; |Restore request&lt;br&gt;submitted| C; \n  D --&gt; |Three months inactivity| E[Deep Glacier access&lt;br&gt;storage tier];\n  E --&gt; |Restore request&lt;br&gt;submitted| C;\n</code></pre>"},{"location":"storage_and_transfers/storage/tier2_storage/#pricing","title":"Pricing","text":""},{"location":"storage_and_transfers/storage/tier2_storage/#storage-costs","title":"Storage Costs","text":"<p>Part of this service is paid for by researchers and the rest is either subsidized or covered by UITS. The data stored in S3 will be billed monthly by AWS to the KFS account used when this is set up. Data in archival storage will be stored at no cost to the researcher. You will receive an email with detailed billing information when charges are made to your account.</p> Tier Cost to Researchers Duration Data Retrieval Standard $0 (First TB)$23/TB/Month<sup>1</sup> (data \\(&gt;\\) 1 TB) Three months (if data not downloaded). After three months, untouched data automatically migrate to Glacier. Data may be immediately downloaded. Glacier $0 Three months (if data not downloaded*). After three months, untouched data automatically migrated to Deep Glacier. A restore request must be submitted. Restores may take a few minutes to hours. Data may be transferred once restored. Deep Glacier $0 Unlimited (if data not downloaded) A restore request must be submitted. Restores may take a few hours to days. Data may be transferred once restored."},{"location":"storage_and_transfers/storage/tier2_storage/#data-transfer-costs","title":"Data Transfer Costs","text":"<p>Data movement costs are subsidized by UITS, researchers are not charged any AWS transfer fees.</p>"},{"location":"storage_and_transfers/storage/tier2_storage/#request-a-bucket","title":"Request a Bucket","text":"<p>Who can submit a request?</p> <p>A group's PI is responsible for submitting a storage request unless they have a group delegate who may submit requests on their behalf.</p> <p>First, log into the User Portal and navigate to the Storage tab at the top of the page. Select Submit Tier 2 Storage Request.</p> <p></p> <p>This will open a web form. Add your KFS number under KFS Number(1) and the email address for the Department's financial contact under Business contact email. There will also be two optional fields: Subaccount and Project. These are used for tagging/reporting purposes in KFS billing. You can safely leave these entries blank if you're not sure what they are. Once you have completed the form, click Send request. </p> <ol> <li>A KFS number is used for accounting purposes and used by your Department's finance specialist. If you do not know your KFS number, contact your department's financial office. </li> </ol> <p></p> <p>Submitting this form will open a ServiceNow ticket. Processing time may take up to a few days. Once your request has been completed, you will receive a confirmation email with a link to subscribe for account alerts (e.g., notifications for a sudden spike in usage). </p> <p></p>"},{"location":"storage_and_transfers/storage/tier2_storage/#checking-your-usage","title":"Checking Your Usage","text":"<p>Tip</p> <p>AWS runs a batch update every night with the results being reported the following day. This means that if you have made any modifications to your allocation, your usage information will not be accurately reflected until the next batch update. </p> User PortalCLI <p>You may check your storage usage at any time in the User Portal. Navigate to the Storage tab, select View Tier 2 Storage, and click Query Usage.</p> <p></p> <p>To view the size and storage classes of individual objects, you will need to use the CLI interface.</p> <p>A command line tool is available on our compute nodes that will allow you to view the size and storage classes of the contents in your bucket. You will need to generate access keys to use this tool (see the next section). This can be accessed using: <pre><code>(elgato) [netid@junonia ~]$ interactive\n[netid@cpu37 ~]$ module load contrib ; module load bjoyce3/sarawillis/tier2-viewer\n</code></pre> For information on usage: <pre><code>tier2-viewer --help\n</code></pre> To play a tutorial in your terminal, use: <pre><code>tier2-viewer --example\n</code></pre></p>"},{"location":"storage_and_transfers/storage/tier2_storage/#generate-access-keys","title":"Generate Access Keys","text":"<p>Access keys will allow you to connect your AWS bucket using tools such as Globus which will enable you to make transfers directly between HPC and your Tier 2 storage allocation. Access keys should be treated as passwords and should only be shared with trusted group members and collaborators. </p> <p>To generate an access key, log into the User Portal, navigate to the Storage tab, and select Regenerate IAM Access Key under the Tier 2 Storage heading.</p> <p></p> <p>This will generate a KeyID and Secret Access Key used to establish the connection. Save these keys somewhere safe since once the window is closed, they cannot be retrieved. If you forget your keys, you can regenerate a new pair.</p> <p></p>"},{"location":"storage_and_transfers/storage/tier2_storage/#transferring-files","title":"Transferring Files","text":"<p>Files must be in the Standard tier to be downloadable</p> <p>Any files that are in archival storage (Glacier or Deep Glacier) must first be restored to the Standard tier to be retrievable. See the next section for more information.</p> <p>The easiest way to transfer files from AWS to HPC is using Globus. We have instructions in our Transferring Files page on how to set up an endpoint to access your AWS bucket as well as how to initiate file transfers.</p> <p>Some other file transfer programs include rclone and Cyberduck.</p>"},{"location":"storage_and_transfers/storage/tier2_storage/#restoring-archived-data","title":"Restoring Archived Data","text":"<p>Data that are not touched for at least 90 and 180 days are automatically re-tiered to archival storage (Glacier and Deep Glacier, respectively). Files stored in an archival state cannot be transferred out of AWS until they are restored. Restore requests can be submitted either via the User Portal or using a command line utility available on our compute nodes. </p> <p>The time it takes for an object to be retrieved is dependent on its storage class. Objects in Glacier may take a few hours while objects in Deep Glacier may take up to a day or two. Once an object has been restored, it will move back up to the frequent access tier and can be downloaded using any transfer method you prefer.</p> User PortalCLI <p>File count</p> <p>Warning: If you are restoring a directory, the portal will only support restore requests for directories containing up to 50 files. If you need to restore a large directory, use the CLI.</p> <p>In the User Portal, navigate to the Storage tab by clicking Restore Archived Tier 2 Storage Object:</p> <p></p> <p>This will open a box where you can enter the path to a file or directory in your bucket. Enter the path to the object you would like to restore, then click  to initiate the process.</p> <p></p> <p>A command line tool is available on our compute nodes that will allow you to view the size and storage classes of the contents in your bucket. You will need to generate access keys to use this tool. This can be accessed using: <pre><code>(elgato) [netid@junonia ~]$ interactive\n[netid@cpu37 ~]$ module load contrib ; module load bjoyce3/sarawillis/tier2-viewer\n</code></pre> For information on usage: <pre><code>tier2-viewer --help\n</code></pre> To play a tutorial in your terminal, use: <pre><code>tier2-viewer --example\n</code></pre> The <code>--restore</code> flag can be used to either restore a file or a full directory. </p> <ol> <li> <p>More up-to-date pricing information can be found on AWS's website.\u00a0\u21a9</p> </li> </ol>"},{"location":"storage_and_transfers/transfers/cyberduck/","title":"Cyberduck","text":"<p>Cyberduck is a graphical file transfer application that can be used to connect to and transfer files between your local computer and various remote servers and cloud storage services. To get started, you can download the application onto your local workstation from their website here: https://cyberduck.io/</p>"},{"location":"storage_and_transfers/transfers/cyberduck/#initiating-transfers","title":"Initiating Transfers","text":"<p>Once you have Cyberduck installed, open the software and select New Browser from the toolbar</p> <p></p> <p>In the window that opens, select Open Connection</p> <p></p> <p>This will give you a number of options to choose from.</p>"},{"location":"storage_and_transfers/transfers/cyberduck/#some-connection-options","title":"Some Connection Options","text":"HPCGoogle Drive <p>To connect to HPC, select SFTP (SSH File Transfer Protocol) from the top dropdown, enter <code>filexfer.hpc.arizona.edu</code> under Server, and your university credentials under Username and Password.</p> <p></p> <p>Once you click Connect, you will be prompted to duo-authenticate</p> <p></p> <p>If your connection is successful, you will see a window open with the contents of your home directory.</p> <p></p> <p>To connect to Google Drive, select the Google Drive option from the dropdown tab and select Connect</p> <p></p> <p>This will open a browser where you will be prompted to log into your Google Drive account.</p> <p></p> <p>Once you have successfully logged in, grant access to Cyberduck where prompted. If this process is successful, you should see a connection window where you can navigate through the contents of your Google Drive.</p> <p></p> <p>To initiate transfers, simply drag and drop your files between the Cyberduck window and your local computer. If you have multiple connections open, you can also initiate transfers between two remotes by dragging and dropping files between two connection windows.</p>"},{"location":"storage_and_transfers/transfers/globus/","title":"Globus","text":""},{"location":"storage_and_transfers/transfers/globus/#overview","title":"Overview","text":"<p>Globus provides a graphical web application that employs GridFTP to transfer data between pre-configured endpoints. GridFTP is an extension of the standard File transfer Protocol (FTP) for high-speed, reliable, and secure data transfer. Because GridFTP provides a more reliable and high performance file transfer (compared to protocols such as SCP or rsync), it enables the transmission of very large files. GridFTP also addresses the problem of incompatibility between storage and access systems. (You can read more about the advantages of GridFTP here).</p> <p>A list of endpoint names managed by HPC are shown below for reference. For more information on usage, see HPC-Managed Globus Endpoints below. </p> Endpoint Name Data Storage UA HPC Filesystems HPC's main storage array (access to home, xdisk, and groups) UA Rental Storage Filesystem HPC rental storage Tier 2 AWS Storage HPC-managed AWS S3 buckets UA HPC HIPAA Filesystems Soteria"},{"location":"storage_and_transfers/transfers/globus/#accessing-globus","title":"Accessing Globus","text":"<p>Globus can be used as a web application. To access it, navigate to https://www.globus.org/. Next, click Log In in the upper right-hand corner</p> <p></p> <p>On the next page, enter The University of Arizona in the search field and click the result.</p> <p></p> <p>This will take you through the standard university WebAuth login process. Once you successfully log in, you will be placed in a File Manager window. The various steps for setting up endpoints, initiating transfers, and viewing a transfer's progress can be found in the sections below.</p> <p></p>"},{"location":"storage_and_transfers/transfers/globus/#globus-connect-personal","title":"Globus Connect Personal","text":"<p>To transfer files to/from your personal computer with Globus, you'll need to have a local endpoint set up. This can be achieved using Globus Connect Personal. Official documentation on how to install the relevant software and configure a local endpoint can be found in Globus' official how-to documentation. An overview is shown for Mac, Linux, and Windows below.</p> <p>To start, regardless of operating system, go to https://www.globus.org/, log in, navigate to the Collections tab, and select Get Globus Connect Personal</p> <p></p> <p>From there, choose your operating system to proceed with the download and setup process</p> <p></p> MacWindowsLinux <p>Once you've downloaded the .dmg file, open it and drag/drop the Globus icon into your Applications directory</p> <p></p> <p>Next, open the application. This will prompt you to log in via the university WebAuth process in a browser session. Once you've logged in, enter an identifying label for your local machine and grant Globus access</p> <p></p> <p>This will bring you back to your local Globus Connect Personal installation. You will fill out your local display name for your endpoint and click Save.</p> <p></p> <p>Once your installation is complete, open the .exe file to initiate the install. Click Yes to allow Globus Connect Personal to make changes to your device</p> <p></p> <p>Next, select the install location, and click Install. </p> <p></p> <p>Once the install is complete, make sure the Run Globus Connect Personal box is checked and click Finish.</p> <p></p> <p>Globus Connect Personal will then open and begin the configuration process. Click Log In to continue.</p> <p></p> <p>This will open a web browser where you will go through the typical UArizona WebAuth login process. Once you're logged in, give your local endpoint a descriptive name and click Allow.</p> <p></p> <p>This will bring you back to your local install. Enter a descriptive local name for your endpoint and click Save.</p> <p></p> <p>Once your installation is complete, open a terminal, navigate to your Downloads directory, and unpack the tar archive. Next, change into the unpacked directory and execute the <code>globusconnectpersonal</code> binary:</p> <pre><code>[user@ubuntu ~]$ cd Downloads\n[user@ubuntu Downloads]$ tar xzvf globusconnectpersonal-latest.tgz\n[user@ubuntu Downloads]$ cd globusconnectpersonal-3.2.0\n[user@ubuntu globusconnectpersonal-3.2.0]$ ./globusconnectpersonal\n</code></pre> <p>This will bring up a graphical application. Click Log In to continue</p> <p></p> <p>This will open a web browser where you will need to go through the typical university WebAuth process. Once you're logged in, give your endpoint a name and click Allow.</p> <p></p> <p>This will bring you back to your local installation. Give your machine a descriptive name, then select Save.  </p> <p>Your setup should now be complete and your endpoint will now be usable to initiate transfers. You can find your endpoint by navigating to the Collections tab and checking the box Administered by you. For example:</p> <p></p>"},{"location":"storage_and_transfers/transfers/globus/#hpc-managed-globus-endpoints","title":"HPC Managed Globus Endpoints","text":"<p>HPC managed endpoints allow you to connect to HPC-affiliated storage to initiate transfers. Transfers can be made between any two endpoints; for example, allowing you to make transfers between your own personal computer and HPC storage, between HPC storage (<code>/home</code>, <code>/groups</code>, or <code>/xdisk</code>) and a rental option (such as Tier 2 AWS buckets or HPC rental storage), or between HPC and another institution's endpoint.</p> <p>Below are a list of HPC managed endpoints and how to configure them:</p> HPC StorageRental StorageTier 2 AWS Storage <p>The endpoint for HPC can be found by searching UA HPC Filesystems under the Collections tab.</p> <p></p> <p>Click the result, then click Open in File Manager to access your HPC files.</p> <p></p> <p>The default location is your /home on HPC. You can navigate through by double-clicking directories, or by entering a full path in the Path search bar and hitting enter. This method can be used to access any /xdisk or /groups directories you have access to.</p> <p></p> <p>The endpoint for rental storage (found on the filexfer nodes under <code>/rental</code>) can be found by searching UA Rental Storage Filesystem under the Collections tab.</p> <p></p> <p>This will open details on the endpoint. Click Open in File Manager to view the contents.</p> <p></p> <p>The root for this endpoint is /rental. Faculty members who have rented storage will have a directory with their NetID in this space. Find the one relevant to you and double-click to access.</p> <p></p> <p>To access a Tier 2 AWS S3 bucket, in the Collections tab, enter UA AWS S3 in the search bar. In the results, you should see the name UA AWS S3 show up with the description Subscribed Mapped Collection. Click the endpoint's name to proceed</p> <p></p> <p>Next, select the Credentials tab and select Add Credential. If you are prompted for Authentication/Consent, click Continue</p> <p></p> <p>If requested, authenticate by selecting your Arizona email address, then Allow. You will then be returned to the Credentials tab. From there, link to your AWS S3 Bucket by entering your public and private keys in the provided fields and click Continue.</p> <p></p> <p>Once you've added your keys, navigate back to the UA AWS S3 collection, go to the Collections tab, and click Add a Guest Collection on the right</p> <p></p> <p>Under Create New Guest Collection, click Browse next to the Directory field to find your group's AWS bucket. You will find it under <code>/ua-rt-t2-faculty_netid/</code> where faculty_netid is the NetID of the faculty member who requested the bucket. Under Display Name, enter a descriptive name that you can use to identify your bucket. Once you've completed the process, click Create Collection at the bottom of the page.</p> <p>Tip</p> <p>If you encounter Authentication/Consent Required after clicking Browse, click Continue, select your university credentials, and click Allow. That should bring you back to the Browse window.</p> <p></p> <p>To find and use your new collection, navigate to the Collections tab and select the display name you assigned to your bucket. That will open your collection in the File Manager window allowing you to view the contents and initiate transfers.</p> <p></p> <p>If you click the display name, this will open the bucket in the Globus file manager window allowing you to see the contents</p> <p></p>"},{"location":"storage_and_transfers/transfers/globus/#making-transfers","title":"Making Transfers","text":"<p>Transfers can be made between any two endpoints of your choosing using the File Manager window in the Globus web application. In this example, we'll make a transfer between a Globus Connect Personal endpoint and the primary HPC storage array.</p> <p>To start, go to the File Manager tab in the Globus web application. Make sure you have the dual-panel mode enabled (upper right-hand corner shown with the red arrow below) to allow you to open two endpoints. Start with opening your first endpoint by clicking the Search bar on the left-hand side. </p> <p></p> <p>This will open a window where you can search for your first endpoint. In this example, we'll use UA HPC Filesystems. Click the result.</p> <p></p> <p>Now, on the left-hand side you should see the contents of your home directory on HPC. You can navigate through the various directories by double-clicking the folder icons, or can enter a full path in the Path search bar. To open a second connection, click the Search bar on the right-hand side. </p> <p></p> <p>You can search for your next endpoint in the same way as we searched for UA HPC Filesystems. You can also find recently used endpoints and your collections (e.g., a Google Drive collection or personal endpoint) under the Recent and Your Collections tabs. In this example, we'll go to Your Collections, find a personal endpoint, and click the result. </p> <p></p> <p>Now you should be back in the File Manager window with two endpoints open. A transfer can be made from one endpoint to another by selecting the item(s) you want to transfer, then clicking the Start button.</p> <p></p>"},{"location":"storage_and_transfers/transfers/globus/#monitoring-your-transfers","title":"Monitoring Your Transfers","text":"<p>When you initiate a transfer following the instructions in the Making Transfers section above, a green box will pop up confirming the request. </p> <p></p> <p>You can get additional information the Activity panel on the left-hand side of the page. This will show you active and past transfers as well as their status. You can view additional details about your transfers by clicking the &gt; shown on the right-hand side next to the target task. You can also cancel a transfer by clicking the \u00d7 on the right. </p> <p></p> <p>Once your transfer has completed, you should receive an email with its status.</p>"},{"location":"storage_and_transfers/transfers/irods/","title":"iRODS","text":"<p>CyVerse Support</p> <p>If you are looking for information on how to connect to CyVerse's data store, see their iRODS documentation for a guide.</p> <p>Only use on DTNs</p> <p>iRODS should only be used on the data transfer nodes (DTNs)(1), which are equipped to handle large data transfers. Use of iRODS on the compute nodes may result in system or network issues. </p> <ol> <li>Hostname: <code>filexfer.hpc.arizona.edu</code></li> </ol> <p>iRODS 4 is installed as a standard package on the operating systems of the data transfer nodes. You will need to <code>iinit</code> the first time you use the software (see below). </p>"},{"location":"storage_and_transfers/transfers/irods/#initializing-irods","title":"Initializing iRODS","text":"<p>Running <code>iinit</code> for any system using iRODS 4.x, unlike its iRODS 3 counterpart, does not help you set up the environment. Instead, you need to run <code>create_irods_env</code> with suitable options for the iRODS host, zone, username, etc manually.</p> For this key Enter this <code>-h</code> <code>&lt;hostname of iRODS server&gt;</code> <code>-p</code> <code>&lt;port number of iRODS server&gt;</code> (1247 is default) <code>-z</code> <code>&lt;Zone name of iRODS zone&gt;</code> <code>-u</code> <code>&lt;user name on the iRODS server&gt;</code> (may not match your netid) <code>-a</code> <code>&lt;authentication method for the iRODS server&gt;</code> (PAM, native,...) <p>For example <pre><code>create_irods_env -a native -h someserver.somewhere.net -z MYZONE\n</code></pre></p> <p>will suffice to create an appropriate <code>~/.irods/irods_environment.json</code> file to allow you to run <code>iinit</code>; we took the default <code>-p 1247</code>, <code>-u &lt;your NetId&gt;</code> in the above example by omitting <code>-p</code> and <code>-u</code>.  You only need to do this step one time; subsequent times you will just run <code>iinit</code> and it will asked for your password. Note <code>create_irods_env</code> will not overwrite or alter an existing <code>~/.irods/irods_environment.json</code> file.</p> <p>Once the  <code>~/.irods/irods_environment.json</code> file is created properly, you should be able to sign in to the iRODS server your selected using <code>iinit</code>, viz:</p> <pre><code>$ iinit\nEnter your current ... password:    # enter your iRODS server password here\n</code></pre> <p>At this point you can use other iRods commands such as <code>icp</code> to move files.</p>"},{"location":"storage_and_transfers/transfers/irods/#examples","title":"Examples","text":"<p>Tip</p> <p>In the following examples:</p> <ul> <li><code>my-files-to-transfer/</code> is the example name of the directory or folder for bulk transfers.</li> <li><code>my-file-to-transfer.txt</code> is the example name for single file transfers.</li> <li>Any filename may be used for the <code>checkpoint-file</code>.</li> </ul> <p>Bulk Files Transfer Example <pre><code>iput -P -b -r -T --retries 3 -X checkpoint-file my-files-to-transfer/\n</code></pre></p> <p>Single Large File Transfer Example <pre><code>iput -P -T --retries 3 --lfrestart checkpoint-lf-file my-file-to-transfer.txt\n</code></pre></p>"},{"location":"storage_and_transfers/transfers/open_on_demand/","title":"Open OnDemand","text":"<p>Tip</p> <p>Open OnDemand file transfers are limited to 64 MB. For larger files, see our data transfer overview for more options.</p> <p>A popular method of transferring files to and from the HPC is the Open OnDemand interface, which is accessed through the browser at ood.hpc.arizona.edu. </p> <p>To access the file browser in Open OnDemand, choose your desired share from the Files dropdown menu.</p> <p></p> <p>From there, you should see a list of folders and files. Click on folders to open them up, or use the file path navigator to ascend the tree. </p> <p></p> <p>Additional actions can be taken using the button ribbon on the top right. To upload or download files to/from your selected directory, select Upload or Download.</p> <p></p> <p>To change the root directory, use the links on the left hand side of the screen. </p> <p></p> <p>When navigating to your group's share within <code>/groups</code> or <code>/xdisk</code>, use the \"Filter\" box to quickly find your folder from the list.</p> <p></p>"},{"location":"storage_and_transfers/transfers/overview/","title":"Overview","text":"<p>Log In Before Transferring</p> <p>To make transfers to/from HPC, you will need to have logged into your account at least once. If you have not, you may encounter \"directory does not exist\" errors. This is because your home directory is not created until you log in for the first time. See our System Access page for information on logging in.</p>"},{"location":"storage_and_transfers/transfers/overview/#designated-data-transfer-node-for-file-transfers","title":"Designated Data Transfer Node for File Transfers","text":"<p>For efficient file transfers to and from the HPC system, utilize the designated data transfer node, hostname: <code>filexfer.hpc.arizona.edu</code>. This node is optimized for handling large data transfers and is equipped with a high-speed 100 Gb interconnect.</p> <p>Why Use the Data Transfer Node?</p> <ul> <li> <p> Optimized Performance.</p> <p>The dedicated data transfer node ensures efficient transfer speeds, particularly for large datasets.</p> </li> <li> <p> Network Stability.</p> <p>Utilizing the data transfer node helps prevent network congestion and potential disruptions on other components of the HPC system.</p> </li> </ul> <p>Do not use hpc.arizona.edu</p> <p>Using the hostname <code>hpc.arizona.edu</code> for transfers will move your data to the HPC bastion host. The bastion host is not connected to the shared storage array (meaning files stored here will not be accessible on login/compute nodes) and has limited storage capacity. Users are restricted to 10 MB of space on this node and may experience login issues if this is exceeded.</p>"},{"location":"storage_and_transfers/transfers/overview/#data-transfers-by-size","title":"Data Transfers By Size","text":"<ol> <li>Transfers \\(\\leq\\) 64 MB: For small data transfers, the web portal offers the most intuitive method.</li> <li>Transfers \\(&lt;\\)100 GB: we recommend SFTP, SCP or Rsync using <code>filexfer.hpc.arizona.edu</code>.  </li> <li>Transfers \\(&gt;\\)100 GB, transfers outside the university, and large transfers within HPC: we recommend using Globus (GridFTP).</li> </ol>"},{"location":"storage_and_transfers/transfers/overview/#best-practices","title":"Best Practices","text":"<ul> <li> <p> Use the file transfer nodes for large data transfers</p> <p>Login and compute nodes are not designed for large file transfers and transfers initiated here may result in network problems. The data transfer nodes (DTNs) are specifically set up for moving large amounts of data and are accessible via the hostname <code>filexfer.hpc.arizona.edu</code>.</p> </li> <li> <p> Limit file copy sessions</p> <p>You share bandwidth with others. Two or three SCP sessions are probably ok; \\(&gt;\\)10 is not.</p> </li> <li> <p> Consolidate files</p> <p>If you are transferring many small files, consider collecting them in a tarball first.</p> </li> </ul>"},{"location":"storage_and_transfers/transfers/overview/#transfer-software-summary","title":"Transfer Software Summary","text":"Software CLI Interface? GUI Interface? Access to Cloud Services? Notes Google Drive AWS Box Dropbox Globus \u2705 \u2705 \u2705 \u2705 \u274c \u274c SFTP \u2705 \u2705 \u274c \u274c \u274c \u274c SCP \u2705 \u2705 \u274c \u274c \u274c \u274c On Windows, WinSCP is available as a GUI interface Rsync \u2705 \u2705 \u274c \u274c \u274c \u274c Grsync is a GUI interface for Rsync for multiple platforms. Rclone \u2705 \u2753 \u2705 \u2705 \u2705 \u2705 Rclone has recently announced they have an experimental GUI. Cyberduck \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 iRODS \u2705 \u2705 \u274c \u274c \u274c \u274c"},{"location":"storage_and_transfers/transfers/overview/#file-transfers-and-ssh-keys","title":"File Transfers and SSH Keys","text":"<p>Several of the file transfer methods listed below use authentication based on the SSH protocol, including SCP, SFTP, and Rsync. Therefore, adding your SSH Key to the <code>filexfer.hpc.arizona.edu</code> node can allow one to avoid entering passwords when using those methods. See the documentation for adding SSH Keys.</p>"},{"location":"storage_and_transfers/transfers/rclone/","title":"Rclone","text":"<p>Rclone is a CLI tool installed on <code>filexfer.hpc.arizona.edu</code> that can be used to transfer files between HPC and Cloud-based storage sites. To use Rclone, you will need to start by configuring it. Rclone's configuration process is fairly straightforward and has a large number of options to choose from. Some configuration examples are shown below. </p> Newer Version Available <p>As of December 12, 2024 a newer version of <code>rclone</code> is available on the file transfer nodes. In the default environment:</p> <pre><code>[user@sdmz-dtn-3 ~]$ rclone --version\nrclone v1.58.1\n</code></pre> <p>The command to activate the new environment is <code>optrclone</code>:</p> <pre><code>[user@sdmz-dtn-3 ~]$ optrclone\nSelecting version 1.68.2\n[user@sdmz-dtn-3 ~]$ rclone --version\nrclone v1.68.2\n</code></pre> <p>Exiting the new environment:</p> <pre><code>[user@sdmz-dtn-3 ~]$ exit\n[user@sdmz-dtn-3 ~]$ rclone --version\nrclone v1.58.1\n</code></pre>"},{"location":"storage_and_transfers/transfers/rclone/#configuring-rclone","title":"Configuring Rclone","text":"<p>To configure Rclone, you'll first want to be in an OnDemand Desktop session. This will give you access to both a terminal session and browser that can be used for granting Rclone access to your target cloud service. To configure Rclone, open a MATE terminal and type <code>rclone config</code></p> <p></p> <p>some examples for configuring rclone for common cloud services are shown below. Some text has been removed for brevity. If there is nothing included after the <code>&gt;</code> rclone prompt, then the option was left empty before hitting Enter. When entries are left blank, Rclone's default options will be used. </p> <p>Verify your input</p> <p>The instructions below were written on 9/6/2024. It's possible some Rclone configuration options have been updated, so verify the any options you enter match the correct cloud provider and desired configuration options. </p> Google DriveBox <pre><code>[netid@sdmz-dtn-4 ~]$ rclone config\nName                 Type\n====                 ====\n\n  e) Edit existing remote\n  n) New remote\n  d) Delete remote\n  r) Rename remote\n  c) Copy remote\n  s) Set configuration password\n  q) Quit config\n  e/n/d/r/c/s/q&gt; n\n  name&gt; GoogleDrive\n  Type of storage to configure.\n  Enter a string value. Press Enter for the default (\"\").\n  Choose a number from below, or type in your own value\n  ...\n  17 / Google Drive\n    \\ \"drive\"\n  ...\n  Storage&gt; 17\n  Google Application Client Id\n  client_id&gt;\n  client_secret&gt;\n  scope&gt; 1\n  root_folder_id&gt;\n  service_account_file&gt;\n  Edit advanced config? (y/n)\n  y) Yes\n  n) No\n  y/n&gt; n\n</code></pre> <p>The next prompt will ask if you would like to use auto config, select <code>y</code>. This will open a Firefox browser prompting you to log into your Google Drive account. Once you've successfully logged in, authorize rclone for access. You will then return to your terminal:</p> <pre><code>Remote config\nUse auto config?\n  * Say Y if not sure\n  * Say N if you are working on a remote or headless machine\ny) Yes\nn) No\ny/n&gt; y\nIf your browser doesn't open automatically go to the following link: &lt;long URL&gt;\nLog in and authorize rclone for access\n...\nConfigure this as a team drive?\ny) Yes\nn) No\ny/n&gt; n\n--------------------\n[GoogleDrive]\ntype = drive\nscope = drive\ntoken = {\"access_token\": &lt;lots of token data&gt;}\n--------------------\ny) Yes this is OK\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt; y\nCurrent remotes:\n\nName                 Type\n====                 ====\nGoogleDrive          drive\n</code></pre> <p><pre><code>[netid@sdmz-dtn-4 ~]$ rclone config\n Name                 Type\n ====                 ====\n\n  e) Edit existing remote\n  n) New remote\n  d) Delete remote\n  r) Rename remote\n  c) Copy remote\n  s) Set configuration password\n  q) Quit config\n  e/n/d/r/c/s/q&gt; n\n  name&gt; Box\n  Storage&gt; 8\n  client_id&gt; \n  client_secret&gt; \n  box_config_file&gt; \n  access_token&gt; \n  box_sub_type&gt; 1\n  Edit advanced config?\n  y/n&gt; n\n  Use auto config?\n    * Say Y if not sure\n    * Say N if you are working on a remote or headless machine\n\n  y) Yes (default)\n  n) No\n  y/n&gt; y\n</code></pre> This will open a Firefox browser. To sign in, click the SSO option and sign in with your university credentials (or log into your personal account), then authorize rclone so it can access Box. Once you're done, back in the terminal:</p> <pre><code>y) Yes this is OK (default)\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt; y\n\n/n/d/r/c/s/q&gt; q\n</code></pre>"},{"location":"storage_and_transfers/transfers/rclone/#rclone-transfers","title":"Rclone Transfers","text":"<p>The syntax to initiate file transfers will typically look something like</p> <pre><code>rclone copyto &lt;source&gt; &lt;destination&gt;\n</code></pre> <p>When <code>&lt;source&gt;</code> and/or <code>&lt;destination&gt;</code> are a cloud service, the format will be:</p> <pre><code>&lt;Name&gt;:/path/to/file/or/directory\n</code></pre> <p>Where <code>&lt;Name&gt;</code> is the name you gave for the service during the configuration process. For example, if I were to access my University Box endpoint (configured in the example in the previous section), I could check its contents using <code>rclone lsf</code>(1) with:</p> <ol> <li>Tip: <code>rclone ls</code> will list all files in a root directory and its subdirectories making it much harder to parse. You may want to start with using <code>lsf</code> which provides more user-friendly output.</li> </ol> <p><pre><code>[netid@sdmz-dtn-4 ~]$ rclone lsf Box:\nHPC Documentation Site/\nIntroToViz.pptx\nPhotos/\nRcloneExample/\n[netid@sdmz-dtn-4 ~]$ rclone lsf Box:RcloneExample\nexample.txt\n</code></pre> If I wanted to transfer the file <code>example.txt</code> to my working directory on HPC:</p> <pre><code>[netid@sdmz-dtn-4 ~]$ rclone copyto Box:RcloneExample/example.txt ./example.txt\n[netid@sdmz-dtn-4 ~]$ ls -l example.txt \n-rw-r-----. 1 netid staff 4 Sep  5 14:14 example.txt\n</code></pre> <p>More detailed information on using Rclone can be found in their documentation https://rclone.org/commands/. </p>"},{"location":"storage_and_transfers/transfers/rsync/","title":"rsync","text":"<p>Rsync is a fast and extraordinarily versatile file copying tool. It synchronizes files and directories between two different locations (or servers). Rsync copies only the differences of files that have actually changed. An important feature of Rsync not found in most similar programs/protocols is that the mirroring takes place with only one transmission in each direction. It can copy or display directory contents and copy files, optionally using compression and recursion. It is similar to SCP in that both a source and a destination must be specified, one of which may be remote. </p>"},{"location":"storage_and_transfers/transfers/rsync/#subdirectory-behavior","title":"Subdirectory Behavior","text":"<p>How can we recursively transfers all files from the directory <code>/src/directory-name</code> on a local machine into <code>/data/tmp/</code> on a remote machine?</p> <p>A trailing slash on the source changes the behavior of <code>rsync</code>. The inclusion of the trailing slash avoids creating an additional directory level at the destination. You can think of a trailing <code>/</code> on a source as meaning \u201ccopy the contents of this directory\u201d as opposed to \u201ccopy the directory by name\u201d, but in both cases the attributes of the containing directory are transferred to the containing directory on the destination. </p> <p>Trailing Slash</p> <p>The below command will copy all of the contents of <code>directory-name</code> into <code>tmp</code>, excluding the parent folder.</p> <p><pre><code>rsync -ravz  computer-name:/src/directory-name/  user@remote.host:/data/tmp --log-file=hpc-user-rsync.log \n</code></pre> The folder <code>remote.host:/data/tmp</code> will then contain anything it held previously in addition to the subfolders of <code>directory-name</code>.</p> <p>No Trailing Slash</p> <p>On the other hand, the below command will copy <code>directory-name</code> as a parent folder into <code>tmp</code> resulting in a new directory <code>/data/tmp/directory-name</code>. The contents of <code>directory-name</code> will appear exactly as they did on the local machine. </p> <pre><code>rsync -ravz  computer-name:src/directory-name  user@remote.host:/data/tmp --log-file=hpc-user-rsync.log \n</code></pre> <p>Note that including <code>computer-name</code> is optional when referring to the local machine. Log files are optional but recommended.</p>"},{"location":"storage_and_transfers/transfers/rsync/#additional-options","title":"Additional Options","text":"Flag Meaning <code>-r</code> Recursive mode; loop through contents of all subfolders <code>-a</code> Archive mode; will preserve time stamps and other metadata <code>-v</code> Increase verbosity <code>-z</code> Compress file data during the transfer <code>--log-file=FILE</code> Log everything done in specified <code>FILE</code>"},{"location":"storage_and_transfers/transfers/scp/","title":"SCP","text":"<p>Secure Copy, or SCP, uses Secure Shell (SSH) for data transfer and utilizes the same mechanisms for authentication, thereby ensuring the authenticity and confidentiality of the data in transit.</p>"},{"location":"storage_and_transfers/transfers/scp/#maclinux","title":"Mac/Linux","text":"<p>You will need to use an SSH v2 compliant terminal to move files to/from HPC. For more information on using SCP, use <code>man scp</code>.</p> <p>Copying to HPC</p> <p>In a local terminal, you can move a file or directory to a designated subdirectory in your account on HPC using the following syntax:</p> <pre><code>scp -rp /path/to/file/or/directory netid@filexfer.hpc.arizona.edu:/path/to/remote/destination\n</code></pre> <p>Copying from HPC</p> <p>In a local terminal, you can copy a remote file from HPC to your current directory using the syntax: <pre><code>scp -rp netid@filexfer.hpc.arizona.edu:/path/to/file/or/directory .\n</code></pre></p> <p>Shorthand</p> <p>Note that the trailing period above refers to the current directory. See our Linux Cheat Sheet for more tips like this. </p> <p>Wildcards can be used for multiple file transfers (e.g. all files with .dat extension). Note the backslash <code>\\</code> preceding <code>*</code></p> <pre><code>scp netid@filexfer.hpc.arizona.edu:subdirectory/\\*.dat .\n</code></pre>"},{"location":"storage_and_transfers/transfers/scp/#windows","title":"Windows","text":"<p>Windows users can use software like WinSCP to make SCP transfers. To use WinSCP, first download/install the software from: https://winscp.net/eng/download.php</p> <p>To connect, enter <code>filexfer.hpc.arizona.edu</code> in the Host Name field, enter your NetID under User name, and enter your password. Accept by clicking Login. You'll be prompted to Duo Authenticate:</p> <p></p>"},{"location":"storage_and_transfers/transfers/sftp_ftp_lftp/","title":"SFTP/FTP/LFTP","text":"<p>Tip</p> <p>To use SFTP/LFTP, you will need to be connected to our file transfer nodes, hostname: <code>filexfer.hpc.arizona.edu</code>.</p>"},{"location":"storage_and_transfers/transfers/sftp_ftp_lftp/#sftp","title":"SFTP","text":"<p>SFTP (Secure File Transfer Protocol) ensures data encryption during transmission over the network. It offers features such as resuming interrupted transfers, directory listing, and remote file management.</p> <p>To perform file transfers using SFTP, you'll need an SSH v2 compliant terminal. To connect to HPC's data transfer nodes, run:</p> <pre><code>sftp your_netid@filexfer.hpc.arizona.edu\n</code></pre> <p>You will then be able to move files between your machine and HPC using <code>get</code> and <code>put</code> commands. For example:</p> <pre><code>sftp&gt; get /path/to/remote/file /path/to/local/directory ### Retrieves file from HPC. Omitting paths will default to working directories.\nsftp&gt; put /path/to/local/file /path/to/remote/directory ### Uploads a file from your local computer to HPC. Omitting paths will default to working directories.\nsftp&gt; help ### prints detailed sftp usage\n</code></pre>"},{"location":"storage_and_transfers/transfers/sftp_ftp_lftp/#ftplftp","title":"FTP/LFTP","text":"<p>Warning</p> <p>Due to security risks, it is not possible to FTP to the file transfer node from a remote machine, however, you may FTP from the file transfer node to a remote machine.</p> <p>HPC uses the FTP client LFTP to transfer files between the file transfer node and remote machines. This can be done using get and put commands. To use lftp, you must first connect to our file transfer node using an SSH v2 compliant terminal:</p> <pre><code>ssh your_netid@filexfer.hpc.arizona.edu\n</code></pre> <p>Once connected, you may connect to the external host using the command <code>lftp</code>. For example:</p> <pre><code>lftp ftp.hostname.gov\n</code></pre> <p>You will then be able to move files between HPC and the remote host using <code>get</code> and <code>put</code> commands. For example:</p> <pre><code>&gt; get /path/to/remote/file /path/to/local/directory ### retrieves file from remote host\n&gt; put /path/to/local/file /path/to/remote/directory ### Uploads file from HPC to remote host\n</code></pre>"},{"location":"support_and_training/cheat_sheet/","title":"Linux Cheat Sheet","text":""},{"location":"support_and_training/cheat_sheet/#why-learn-bash","title":"Why Learn Bash","text":"<p>Bash is a powerful command line shell that allows you to interact with our systems efficiently. It enables scripting and automation, job submission, file and directory management, remote access, resource monitoring, environment customization, parallel computing, data preprocessing, version control, error handling, software dependency management, custom workflows, and offers a valuable skill set applicable beyond HPC tasks. In other words, it allows you to do a lot!</p> <p>In the cheat sheet below, we offer some basics as a reference. We also highly suggest you explore some more comprehensive guides such as the following: Introduction to Linux on HPC</p>"},{"location":"support_and_training/cheat_sheet/#shell-basics","title":"Shell Basics","text":""},{"location":"support_and_training/cheat_sheet/#shortcuts","title":"Shortcuts","text":"<p>Shortcuts can be used as stand-ins for system locations, avoiding the need for full paths. A few examples:</p> Shortcut Description <code>~</code> Your home directory <code>.</code> Your working directory <code>..</code> One directory above your working directory"},{"location":"support_and_training/cheat_sheet/#commands","title":"Commands","text":"<code>ls</code> List the contents of a directory <code>ls -la</code> List the contents of a directory including hidden files <code>cd</code> Change your working directory <code>pwd</code> Show the path to your working directory <code>mkdir</code> Create a new directory <code>rm</code> Delete a file. Be careful, files deleted this way can't be retrieved <code>rm -r</code> Delete a directory. Be careful, directories deleted this way can't be retrieved"},{"location":"support_and_training/cheat_sheet/#hidden-files-and-directories","title":"Hidden Files and Directories","text":"<p>Hidden files and directories start with a dot <code>.</code> and won't show up when you do a simple <code>ls</code>. Some of these files are used to configure your environment when you first log in. Be careful when removing or modifying these files since doing so can cause unintended consequences.</p> <p>Tip</p> <p>The <code>~</code> in the filenames below indicates your home directory. For example, <code>~/.bashrc</code> is specifying a file in your home called <code>.bashrc</code>.</p> File/Directory What It Does Warnings <code>~/.bash_profile</code> This file sets your working environment when you first log into HPC. This file sources your <code>~/.bashrc</code> (see below). See the list in the danger block below. <code>~/.bashrc</code> This file sets your working environment when you first log into HPC. See the list in the danger block below. <code>~/.local</code> This is a hidden directory in your home where pip-installed python packages, jupyter kernels, RStudio session information, etc. goes. If you pip-install python packages when a virtual environment is not active, they will be installed in this directory. These will then be automatically loaded for all future python sessions (version-specific), including in Singularity images. This may cause versioning issues. We recommend always using virtual environments. <code>~/.apptainer</code> A hidden directory in your home where Apptainer images and cache files are stored by default. This directory can grow large quickly and fill up your home. You can modify your <code>~/.bashrc</code> to set a different cache directory location that has a larger quota. <p>Danger</p> <p>When working with hidden configuration files in your account (<code>.bashrc</code> and <code>.bash_profile</code>), be careful of:</p> <ol> <li> <p>Aliasing important Linux commands</p> <p>For example, the character <code>.</code> is a shortcut for <code>source</code>. If you do something like add <code>alias .=&lt;command&gt;</code> to your bashrc, you will lose basic functionality in the terminal, e.g., access to modules, virtual environments, etc. </p> </li> <li> <p>Recursively sourcing configuration files</p> <p>If you add <code>source ~/.bashrc</code> or <code>source ~/.bash_profile</code> to your bashrc, then you will enter an infinite sourcing loop. This means when you try to log in, your terminal will freeze, then your access will be denied. </p> </li> <li> <p>Using <code>echo</code></p> <p>If you use CLI tools for data transfer, e.g. <code>scp</code> or <code>sftp</code>, they may require a \"silent\" terminal. If you're trying to initiate transfers and are getting the error \"Received message too long\", check your bashrc to make sure you aren't printing anything to the terminal. </p> </li> <li> <p>Removing your files</p> <p>If you delete either your <code>~/.bashrc</code> or <code>~/.bash_profile</code>, you will lose access to <code>module</code> commands. </p> </li> </ol>"},{"location":"support_and_training/cheat_sheet/#environment-variables","title":"Environment Variables","text":"<p>Bash variables control how your environment is configured. For example: what executables, libraries, header files, etc. are findable by default; information about your Slurm job; MPI settings; GPU configuration; etc. To see all the environment variables that are defined for your session, try running the command <code>env</code>.</p> <p>An example of an important environment variable is <code>PATH</code>. This is set to a <code>:</code>-delimited list of paths that it uses to search for executables. You can see what it's set to by running <code>echo $PATH</code>. Any time you run a command, for example <code>ls</code>, that list is searched in order. To add new executables to your environment, you can add a path to <code>PATH</code>. For example:</p> <pre><code>export PATH=/path/to/new/directory:$PATH\n</code></pre> <p>This is what modules do: they update your environment variables to put new software and libraries into your environment for use. Some examples of some important variables are listed below:</p> Variable Function <code>PATH</code> A colon-delimited list of paths used to search for executables. When you run a command, such as <code>ls</code>, the directories listed in <code>PATH</code> are searched in order. <code>LD_LIBRARY_PATH</code> A colon-delimited list of directories in which the linker should search for shared libraries at runtime. Useful for specifying additional library directories for dynamically linked programs. For example: <code>export LD_LIBRARY_PATH=/path/to/library:$LD_LIBRARY_PATH</code> <code>CFLAGS</code>, <code>CXXFLAGS</code>, <code>LDFLAGS</code> Environment variables used to specify compiler flags for compiling C, C++, and linking respectively. These variables are often used to customize the build process of software. For example: <code>export CFLAGS=\"-O2 -march=native $CFLAGS\"</code> <code>CC</code>, <code>CXX</code>, <code>FC</code> Environment variables specifying the C, C++, and Fortran compilers, respectively. These variables allow users to specify which compiler should be used during the build process of software. For example: <code>export CC=gcc</code> <code>OMP_NUM_THREADS</code> Specifies the number of OpenMP threads to use for parallelized programs. This variable controls the number of threads used in parallel regions of OpenMP-enabled programs. For example: <code>export OMP_NUM_THREADS=4</code> <p>Slurm also sets its own environment variables.</p>"},{"location":"support_and_training/cheat_sheet/#linux-file-permissions","title":"Linux File Permissions","text":"<p>Linux file permissions control who can access files and directories and what they are able to do with them.</p> <p>HPC users may find it useful or necessary to share files and/or directories with collaborators. To do this effectively, it may become necessary to familiarize yourself with the linux file permission system in order to give collaborators access to your files. </p>"},{"location":"support_and_training/cheat_sheet/#types-of-permissions","title":"Types of Permissions","text":"<p>All files and directories on a Linux system have permissions defined for them. There are three types:</p> Permission Symbol Directory meaning File meaning Read <code>r</code> Users can view the contents of the directory Users can read the contents of a file Write <code>w</code> Users can modify the contents of a directory Users can write to a file Execute <code>x</code> Users can <code>cd</code> into a directory Users can directly execute a file <p>When a permission is missing, you will see a <code>-</code></p>"},{"location":"support_and_training/cheat_sheet/#viewing-permissions","title":"Viewing Permissions","text":"<p>To see a file or directory's file permissions, run the command <code>ls -l</code>. This will show you output that looks like the following:</p> <p><code>-</code><code>rwx</code><code>r-x</code><code>---</code><code> 1 </code><code>username</code><code> </code><code>groupname</code><code> 0 Feb 27 09:54 file</code></p> String Access Group Meaning <code>-</code> Tells you whether this item is a regular file (<code>-</code>), directory (<code>d</code>), or link <code>(l)</code> In this example, we're looking at a regular file <code>rwx</code> This describes the permissions that are set at the user level. In this case, they apply to the user with the username <code>username</code>. Your username is your NetID In this example, the file's owner can read, modify, and execute this file. <code>r-x</code> This describes the permissions that are set at the group level. In this case, they apply to anyone who is a member of the group <code>groupname</code>. To see your groups you're a member of, run the command <code>groups</code>. In this example, group members are allowed to see and execute the contents of this file, but they cannot modify it. <code>---</code> This describes the permissions that are set for anyone else on the system. In this example, the rest of the HPC community can't see or edit the contents of this file and can't execute it"},{"location":"support_and_training/cheat_sheet/#changing-permissions","title":"Changing Permissions","text":"<p>Permissions changes limitations</p> <p>Only the owner of a file or directory can change its permissions. </p> <p>To change the permissions of a file or directory, you can use the command <code>chmod</code>. This command accepts two types of input: symbolic and octal.</p>"},{"location":"support_and_training/cheat_sheet/#symbolic","title":"Symbolic","text":"<p>Symbolic notation is the most intuitive to understand. It involves a comma-delimited list of permissions symbols, each representing a specific permission type (read, write, execute) for a particular user or group. Here's a breakdown of the symbols used:</p> Symbol Meaning <code>u</code> \"User\". Refers to the user who owns the file. <code>g</code> \"Group\". Refers to the group that the file belongs to. <code>o</code> \"Other\". Refers to other users who are not the owner or in the group. <code>a</code> \"All\". Refers to all users (<code>u</code>, <code>g</code>, and <code>o</code>). <p>For each of these, you can use <code>+</code> to add a permission, <code>-</code> to remove a permission, or <code>=</code> to set the permissions explicitly. The permission symbols are: <code>r</code>,<code>w</code>, and <code>x</code> and match those described under Types of Permissions above.</p> <p>The general syntax is</p> <pre><code>chmod [who][operator][permissions]\n</code></pre> <p>Here are some examples of how you might use symbolic notation with the <code>chmod</code> command:</p> <ul> <li>To give the owner read and write permissions: <code>chmod u+rw file.txt</code></li> <li>To remove execute permissions for the group: <code>chmod g-x file.txt</code></li> <li>To give all users read and execute permissions without write permissions: <code>chmod a=rx file.txt</code></li> </ul>"},{"location":"support_and_training/cheat_sheet/#octal","title":"Octal","text":"<p>Octal notation is a more compact way of representing permissions using numbers. Each permission type (read, write, execute) is assigned a numeric value:</p> Octal notation Permission <code>4</code> read <code>2</code> write <code>1</code> execute <p>To set permissions, you add these values together for each permission type. For example:</p> <ul> <li>Read and write permissions: <code>4 (read) + 2 (write) = 6</code></li> <li>Execute permission only: <code>1 (execute)</code></li> </ul> <p>You then represent the permissions for the user, group, and others as a three-digit number. For instance:</p> <ul> <li><code>600</code> gives read and write permissions to the owner and no permissions to the group and others.</li> <li><code>755</code> gives read, write, and execute permissions to the owner, and read and execute permissions to the group and others.</li> </ul> <p>To use octal notation with the <code>chmod</code> command, you specify the permissions directly as numbers. For example:</p> <ul> <li><code>chmod 600 file.txt</code></li> <li><code>chmod 755 file.txt</code></li> </ul> <p>These commands will set the permissions of <code>file.txt</code> accordingly.</p>"},{"location":"support_and_training/cheat_sheet/#changing-group-ownership","title":"Changing Group Ownership","text":"<p>Ownership changes limitations</p> <p>Only the owner of a file or directory can change its group ownership</p> <p>To change the group ownership of a file or directory, you can use <code>chgrp</code>. For example:</p> <pre><code>chgrp groupname /path/to/file\n</code></pre>"},{"location":"support_and_training/cheat_sheet/#changing-user-ownership","title":"Changing User Ownership","text":"<p>Changing the user ownership of a file requires root permissions, even if the user owns the file they are trying to modify. If ownership modifications are needed, contact our consulting team for assistance. </p>"},{"location":"support_and_training/cheat_sheet/#file-management-in-shared-spaces","title":"File Management in Shared Spaces","text":"<p>ACLs unavailable</p> <p>Using something like <code>setfacl</code> is possible on some systems to set the default permissions for specific directories and their contents. Unfortunately, ACLs are not available on our systems due to software limitations on our file servers. Some alternatives are detailed below. </p> <p>If you're working in shared storage and want your files to be accessible to collaborators, sometimes it can be challenging to ensure those files are created with the correct group ownership and file permissions to allow access. In those cases, it might be helpful to look into setting the SGID (Set Group ID) and using the <code>umask</code> command described below.  </p>"},{"location":"support_and_training/cheat_sheet/#default-group-ownership","title":"Default Group Ownership","text":"<p>The SGID (Set Group ID) bit can be set on a directory to ensure that files and directories created within that parent directory inherit its group ownership. This ensures that those files can be made accessible (with the proper permissions) to other members of that group.</p> <p>To set the SGID bit on a directory, use the <code>chmod</code> command with the <code>g+s</code> option:</p> <pre><code>chmod g+s /path/to/directory\n</code></pre> <p>You'll notice that when a directory does not have the SGID bit set, its group permissions will look something like <code>rwx</code>. When the SGID bit is set, the <code>x</code> will become either an <code>s</code> (if group execute permissions are set) or <code>S</code> (if group execute permissions are disabled). For example:</p> <pre><code>(puma) [netid@wentletrap EXAMPLES]$ ls -ld parent_directory/\ndrwxr-xr-x. 2 netid hpcteam 0 Sep  4 12:50 parent_directory/\n(puma) [netid@wentletrap EXAMPLES]$ chmod g+s parent_directory/\n(puma) [netid@wentletrap EXAMPLES]$ ls -ld parent_directory/\ndrwxr-sr-x. 2 netid hpcteam 0 Sep  4 12:50 parent_directory/\n</code></pre>"},{"location":"support_and_training/cheat_sheet/#default-permissions","title":"Default Permissions","text":"<p>The <code>umask</code> command controls the default file permissions for all new files and directories created at a user level. It defines which permissions should not be set by default, and thus indirectly determines the default permissions. </p> <p>By setting an appropriate <code>umask</code>, you can ensure that files and directories are created with the permissions that are appropriate for your collaborative environment.</p> <p>To check your active umask value, you can run the command without arguments. For example, the default on our system is:</p> <p><pre><code>(puma) [netid@wentletrap EXAMPLES]$ umask\n0022\n</code></pre> To make sense of this output output, take the last three digits and subtract them from either <code>777</code> (for directories) or <code>666</code> (for files). So for example, in the output shown above, <code>0022</code> means new directories will always be created with the permissions <code>777</code> - <code>022</code> = <code>755</code> which is the same as <code>rwxr-xr-x</code>. Similarly, for files: <code>666</code> - <code>022</code> = <code>644</code> which is the same as <code>r-wr--r--</code>. If you're unfamiliar with octal formatting, see octal file permissions above for more information on how these numbers correspond to file permissions.</p> <p>To set a new <code>umask</code>, you can include arguments either in octal or symbolic format. Symbolic notation is a little more straightforward and intuitive than octal and uses the syntax </p> <pre><code>umask u=&lt;user permissions&gt;,g=&lt;group permissions&gt;,o=&lt;other permissions&gt;\n</code></pre> <p>For example:</p> <p><pre><code>(puma) [netid@junonia umask]$ touch test.txt &amp;&amp; mkdir test_dir\n(puma) [netid@junonia umask]$ ls -l test.txt &amp;&amp; ls -ld test_dir/\n-rw-r--r--. 1 netid hpcteam 0 Mar 30 10:24 test.txt\ndrwxr-sr-x. 2 netid hpcteam 0 Mar 30 10:24 test_dir/\n(puma) [netid@junonia umask]$ umask u=rwx,g=rwx,o=\n(puma) [netid@junonia umask]$ rm -r test.txt test_dir/\n(puma) [netid@junonia umask]$ touch test.txt &amp;&amp; mkdir test_dir\n(puma) [netid@junonia umask]$ ls -l test.txt &amp;&amp; ls -ld test_dir/\n-rw-rw----. 1 netid hpcteam 0 Mar 30 10:24 test.txt\ndrwxrws---. 2 netid hpcteam 0 Mar 30 10:24 test_dir/\n</code></pre> Note how once <code>umask</code> is set by the user, the default permissions applied to the new test file and directory allow group edit access and remove \"other\" access completely. The octal notation to accomplish the same result would be <code>umask 007</code>. </p> <p>Some things to keep in mind: </p> <ul> <li> <p><code>umask</code> only affects your active terminal session and does not propagate to future sessions. This means if you log out and log back in, your umask will be reset to the system default. If you'd like your default file permissions to be permanently changed, you can add your <code>umask</code> command to your ~/.bashrc. For more information on this file, see hidden files and directories above. </p> </li> <li> <p><code>umask</code> applies to all new files and directories you create, so you'll want to make sure you are not inadvertently giving unwanted access to your data.</p> </li> </ul>"},{"location":"support_and_training/cheat_sheet/#sticky-bits","title":"Sticky Bits","text":"<p>In shared environments with group write permissions, it's possible for users to accidentally or intentionally delete files or directories created by others. The Sticky Bit is a security feature that can help manage this issue by restricting file deletion within a directory.</p> <p>The Sticky Bit, when set on a directory, ensures that only the file's or directory's owner can delete or rename them. This can be particularly useful in shared directories where multiple users need to collaborate but should not interfere with each other's files. To set the Sticky Bit on a directory, use the chmod command with the <code>+t</code> option:</p> <p><pre><code>chmod +t /path/to/directory\n</code></pre> The change will be displayed at the end of the permissions string either as a <code>t</code> (if execute permissions are set for \"other\") or a <code>T</code> (if execute permissions are disabled for \"other\"). For example:</p> <pre><code>(puma) [netid1@wentletrap EXAMPLES]$ ls -ld parent_directory/\ndrwxrws--T. 3 netid2 hpcteam 512 Sep  4 13:36 parent_directory/\n(puma) [netid1@wentletrap EXAMPLES]$ ls -l parent_directory/\ntotal 4\ndrwxrws---. 2 netid2 hpcteam 0 Sep  4 13:36 other_user\n(puma) [netid1@wentletrap EXAMPLES]$ rm -r parent_directory/other_user/\nrm: cannot remove \u2018parent_directory/other_user/\u2019: Operation not permitted\n</code></pre> <p>Note that although <code>netid1</code> has full <code>rwx</code> permissions set for both <code>parent_directory</code> and <code>other_user</code>, they cannot delete <code>other_user</code> because the sticky bit is set on the parent directory. Note that if <code>netid2</code> were to try to delete it, they would be successful. </p> <p>One thing to keep in mind is that the owner of the parent directory will still be able to delete the contents even if the sticky bit is set and they do not own the directory they are deleting.</p>"},{"location":"support_and_training/cheat_sheet/#compression-and-archiving","title":"Compression and Archiving","text":"<p>Tip</p> <p>For very large directories, use the file transfer node. Hostname: <code>filexfer.hpc.arizona.edu</code></p> <p>Are you planning on transferring files to or from HPC? Do you have a lot of them? Then archiving is for you! </p> <p>Archiving files is the process of consolidating one or more files or directories into a single, compressed package or archive file. This simplifies data management, reduces storage space, and streamlines file transfer and backup operations. Transferring a single archived file to an external backup location my result in transfer speeds that are an order of magnitude faster than transferring the same data as an uncompressed directory with thousands (or sometimes millions) of files.</p> <p>As an example, we'll use the archiving tool <code>tar</code>. The syntax to create an archive is  <pre><code>tar &lt;options&gt; &lt;output_archive_name&gt; &lt;contents&gt;\n</code></pre></p> <p>We'll use the options <code>czvf</code> which stands for:</p> <ul> <li><code>c</code>: Create a new archive</li> <li><code>z</code>: Filter the archive through gzip</li> <li><code>v</code>: Verbosely print the output of the archiving process. Optional</li> <li><code>f</code>: User archive file</li> </ul> <p>Archiving a directory <code>dir</code> and its contents into a single file called <code>dir.tar.gz</code> then looks like:</p> <pre><code>(puma) [user@wentletrap archiving_example]$ tar czvf dir.tar.gz dir\ndir/\ndir/subdir1/\ndir/subdir1/file.txt\ndir/file1.txt\ndir/file3.txt\ndir/file2.txt\n(puma) [user@wentletrap archiving_example]$ ls\ndir  dir.tar.gz\n</code></pre> <p>To unpack this archive, change the <code>c</code> to an <code>x</code> which stands for \"extract\":</p> <pre><code>tar xzvf dir.tar.gz\n</code></pre>"},{"location":"support_and_training/consulting_services/","title":"Consulting Services","text":""},{"location":"support_and_training/consulting_services/#hpc-consulting-services","title":"HPC Consulting Services","text":"<p>Contact Us</p> <p></p> <p>Consulting is available free of cost to everyone and we welcome you to reach out! Our services include, but are not limited to:</p> <ul> <li>Help for new users getting started with our resources. We know using HPC systems for the first time can be intimidating so scheduling an in-person meeting where you can ask loads of questions can help a lot.</li> <li>General issues that may occur, e.g., why is my job spending so long in the queue, where can I find information on topic &lt;A&gt;, why is this strange and unexpected thing happening, etc.</li> <li>Advice on code optimization and utilizing our resources more effectively/efficiently.</li> </ul> When can I ask for help? <p>Any time! But first, we encourage you to:</p> <ul> <li>Double-check our FAQs - We keep track of commonly asked questions and document their solutions. You might find what you're looking for there.</li> <li>Look through our online documentation - There's lots of information to help get you started that may answer your question or help give you a better idea of what to ask.</li> </ul> How can I effectively write a support request? <p>Glad you asked! Helping us help you goes a long way and can give you better answers faster. Some general rules of thumb:</p> <ul> <li> <p>Detail detail detailA full error log may seem like a lot to send, but the more information we have, the more likely we are to be able to diagnose and/or replicate your issue.</p> </li> <li> <p>Use reply-all to email chains We will cc hpc-consult in our responses so that our small consulting team is able to view the issue and contribute.</p> </li> <li> <p>Provide contextThere's a common support issue called The XY Problem. Say you have a problem and try to solve it yourself but the attempted solution produces an additional problem. Submitting a ticket requesting help with the attempted solution without information about the original issue can lead to more confusion. </p> </li> <li> <p>Submit a ticket for your questionsIf you reply to general system announcements or send emails to an HPC staff member's private inbox without cc'ing hpc-consult, your ticket may get lost and go unanswered. Submitting a ticket will ensure we have a record of your question and will get to it as promptly as we are able.</p> </li> </ul> What are our support policies? <p>Mostly it is common sense rather than strict rules. The primary consideration is that our consultants work regular hours with some flexibility built-in. So don't expect detailed responses at night or on the weekends. You might get a quick response but don't count on it.</p> <p>Our consultants typically don't know how to run your applications unless they have broad usage like Python or R. So once we determine it is likely an issue with the code we will refer you to the provider. We don't troubleshoot bugs or run profilers, but we support tools like Valgrind for you to use.</p> <p>We don't mind you asking lots of questions. We encourage you to ask for a consulting session via Zoom or Office Hours (see below). We are not in the office so you can't drop by (although we kind of miss that personal engagement).</p> <p>The bottom line is that the supercomputers are only really productive tools when you have the support to gain the most out of them to improve both your results and the time to get results.</p> How can I reach HPC consulting? <p>We use ServiceNow and can be reached with a support ticket. Many in our research community are accustomed to using the list service for hpc-consult. That continues to work but is not as efficient. And we really want to discourage sending emails directly to your favorite consultant.  </p>"},{"location":"support_and_training/consulting_services/#office-hours","title":"Office Hours","text":"<p>We host virtual, drop-in office hours every Wednesday from 2:00-4:00pm. Drop by to visit with our consultants to ask any questions you have about using HPC resources. It might be a bunch of getting started questions, or you might want to share your screen to walk us through a particular problem you're hung up on. We have private spaces for one-on-one consults. You can join us in Gather Town at this link. </p> <p>If you have never used Gather Town before and would like additional information, check out this page.</p>"},{"location":"support_and_training/consulting_services/#visualization-consulting","title":"Visualization Consulting","text":"<p>Visualization Consulting is a service that allows researchers to create graphical representations of their data. These computer-generated images and animations allow researchers to visually analyze that data and see the results of changing specific parameters. Services include consultation, demonstration, and training for high-resolution visualization output and simulation. </p> <p>Visualization consulting can be requested by contacting vislab-consult@list.arizona.edu</p> <p>We also offer drop-in hours at the Main Library. https://libcal.library.arizona.edu/event/10712342</p>"},{"location":"support_and_training/external_resources/","title":"External Resources","text":"<p>Explore a number of resources beyond our center's offerings, from local community events to comprehensive software support and opportunities for accessing national supercomputing clusters. We've curated a collection of external organizations below to assist with education, research, and community building.</p>"},{"location":"support_and_training/external_resources/#access","title":"ACCESS","text":"<p>ACCESS has succeeded the XSEDE program which concluded in August 2022.</p> <p>ACCESS is a comprehensive NSF-funded initiative designed to facilitate usage of national supercomputing resources for researchers, educators, and students. These resources encompass state-of-the-art computing clusters, data and storage services, scientific applications, educational materials, workshops, and dedicated support services.</p> <p>For University of Arizona (UArizona) affiliates, ACCESS presents an opportunity to leverage additional computing power and resources beyond what is available through UArizona's existing High-Performance Computing (HPC) infrastructure. Notable advantages and use cases for UArizona affiliates include:</p> <ul> <li>Researchers requiring extended computing time or additional computational resources beyond the capacity of UArizona's HPC systems.</li> <li>Users seeking computational resources for Windows-based software applications. For instance, Jetstream2, a national resource, offers Windows Virtual Machines (VMs) to accommodate such needs.</li> <li>Educators looking to harness GPU resources for classroom instruction. While UArizona's HPC facilities do provide GPU resources, high demand may lead to prolonged wait times, making ACCESS an appealing alternative.</li> </ul> <p>For more information on ACCESS, see: https://access-ci.org/</p>"},{"location":"support_and_training/external_resources/#ansys","title":"Ansys","text":"<p>For help with local installations, contact the College of Engineering IT services: support@engr.arizona.edu</p> <p>Ansys-specific support (debugging, questions about usage, etc) is available through PADT: support@padtinc.com</p> <p>To report license connection issues, contact: HPC consulting</p>"},{"location":"support_and_training/external_resources/#code-commons","title":"Code Commons","text":"<p>Code Commons provides a physical space for community and collaboration. Join to share experience, learn, mentor, discover opportunities, and work on your programming projects in the presence of others doing the same. Held every Wednesday from 2:00-6:00pm at the UArizona Library in the CATalyst Data Studios. For more information, see: https://codecommons.net/</p>"},{"location":"support_and_training/external_resources/#cyverse","title":"CyVerse","text":"<p>CyVerse provides life scientists with powerful computational infrastructure to handle huge datasets and complex analyses, thus enabling data-driven discovery. Their extensible platforms provide data storage, bioinformatics tools, image analyses, cloud services, APIs, and more: http://www.cyverse.org/about</p> <p>CyVerse is funded by the National Science Foundation\u2019s Directorate for Biological Sciences. They are a dynamic virtual organization led by the University of Arizona to fulfill a broad mission that spans our partner institutions: Texas Advanced Computing Center, Cold Spring Harbor Laboratory, and the University of North Carolina at Wilmington.</p>"},{"location":"support_and_training/external_resources/#neuroimaging-core","title":"Neuroimaging Core","text":"<p>Are you part of the neuroimaging community and interested in using HPC? The UArizona Neuroimaging Core has excellent documentation to help you get started.</p>"},{"location":"support_and_training/external_resources/#open-science-framework","title":"Open Science Framework","text":"<p>The OSF is a free, open source service maintained by the Center for Open Science. Here are a few things you can do with the OSF:</p> <ul> <li> <p>Store your filesArchive your materials, data, manuscripts, or anything else associated with your research during the research process or after it is complete.</p> </li> <li> <p>Affiliate your projects with your institutionAssociate your projects with the University of Arizona which is a member. They will be added to UArizona's central commons, improving discoverability of your work and fostering collaboration.</p> </li> <li> <p>Share your workKeep your research materials and data private, make it accessible to specific others with view-only links, or make it publicly accessible. You have full control of what parts of your research are public and what remains private.</p> </li> <li> <p>Register your researchCreate a permanent, time-stamped version of your projects and files. Do this to preregister your design and analysis plan to conduct a confirmatory study, or archive your materials, data, and analysis scripts when publishing a report.</p> </li> <li> <p>Make your work citableEvery project and file on the OSF has a permanent unique identifier, and every registration can be assigned a DOI. Citations for public projects are generated automatically so that visitors can give you credit for your research.</p> </li> <li> <p>Measure your impactYou can monitor traffic to your public projects and downloads of your public files.</p> </li> <li> <p>Connect services that you useGitHub, Dropbox, Google Drive, Box, Dataverse, figshare, Amazon S3, ownCloud, Bitbucket, GitLab, OneDrive, Mendeley, Zotero. Do you use any of these? Link the services that you use to your OSF projects so that all parts of your project are in one place.</p> </li> <li> <p>CollaborateAdd your collaborators to have a shared environment for maintaining your research materials and data and never lose files again.</p> </li> </ul> <p>Learn more about the OSF on their Guides page, or email contact@osf.io with questions for support.</p>"},{"location":"support_and_training/external_resources/#research-bazaar","title":"Research Bazaar","text":"<p>Want to get involved with the Tucson coding community? ResBaz AZ offers weekly events that brings together scientists, software engineers, and enthusiasts of all skill levels. Additionally, an annual Research Bazaar is held each spring hosting research computing workshops and career panels: https://researchbazaar.arizona.edu/</p>"},{"location":"support_and_training/external_resources/#uarizona-data-science","title":"UArizona Data Science","text":"<p>Have some code-specific, data science, or related questions? Consider joining the UArizona Data Science Slack channel: https://jcoliver.github.io/uadatascience-slack/user-guide.html</p>"},{"location":"support_and_training/external_resources/#data-cooperative","title":"Data Cooperative","text":"<p>Many non HPC existing services available through the main library and the great folks at. https://data.library.arizona.edu/</p> <p></p>"},{"location":"support_and_training/faqs/","title":"FAQs","text":""},{"location":"support_and_training/faqs/#faqs","title":"FAQs","text":"<p>Welcome to our Frequently Asked Questions page. The FAQs are organized by topic for ease of navigation. Use the sidebar on the left to find your topic. If you cannot find an entry related to your question, please let us know and we will be happy to add it. </p>"},{"location":"support_and_training/faqs/#account-access","title":"Account Access","text":""},{"location":"support_and_training/faqs/#registration","title":"Registration","text":"How do I create an account? <p>A step by step guide is available in our Account Creation page. </p> I'm a faculty member. How do I sponsor users/add members to my HPC research group? <p>Faculty members who manage their own HPC groups can use our user portal to add members to their group. If the user does not have an HPC account, this will sponsor them for access. You may also add existing HPC members to your group. We have instructions in our Research and Class Groups page that detail the process with screenshots.</p> I'm a faculty member and have received a sponsorship request email from someone. How can I approve them for access? <p>If you have received a sponsorship request email, you can click the link at the bottom to approve it. This will redirect you to the user portal and will automatically add the requestor to your HPC group. No further action is required after clicking the link.</p> <p>If there are any issues with the sponsorship process, you may add the user manually through our user portal by following the instructions in our Research and Class Groups guide.</p> I'm leaving the university/retiring/not affiliated with the university, can I maintain/receive access to HPC? <p>Yes, if you are a former university affiliate or campus collaborator participating in research, you may register as a Designated Campus Colleague (DCC). Once your DCC status has been approved, you will receive a NetID+ which you may use to create an HPC Account.</p> <p>If you are a graduating student or leaving your position of employment, see our policies page on affiliation loss for more details.</p>"},{"location":"support_and_training/faqs/#login-issues","title":"Login Issues","text":"Why can't I log in? <p>There are many reasons you may be having issues logging in. A possible list of reasons may include:       <ul> <li>You haven't created an account yet or you have not yet been sponsored.</li> <li>You aren't using two-factor authentication (NetID+).</li> <li>You need to wait 15 minutes. If you just created your account, it takes time before you can log in.</li> <li>You're trying to connect using ssh <code>&lt;netid&gt;@login.hpc.arizona.edu</code>. This will not work. Instead, use: <code>ssh &lt;netid&gt;@hpc.arizona.edu</code>.</li> <li>You're using <code>&lt;netid&gt;@hpc.arizona.edu</code> or <code>&lt;netid&gt;@email.arizona.edu</code> as your username in PuTTY. Instead, use only your NetID.</li> <li>You've entered your password incorrectly too many times. After multiple failed password entries, the system will place a 60 minute ban on your account for security reasons. Your account will automatically unlock after 60 minutes. Attempting to log in before your account unlocks will reset the timer. </li> </ul></p> Why doesn't my password show after the prompt when logging in'? <p>Linux systems do not display character strokes while entering your password which can make it look like the SSH client is frozen. Even though it doesn't appear that anything is happening, the system is still logging your input. To proceed, type your password at the prompt and press enter.</p> I've forgotten my password, how can I reset it? <p>HPC uses the same NetID login credentials as all UArizona services. If you need to reset your NetID password you can do so using the NetID portal.</p> Why am I getting \"You do not appear to have registered for an HPC account\"? <p>If you have just registered for an HPC account, you need to wait a little while for the request to propagate through the University systems (this can take up to an hour). </p> <p>If you have not yet received sponsorship for HPC access, you will need a faculty member/PI to sponsor you. If you are a PI, ensure you have sponsored yourself by adding yourself to your group. Instructions on account creation can be found in our Account Creation page.</p> Why am I getting \"permission denied\" when I try to log in? <p>You need an HPC account - see our Account Creation page for details.  Once you've done that, you'll need to wait a little while to log in. If your PI hasn't already added you to their group, you'll need to wait for that as well.</p> Why am I unable to log in with the error \"Your account is disabled and cannot access this application. Please contact your administrator.\"? <p>This error shows up when your NetID has been locked, usually due to multiple failed login attempts when trying to access university services. Contact 24/7 to unlock your account: https://it.arizona.edu/get-support</p> Why am I getting the message \"incorrect password\" when I try to log in? <p> <ul> <li>Ensure you are using the correct password. Sometimes typing your password into a plain text file and copying/pasting it into the terminal can help.</li> <li>You need to wait about 15 minutes after your account is approved for the account to be available</li> <li>You must enroll in NetID+. Depending on the application you use to log in, you may not get the typical NetID+/Duo menu of options, or an error message indicating this is your problem</li> </ul> </p>"},{"location":"support_and_training/faqs/#jobs-and-scheduling","title":"Jobs and Scheduling","text":"Why isn't my job running? <p>     There are a few reasons your job may not be running, check below for some ideas on diagnosing the issue:         <ul> <li> Run squeue: On the command line, run <code>squeue --job=&lt;jobid&gt;</code> and see if there is anything listed under the column <code>(REASON)</code>. This may give an idea why your job is stuck in queue. We have a table in our Slurm documentation that describes what each Reason code means.</li> <li>Resource availability: Due to the number of HPC users, it may not always be possible to run a submitted job immediately. If there are insufficient resources available, your job will be queued and it may take up to a few hours for it to begin executing.</li> <li>Allocation exhausted: Your group may have run out of standard hours. You can check your allocation using the command <code>va</code>.</li> <li>Resource usage limitations: Your group/job has reached a resource usage limit (e.g., number of GPUs that may be used concurrently by a group, or a job has requested more than the 10 day max walltime). Try running <code>job-limits &lt;group_name&gt;</code> to see what limits you're subject to and if there are any problem jobs listed. For more information on limits, see Job Limits documentation.</li> <li>Requesting rare resources: You may be requesting a rare resource (e.g., 4 GPUs on a single node on Puma or a high memory node).         <ul> <li>If you are requesting a single GPU on Puma and are frustrated with the wait times, you might consider checking if Ocelote will work for your analyses. There are more GPU nodes available on that cluster, typically with shorter wait times. See our Compute Resources page for more information on the resources available by cluster.</li> <li>If you are trying to run a job on a standard node and have been waiting for a very long time, try checking its status using <code>job-history &lt;jobid&gt;</code>. If you see <code>Allocated RAM/CPU</code> above 5 GB on Puma or above 6 GB on Ocelote, then you are queued for the high memory node which can have very long wait times. To queue for a standard node, cancel your job and check that your script has the correct ratios.</li> </ul></li> </ul> </p> My job has a Reason code when I check it with <code>squeue</code>. What does this mean? <p>If your job is in queue, sometimes Slurm will give you information on why it's not running. This may be for a number of reasons, for example there may be an upcoming maintenance cycle, your group's allocation may be exhausted, you may have requested resources that surpass system limits, or the node type you've requested may be very busy running jobs. We have a list of reason codes in our Monitoring Jobs and Resources page that will give more comprehensive information on what these messages mean. If you don't see the reason code listed, contact our consultants.     </p> Why do my jobs keep getting interrupted? <p>If your jobs keep stopping and restarting, it's likely because you are using Windfall. Windfall is considered lower priority and is subject to preemption by higher priority jobs. Before submitting a job to Windfall, consider using your group's allotted monthly hours first. Jobs using Standard hours will queue for a shorter period of time and will not be interrupted. You can check your group's remaining hours using the command <code>va</code>. To see more information on your allotted hours and the different job queues, see our page on compute allocations.If your job is not using the Windfall partition and is being interrupted, contact our consultants.      </p> Can I run programs on the login nodes? <p>No, software to run applications is not available on the login nodes. To run/test your code interactively, start an interactive session on one of the system's compute nodes. Processes running on the login nodes are subject to termination if we think they are affecting other users. Think of these as 'submit' nodes where you prepare and submit job scripts.      </p> Can I get root access to my compute nodes? <p> Unfortunately, that is not possible. The compute nodes get their image from the head node and have to remain the same. If you need to install software, for example, you can install the software locally in your account. See this example. Another option is to use containers as part of your workflow.     </p> Can I ssh to compute nodes? <p>Slurm will let you <code>ssh</code> to nodes that are assigned to your job, but not to others.      </p> Why am I getting out of memory errors? <p>         There are a few reasons you might get out of memory errors:         <ul> <li>You're using <code>-c &lt;N&gt;</code> to request CPUs. Based on the way our scheduler is set up, this will reduce the memory allocation for your job to 4 MB. To solve this, change your CPU request by either setting <code>--ntasks=&lt;N&gt;</code> or <code>--ntasks=1 --cpus-per-task=&lt;N&gt;</code>.</li> <li>You may not have specified the number of nodes required for your job. For non-MPI workflows, if Slurm scatters your CPUs across multiple nodes, you will only have access to the resources on the executing node. Explicitly setting <code>--nodes</code> in your script should help, e.g.:              <pre><code>#SBATCH --nodes=1</code></pre> </li> <li>You may not have allocated enough memory to your job. Try running <code>seff &lt;jobid&gt;</code> to see your memory usage. You may consider using memory profiling techniques, allocating more CPUs, or using a high memory node.</li> </ul> </p> Why shouldn't I use Windfall with OnDemand? <p>         Windfall jobs can be preempted by a higher priority queue. Each session creates an interactive job on a node and it is unsatisfactory to be dumped in the middle of that session. A desktop session would have the same unpleasant result.  Windfall can be used if you do not have enough standard time left. Consider though that a one hour session using one compute core only takes up 1 CPU hour.      </p> My interactive terminal session has been disconnected, can I return to it? <p>         No, unfortunately when an interactive job ends it is no longer accessible. This applies to both OnDemand sessions and those accessed via the command line. We recommend using the standard partition rather than windfall when running interactive jobs to prevent preemption.      </p> How can I disable core dumps? <p>         To prevent core dump files from being generated in jobs (which can be quite large and may fill up your working directory), add the following to your batch script:         <pre><code>ulimit -c 0</code></pre> </p> Is there a way to automatically cancel jobs if they encounter errors during execution? <p>         Yes, you can use a pipefail. This is a way to automatically kill a job after an error without moving on to any subsequent steps. To use this, add the following to the beginning of your batch script:         <pre><code>set -oe pipefail</code></pre> </p> Why am I getting the error <code>QOSGrpSubmitJobsLimit</code> when I try submitting my job? <p>     If you're trying to submit a job and get:     <pre><code>sbatch: error: QOSGrpSubmitJobsLimit\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)</code></pre>     check that you're including a <code>--qos</code> directive in your job request. This is necessary to use high priority and qualified hours. Check our Slurm Batch Directives page for specifics on how to do this.     </p>"},{"location":"support_and_training/faqs/#command-line","title":"Command Line","text":"Why aren't common commands working? <p>There may be a few reasons for this. First, make sure your shell is set to Bash. If your shell is not set to Bash, contact our consultants so that they can reset it for you.          If your shell is set to Bash, double-check that you haven't changed, overwritten, or aliased anything important either your <code>~/.bashrc</code> or <code>~/.bash_profile</code>. E.g., unsetting your <code>PATH</code>, aliasing <code>.</code>, and so on will corrupt your environment and prevent you from interacting with HPC normally.      </p> Why is my terminal glitching (e.g. Ctrl+a puts me in the middle of my command prompt)? <p>When you log into HPC, the variable <code>$COMMAND_PROMPT</code> is set to display your current cluster (e.g. <code>(puma)</code>). Sometimes this can cause formatting problems. If you'd prefer to modify your <code>$PS1</code> (command prompt variable) instead, you can add the following to your <code>~/.bashrc</code>:      <pre><code>if [ -n \"${PROMPT_COMMAND}\" -a -r /usr/local/bin/slurm-selector.sh ]; then\n    SavePS1=${PS1}\n    Cur_Cluster=$(eval ${PROMPT_COMMAND} 2&gt;/dev/null)\n    PS1=\"${Cur_Cluster}${SavePS1}\"\n    unset PROMPT_COMMAND\n    for c in puma ocelote elgato; do\n      alias ${c}=\"PS1=\\\"(${c}) ${SavePS1}\\\"; . /usr/local/bin/slurm-selector.sh ${c}; unset PROMPT_COMMAND\"\n    done\n    unset Cur_Cluster SavePS1\n  fi\n    </code></pre> </p> What are dot files and what is a <code>~/.bashrc</code>? <p>Files that start with a dot, i.e. a <code>.</code>, are hidden when executing <code>ls</code> without additional options. If you use <code>ls -la</code>, that will show you all the files that exist, both standard and hidden. Dot files are generally important configuration files so it's important to be careful with their contents and deleting them.           Your bashrc is a specific dot file that lives in your home directory (<code>~</code> is just shorthand for your home) and defines your environment every time you log in. Any commands you add to that file will be run whenever you access the system. This means, if you have common aliases you like to use, paths you like exported in your environment, etc., these can be added to your bashrc to avoid the headache of having to define them in every session.          For more information on working with hidden files, see our Linux cheat sheet page."},{"location":"support_and_training/faqs/#maintenance","title":"Maintenance","text":"Why are my jobs sitting in queue for so long after maintenance? <p>   If your jobs are sitting in queue for a long time after a system maintenance period, it's likely because it was a rolling maintenance cycle. During rolling maintenance, all nodes are put into a \"drain\" state. This means they stop accepting any new jobs until all the jobs running on them complete. Once they are empty, they are taken offline, updated, rebooted, and then put back online to accept new jobs. Because job runtimes can be up to 10 days, it may take up to 10 days for job queues to return to normal.    For more information on maintenance, see our maintenance documentation.    </p>"},{"location":"support_and_training/faqs/#open-ondemand-issues","title":"Open OnDemand Issues","text":""},{"location":"support_and_training/faqs/#website-access","title":"Website Access","text":"Why am I getting a message saying I'm not sponsored when trying to log in? <p>         If you are trying to log in to Open Ondemand and are seeing the following:          <ul> <li>You have not yet been sponsored by a faculty member. See our Account Creation page for instructions on getting registered for HPC.</li> <li>If you are already registered for HPC, this may be a browser issue. Try logging in again in an incognito session or different browser to test. If this succeeds, clearing your browser's cookies should help.</li> </ul> </p> Why am I getting a message saying \"Home directory not found\"? <p>If you're trying to log into Open OnDemand and are seeing the following, it means your home directory has not yet been created:</p> <p>      Your home directory is created the first time you log into our system using a command line utility (such as the terminal, PuTTY, etc.). To create your home directory, either click the button Open Shell to create home directory on the page, or log in via a command line utility, then try logging into Open OnDemand again.     </p> Why am I getting a \"Bad Request\" message when trying to connect? <p>         If you are trying to log in to Open Ondemand and are seeing the following:                   this may be a browser issue. Try logging in again in an incognito session or different browser to test. If this succeeds, clearing your browser's cache should help.     </p> Why am I getting an error saying \"We're sorry, but something went wrong\" when trying to log in? <p>         If you are trying to log in to Open OnDemand and are seeing the following:                   check your storage usage in your home directory. You can do this by logging into HPC in a terminal session and using the command <code>uquota</code>. If your storage usage is &gt;50 GB, OnDemand cannot create the temporary files necessary to give access to the website. Try clearing out some space in your home and then logging back into OnDemand.     </p>"},{"location":"support_and_training/faqs/#applications","title":"Applications","text":"Why are my Desktop sessions failing with 'Could not connect to session bus: failed to connect to socket /tmp/dbus-'? <p>         If you're seeing:                   when trying to connect to an interactive desktop session, the likely culprit is Anaconda. For a permanent solution, you can run the following command from an interactive terminal session:         <pre><code>\n  conda config --set auto_activate_base false\n        </code></pre>         This will prevent conda from auto-activating when you first log in and allow you to have more control over your environment. When you'd like to activate Anaconda, run <code>conda activate</code>.      </p> Why are my Desktop sessions freezing with: 'noVNC encountered an error: Incomplete zlib block'? <p>        If you're seeing the following error pop up when using an OnDemand Desktop session:               Check the compression value set for your job. If it has been set to zero, increase the value to something nonzero and retry your session.               To check and modify your compression, go to your job tile in ood.hpc.arizona.edu under My Interactive Sessions and check the lower left-hand section.        </p>"},{"location":"support_and_training/faqs/#software-and-modules","title":"Software and Modules","text":"Are any software modules loaded by default? <p>         Yes, when you start an interactive terminal session or submit a batch script, the modules ohpc, gnu8, openmpi3, and cmake are automatically loaded. If your code uses Intel compilers, you will want to manually unload gnu8 and openmpi3 to prevent conflicts.                  The exception: If you are working in a terminal in an Open OnDemand interactive desktop session, nothing is loaded by default and you will need to manually load any necessary modules. To start, always use <code>module load ohpc</code> </p> What executables are available when I load a module? <p>         Load the module, find the path to the executable by checking the <code>$PATH</code> variable, then list the contents.  For example:         <pre><code>\n  module load lammps\n  echo $PATH\n  ls /opt/ohpc/pub/apps/lammps/3Mar20/bin\n  lmp_mpi\n        </code></pre>         Alternatively, use the command <code>module show &lt;software&gt;</code> to see all the environment variables that are set at load time.      </p> Why am I getting \"command: module not found\"?  <p>         There are a few different possibilities:         <ul> <li>You are not in an interactive session. Modules are not available on the login nodes. You may request an interactive session by using the command <code>interactive</code>. </li> <li>Your shell is not set to bash. If this is the case, contact our consultants so that they can reset it for you.</li> <li>You have modified or deleted your <code>~/.bashrc</code>. If this is the case, open (if the file exists) or create and open (if the file is missing) the file .bashrc in your home directory and add the lines:</li> <pre><code>\n  if [ -f /etc/bashrc ]; then\n          . /etc/bashrc\n  fi\n            </code></pre> </ul> </p> How do I access the module for Gaussian or Gaussview? <p>       You need to belong to a special group called g03.  You can request to be added by submitting a help ticket. This is a constraint in Gaussian that other modules do not have.     </p> How can I maximize my software performance on Puma? <p>         If you are able to compile your software you can take advantage of most of the AMD Zen architecture.     </p> Compiler Arch-Specific Arch-Favorable GCC 9 <code>-march=znver2</code> <code>-mtune=znver2</code> LLVM 9 <code>-march=znver2</code> <code>-mtune=znver2</code> Is the Intel compiler faster than GCC on Puma? <p>       Intel compilers are optimized for Intel processors. There is some debate around the concept of unfair CPU dispatching in Intel compilers. By default, software on the HPC clusters is built with GCC (on Puma it is GCC 8.3).  This is in keeping with our preference for community software.     </p> I've been using an older version of Apptainer (formerly Singularity), why isn't available anymore? <p>       Prior versions of Apptainer are routinely removed since only the latest one is considered secure. Notify the consultants if you need help with transition to the current version. Apptainer is installed on the operating systems of all compute nodes so does not need to be loaded with a module.      </p> Can I use Windows applications on HPC? <p>       Unfortunately, Windows applications can't be run on HPC. However, AWS has been used successfully for Windows software with GPU needs. It\u2019s easy to set up, cost effective, and very scalable. Amazon also has a cloud credit for research program available.              You may also consider trying Jetstream2, a national resource where you can create and use Windows virtual machines. More information can be found here: https://jetstream-cloud.org/ </p> How do I install this R package/Why can't I install this R package? <p>         R installations can sometimes be frustrating. We have instructions for how to set up a usable R environment, how to diagnose and troubleshoot problems, and steps to help with known troublesome packages documented in in our Using and Customizing R Packages section.      </p> How do I install Python packages? <p>        You can install python packages locally using either a virtual environment or a local conda environment.      </p> How do I access custom Python packages from an OOD Jupyter session? <p>     You can install packages and make them available by first creating a virtual environment or conda environment, then setting up a custom Jupyter kernel. See instructions in our Python or Anaconda documentation for details.      </p> How do I take advantage of the Distributed capability of Ansys? <p>       Ansys has the Distributed capability built in to increase performance. Ansys uses the Intel compiler and so uses Intel MPI.  By default, we load OpenMPI, so you will need to do this:        <pre><code>\n  module unload gnu8 openmpi3\n  module load intel\n  module load ansys\n      </code></pre> </p> How do I initialize <code>micromamba</code> in Slurm batch scripts? <p>         You should initialize <code>micromamba</code> only once, and from an interactive session. If you are getting <code>micromamba</code> initialization errors when submitting jobs with Slurm batch scripts, then see <code>micromamba</code> in Batch Jobs for solutions.     </p>"},{"location":"support_and_training/faqs/#storage","title":"Storage","text":""},{"location":"support_and_training/faqs/#general","title":"General","text":"Do you allow users to NFS mount their own storage onto the compute nodes? <p>          No. We NFS mount storage across all compute nodes so that data is available independent of which compute nodes are used. See our page on transferring data for more information.     </p> I can't transfer my data to HPC with an active account. What's wrong? <p>         After creating your HPC Account, your home directory will not be created until you log in for the first time. Without your home directory, you will not be able to transfer your data to HPC. If you are struggling and receiving errors, sign into your account either using the CLI through the bastion or logging into Open OnDemand and then try again.     </p> I accidentally deleted files, can I get them back? <p>         It's unlikely. Backups are not made and anything deleted is permanently erased. If you have deleted an important file, reach out to our consultants as soon as possible and they may be able to retrieve it, however, this is not guaranteed.                   To ensure your data are safe, we strongly recommend:         <ul> <li>Making frequent backups, ideally in three places and two formats. Helpful information on moving data can be found on our page Transferring Data.</li> <li>Use <code>rm</code> and <code>rm -r</code> with caution as these commands cannot be undone! Consider using <code>rm -i</code> when removing files/directories. The <code>-i</code> flag will prompt you to manually confirm file removals to make really sure they can be deleted.</li> <li>You can open a support ticket to request assistance.  Files that are deleted may not have been removed from the storage array immediately (though this is not guaranteed), don't wait more than a few days.</li> </ul> </p> My home directory is full, what's using all the space? <p>         If your home directory is full and you can't find what is taking up all the space, it's possible the culprit is a hidden file or directory. Hidden objects are used for storing libraries, cached singularity/apptainer objects, saved R session, Anaconda environments, configuration files, and more. It's important to be careful with hidden, or \"dot\", files since they often control your environment and modifying them can lead to unintended consequences.         To view the sizes of all the objects (including hidden) in your home, one quick command is <code>du -hs $(ls -A ~)</code>, for example:         <pre><code>\n            [netid@junonia ~]$ du -hs $(ls -A ~)\n            32K     Archives\n            192M    bin\n            4.7G    Software\n            46M     .anaconda\n            1.9M    .ansys\n            4.0K    .apptainer\n            16K     .bash_history\n            4.0K    .bash_logout\n            4.0K    .bash_profile\n            12K     .bashrc\n            20M     ondemand\n        </code></pre> </p> I'd like to share data I have stored on HPC with an external collaborator, is this possible? <p>         Unfortunately, without active university credentials it is not possible to access HPC compute or storage resources. External collaborates who need ongoing access may apply for Designated Campus Colleague, or DCC, status. This is a process done through HR and will give the applicant active university credentials allowing them to receive HPC sponsorship.                  Otherwise, data will need to be moved off HPC and made available on a mutually-accessible platform. This may include (but is not limited to): Google Drive, AWS S3, Box, and CyVerse's Data Store.     </p>"},{"location":"support_and_training/faqs/#xdisk","title":"xdisk","text":"Can someone other than a PI manage a group's xdisk? <p>       Yes, a PI can add a trusted group member as a delegate by following the instructions in our Research and Class Groups page. Once a group member is added as a delegate, they can manage the group's xdisk allocation on behalf of their PI in the user portal.     </p> Who owns our group's /xdisk? <p>         A group's PI owns the xdisk allocation. By default, your PI has exclusive read/write/execute privileges for the root folder <code>/xdisk/&lt;pi_netid&gt;</code>.     </p> Can a PI make their /xdisk writeable for their whole group?  <p>         By default, members of a research group only have write access to their subdirectories under <code>/xdisk/&lt;pi_netid&gt;</code>. If they so choose, a PI may allow their group members to write directly to that location by running the following command:         <pre><code>\n  chmod g+w /xdisk/&lt;pi_netid&gt;\n        </code></pre>         For more information on Linux file permissions, see our cheat sheet.      </p> Where can group members store their files? <p>         When an xdisk allocation is created, a subdirectory is automatically generated for and owned by each individual group member. Group members can access their individual spaces by:         <pre><code>\n  cd /xdisk/&lt;pi_netid&gt;/&lt;netid&gt;\n        </code></pre>         If a user joins the group after the xdisk was created and <code>/xdisk/&lt;pi_netid&gt;</code> is not writeable for group members, contact our consultants and they can create one.     </p> A group member's directory isn't in our /xdisk, how can we add it? <p>         Typically when an xdisk allocation is created, it will automatically generate a directory for each group member. In the unlikely event that it doesn't or, more commonly, a group member is added after the allocation has been created, contact our consultants and they can create one.      </p> Do we need to request an individual allocation within the /xdisk for each user in our group? <p>         No, the full xdisk allocation is available for every member of the group. It's up to group members to communicate with one another on how they want to utilize the space.     </p> Why am I getting xdisk emails?  <p>          xdisk is a temporary storage space available to your research group. When it's close to its expiration date, notifications will be sent to all members of your group..     </p> Why am I getting \"/xdisk allocations can only be authorized by principal investigators\"?  <p>         xdisks are managed by your group's PI by default. This means if you want to request an xdisk or modify an existing allocation (e.g., extending the time limit or increasing the storage quota), you will need to consult your PI. Your PI may either perform these actions directly or, if they want to delegate xdisk management to a group member, they may do so by following the instructions under Delegating Group Management Rights.     </p> How can we modify our xdisk allocation? <p>         To modify your allocation's time limit or storage quota, your PI can either do so through the Web Portal under the Storage tab, or via the command line. If your PI would like to delegate management rights to a group member, they may follow the instructions under Delegating Group Management Rights. Once a group member has received management rights, they may manage the allocation through our web portal.     </p> Why am I getting \"xdisk: command not found\"? <p>         If you're getting errors using <code>xdisk</code> commands in a terminal session, check that you are on a login node. If you are on the bastion host (hostname: <code>gatekeeper</code>), are in an interactive session, or are on the filexfer node, you won't be able to check or modify your xdisk. When you are on a login node, your terminal prompt should show the hostname junonia or wentletrap. You can also check your hostname using the command <code>hostname</code> </p> Why am I getting errors when trying to extend my allocation? <p>         If you're trying to extend your group's allocation but are seeing something like:         <pre><code>\n  (puma) [netid@junonia ~]$ xdisk -c expire -d 1\n  invalid request_days: 1\n        </code></pre>         for every value you enter, your xdisk has likely reached its maximum time limit. To check, have a delegate or PI go to portal.hpc.arizona.edu, click Manage XDISK, and look at the box next to Duration. If you see 300, your allocation cannot be extended further.        If your allocation is at its limit, you will need to back up your data to external storage (e.g., a local machine, lab server, or cloud service). Once your xdisk has been removed (either by expiring or through manual deletion), you can immediately create a new allocation and restore your data. Detailed xdisk information can be found on our HPC High Performance Storage page. You may also want to look at our page on Transferring Data.     </p> Can we keep our xdisk allocation for more than 300 days? <p>         No, once an xdisk has reached its time limit it will expire. It's a good idea to start preparing for this early by making frequent backups and paying attention to xdisk expiration emails.      </p> What happens when our xdisk allocation expires? <p>         Once an xdisk expires, all the associated data are deleted. Deleted data are non-retrievable since HPC is not backed up. It's advised to keep frequent backups of your data on different platforms, for example a local hard drive or a cloud-based service, or (even better) both! Check our Storage documentation for more information on alternative storage offerings.     </p> What's the best way to backup/transfer our data before our xdisk expires? <p>         Before your group's xdisk expires, you'll want to make an external backup of anything you need to keep. External storage options include personal computers, lab servers, external hard drives, or cloud services such as AWS.         If you're moving large quantities of data, Globus is a great option. We have instructions in our Globus documentation for setting up and using this software.        We strongly recommend making archives (.tar, .zip, files etc.) of large directories prior to transferring them off the system. In general, transfer software struggles with moving many small files and performs much more efficiently moving fewer large files. You will get the better transfer speeds (sometimes by orders of magnitude) if you compress your files prior to transferring them. This can be done on our filexfer node which is designed for large file management operations (hostname: <code>filexfer.hpc.arizona.edu</code>).      </p> Once our xdisk expires, can we request a new one? <p>         Yes, a new xdisk may be requested immediately after the old partition expires. Data, however, may not be transferred directly from the old partition to the new one.      </p> Can a PI have more than one xdisk active at a time? <p>         No, only one xdisk may be active per PI at a given time.      </p>"},{"location":"support_and_training/faqs/#rental","title":"Rental","text":"How do I request rental storage? <p>          Rental storage can be requested by PIs through the user portal. A guide with screenshots can found in our rental storage documentation.      </p> Why can't I see my rental allocation when I log into HPC? <p>         Rental allocations are not mounted on the login or compute nodes. You can access your allocation by logging in to our data transfer nodes: <code>filexfer.hpc.arizona.edu</code> </p> Can I analyze data stored in /rental directly in my jobs? <p>         No, rental storage is not mounted on the HPC login or compute nodes which means jobs cannot access <code>/rental</code> directly. Data stored in <code>/rental</code> need to be moved to <code>/home</code>, <code>/groups</code>, or <code>/xdisk</code> to be available.      </p>"},{"location":"support_and_training/faqs/#r-das","title":"R-DAS","text":"If my VPN connection is dropped, will my connection survive?  <p>         Yes, if you reconnect to the VPN, your connection will still be available.      </p> Why am I getting \"There was a problem connecting to the server\"? <p>       This error is seen if you are not connected to the University VPN.     </p> Can I access R-DAS for HPC usage? <p>         R-DAS is not mounted on the HPC compute nodes or login nodes, and is not meant for running computations. But you can follow the steps in our R-DAS documentation to share data between your R-DAS allocation and your HPC storage (<code>/home</code>, <code>/groups</code>, <code>/xdisk</code>):      </p>"},{"location":"support_and_training/faqs/#tier-2-aws","title":"Tier 2 AWS","text":"Who can submit a Tier 2 storage request? <p>       A PI must submit their group's storage request. The exception to this is if a group member has been designated xdisk/storage delegate. Delegates may submit a request on behalf of their PI by switching users in the user portal.     </p> How long is the turnaround time to between submitting the storage request and obtaining an S3 account? <p>        In most cases it will be in one business day.     </p> What is the pricing for S3? <p>       You should check the Amazon site. As of March 2022:       <ul> <li>Frequent Access Tier, First 50 TB / Month $0.023 per GB</li> <li>Frequent Access Tier, Next 450 TB / Month $0.022 per GB</li> <li>Frequent Access Tier, Over 500 TB / Month $0.021 per GB</li> </ul> </p> Can I move my data directly to Glacier if I know it is archival. That way I can skip the S3 expenses? <p>       Not in the first release, but potentially as a future offering.     </p> Is the monthly billing based on average usage? <p>        Yes. The capacity used is recorded daily and the billing is a monthly average.     </p> What is a KFS number? <p>        It is used for accounting purposes and used by your Department's finance specialist. If you are unsure what your KFS number is, contact your department's financial services office.     </p> Can I view my storage in each of S3, Glacier and Deep Glacier? <p>        Yes, you can use the CLI (command line interface) for information about your usage     </p> Can I limit the amount of data that goes into S3 so I can control the expense? <p>        No, but you can track the usage and remove any data that should not be there.     </p> What is the difference between Glacier and Deep Glacier? <p>        Glacier is effectively large, slow disks and Deep Glacier is tape storage.     </p> Can I have multiple S3 buckets associated with my account? <p>       Yes     </p> What why am I receiving an email: \"ALARM: \"ua-rt-t2-netid capacity growth alarm\" in US West (Oregon)\"? <p>       This alert is sent to notify you whenever your storage usage grows by 10% relative to the previous week. This ensures that you're aware of any unintended spikes and the potential resulting costs.      </p>"},{"location":"support_and_training/faqs/#transfers","title":"Transfers","text":""},{"location":"support_and_training/faqs/#hpc","title":"HPC","text":"Why am I getting the error <code>Received message too long</code> when I try to use scp/sftp? <p>         Check your <code>~/.bashrc</code> or <code>~/.bash_profile</code> to see if they are printing any output to the terminal. Transfer software like SCP or SFTP require a \"silent\" terminal to work successfully. If you find any <code>echo</code> or <code>printf</code> statements, comment them out and retry your transfer.      </p>"},{"location":"support_and_training/faqs/#tier-2-aws_1","title":"Tier 2 AWS","text":"What does this file transfer error mean: \"The operation is not valid for the object's access tier\"? <p>       This error indicates your data have migrated to an archival storage tier (Glacier or Deep Glacier). They will first need to be restored to the Standard access tier to make them downloadable.     </p> Are there any limits on uploads, e.g. may transfer size/day? <p>       The max file size is 5 TB based on Amazon's official documentation.     </p> Are there charges for data movement? <p>        You will not be charged for data ingress, egress or other operations.     </p>"},{"location":"support_and_training/faqs/#secure-services","title":"Secure Services","text":"How do I get access to Soteria? <p>       You will first need to request access. Follow the instructions in our Secure HPC page to see all the steps that are required.     </p> Why am I getting \"Operation timed out\" when I try to ssh to Soteria? <p>       It's possible you're not connected to the VPN. You will need to be connected to access any Soteria resources.     </p> How do I transfer my data to Soteria? <p>       The easiest way to transfer your data to Soteria is using Globus. We have a high assurance endpoint set up accessible under the endpoint UA HPC HIPAA Filesystems.     </p> Does my personal computer's Globus endpoint need to be set to high assurance? <p>     It depends.      You do not need your personal computer's Globus endpoint set to high assurance for the purposes of transferring data to Soteria using our UArizona HPC HIPAA Filesystems endpoint. HIPAA requires data transfer auditing. We log transfers on our DTN that use the HIPAA endpoint, regardless of the client's managed status.          If you are working with another institution/endpoint that only allows connections from high assurance endpoints, then you must enable High Assurance during your Globus Personal Connect configuration process. This requires that you are added to our Globus subscription which can be done by opening a support ticket to request help from our consultants. Your endpoint must be set to public to allow us to add you. Once you have been added, you may set your endpoint to private.      </p>"},{"location":"support_and_training/glossary/","title":"Glossary","text":"<p>Cluster</p> <p>A group of nodes connected to each other by a fast network.  The network in El Gato and Ocelote is 56Gb Infiniband.  What this gains for the user is the ability to connect nodes together to perform work beyond the capacity of a single node. Some jobs use hundreds of cores and terabytes of memory.</p> <p>CPU/processor/socket/core</p> <p>These terms are often used interchangeably, especially processor and CPU. The most straight forward way to think of the compute nodes is that they contain two physical sockets (or processor chips) which are located under their heatsinks. Each socket contains multiple cores.  Each core functions like a separate processor. Ocelote has 2 sockets with 14 cores in each so all you need to know is that there are 28 cores.  El Gato has 2 sockets with 6 cores in each, for a total of 12 cores. If your laptop is quad core, it has one socket with four cores, as a comparison.</p> <p>Data Transfer Node (DTN)</p> <p>A node connected to the public internet and dedicated to moving data to/from external computers. We have two DTN nodes known collectively as <code>filexfer.hpc.arizona.edu</code>.</p> <p>Distributed memory computing</p> <p>In software, a program or group of programs that run on multiple nodes or shared-memory instances and use programs such as MPI to communicate between the nodes. In hardware, a cluster that runs distributed-memory programs. Distributed-memory programs are limited in memory size only by job limits to support many users. </p> <p>Embarrassingly parallel</p> <p>A program where little effort is involved in separating the code into parallel tasks and so parallel scaling is very efficient.  Some astronomy codes fit this model.</p> <p>Encumbered hours</p> <p>This refers to any hours in your allocation that are reserved by running jobs. When you submit a job, any hours that it requires are moved to the \"encumbered\" category. As soon as your job ends, unused hours will be refunded and used hours will permanently be deducted from your monthly allotment. </p> <p>GPU</p> <p>A graphical processing unit, a specialized type of CPU derived from a graphics card. Effectively has hundreds of small cores. For certain tasks (those that can be effectively parallelized), a GPU is much faster than a general-purpose CPU.</p> <p>Head node</p> <p>The head node is for managing the cluster and is not available to users.</p> <p>HPC</p> <p>High performance computing. Implies a program too large for, or that takes too long on, a laptop or workstation. Also HTC (high throughput computing), similar, but oriented to processing many small compute jobs.</p> <p>Hyperthreading</p> <p>Intel processors (in this case \"cores\") have hyper-threading which can make one core look like two; but it does not add compute capacity in most HPC cases, so we turn it off.</p> <p>Login node</p> <p>A cluster node accessible to users and dedicated to logins, editing, moving data, submitting jobs. We have two of them and it does not matter which one you connect to. Sometimes referred to as shell nodes.</p> <p>MPI computing</p> <p>Message passing interface, the software standard used for most programs that use distributed memory. MPI calls lower-level functions, either networking or shared memory. On a cluster that means it can run transparently either on one node or multiple nodes. MPI has multiple implementations (OpenMPI, MVAPICH, OpenMPI or Intel MPI) that must be used consistently to both compile and run an MPI program.</p> <p>Network bandwidth</p> <p>The amount of data that can be moved over a network per second. For FDR Infiniband on Ocelote that is 56 Gbps (Giga bits per second)</p> <p>Network latency</p> <p>In HPC terms, it is usually the delay in the network for messages being passed from one node to another.  This is optimized by a hardware technology called RDMA (Remote Direct Memory Access)</p> <p>Node (aka compute node)</p> <p>A single computer in a box, functionally similar to a desktop computer but typically more powerful and packaged for rackmount in a datacenter. Usually two CPU sockets or four sockets with very large memory vs. one socket for a desktop. Ocelote standard nodes have 28 cores and 192GB memory.</p> <p>Parallel Programming</p> <p>A program that is either multi-tasking (like MPI) or multi-threaded (like OpenMP) or both, in order to effectively use more cores and more nodes and get more computing done. May be either shared-memory or distributed-memory. Unlike a serial program.</p> <p>Parallel Scaling</p> <p>The efficiency of a parallel program, usually defined as the parallel speedup of the program divided by the number of cores occupied. Speedup is defined as the serial run time divided by the parallel run time. Usually parallel computing introduces overhead, and scaling is less than 1 (or 100%).  In most cases, scaling starts at 1 on 1 core (by definition) and decreases as more cores are added, until some point is reached at which adding more cores adds overhead and makes the program slower.</p> <p>Scheduler/HPC scheduler</p> <p>A program that maintains a list of batch jobs to be executed on a cluster, ranks them in some priority order, and executes batch jobs on compute nodes as they become available. It tries to keep the cluster from being overloaded or idle. Puma, Ocelote, and El Gato use Slurm.</p> <p>Scratch storage</p> <p>A temporary file system, designed for speed rather than reliability, and the first tier in the storage hierarchy. On Puma these are internal SSD's and referenced as <code>/tmp</code>.</p> <p>Shared memory computing</p> <p>A program that runs multiple tasks or software threads, each of which sees the same available memory available from the operating system, and shares that memory using one of the multiple shared memory/multi-threading communication methods (OpenMP, pthreads, POSIX shm, MPI over shared memory, etc.). Shared memory programs cannot run across multiple nodes. Implies a limit (a little less than the amount of memory in the node) to the memory size of the running program.</p> <p>Single-threaded computing</p> <p>A software program that cannot take advantage of multi-threading because it was written without multi-threading support. Essentially can use only one core on one node regardless of the number of cores available. Multiple single-threaded programs can be run on a single node on multiple cores.</p> <p>SSD</p> <p>Solid state disk, memory chips packaged with an interface that appears to the computer to be a disk drive. Faster than rotating disk drives and still more expensive, though decreasing in price over time.</p> <p>Storage hierarchy</p> <p>Each tier of storage is larger and slower than the preceding tier. The first is data in the processor including the processor cache.  The next tier is memory.  Page or swap is an extension of memory but is very inefficient since it actually writes to disk. You should next consider <code>/tmp</code> which is the local disk on each node.  You have no access to <code>/tmp</code> once the job ends.  Shared storage is all of <code>/home</code>, <code>/groups/PI</code>, and <code>/xdisk</code>, and is the slowest.</p> <p>Supercomputer</p> <p>A large and powerful cluster. We currently have three: Puma, Ocelote, and El Gato.</p> <p>VM or virtual machine</p> <p>This compute model is not usually found in the HPC environment.  It is a method of running several or many virtual machines on one physical machine.  Since HPC nodes are busy most of the time the cost of the VM overhead and management is not worthwhile. </p>"},{"location":"support_and_training/grants/","title":"Grants","text":""},{"location":"support_and_training/grants/#grant-resources","title":"Grant Resources","text":""},{"location":"support_and_training/grants/#overview","title":"Overview","text":"<p>University of Arizona (UArizona) researchers have access to a variety of high performance computing and storage resources through both UArizona HPC and the NSF-funded CyVerse project. A moderate amount of compute time and storage is free to any research faculty, also known as principal investigators (PIs).</p> <p>You might consider these scenarios:</p> <ol> <li> <p>Run proof-of-concept on the HPC clusters</p> <p>Compute time and storage are free on a moderate scale so no funding is needed to test and develop code, workflows, and methodologies for grants.</p> </li> <li> <p>Run the funded compute on the HPC clusters </p> <p>Using our buy-in model, the compute part of the funding can complement the existing compute resources and reduce personnel costs.</p> </li> <li> <p>Co-locate your cluster in the Computer Center</p> <p>Take advantage of the enterprise class facilities and support to save space and reduce administration and setup costs. More information here.</p> </li> <li> <p>CyVerse data center</p> <p>These reside at UArizona, are funded by the National Science Foundation\u2019s Directorate for Biological Sciences, and provide life scientists with powerful computational infrastructure.</p> </li> <li> <p>ACCESS (formerly XSEDE) national cyberinfrastructure HPC and Cloud providers.</p> <p>Available for free to any US-based researcher. Get started here. UArizona researchers can request startup allocations to test out their system(s) of choice. After the startup allocation, full allocations can be requested for millions of CPU-hours.</p> </li> </ol>"},{"location":"support_and_training/grants/#the-university-of-arizona-research-computing-and-cyberinfrastructure-plan-2022","title":"The University of Arizona Research Computing and Cyberinfrastructure Plan 2022","text":"<p> Click here to download .docx version <p></p>"},{"location":"support_and_training/grants/#data-management-plans","title":"Data Management Plans","text":"<p>A data management plan documents the lifecycle of your data. The plan provides details on data collection for storage, access, sharing, and reproducibility of your results.  A good data management plan will ensure the availability and accessibility of your research results after your project is complete and you have published the results, increasing the value of your research and possible reuse by other researchers. </p> <p>The UArizona Libraries host automated tools to design and execute data management plans for grants. More information here: https://data.library.arizona.edu/</p>"},{"location":"support_and_training/grants/#funding-agency-requirements","title":"Funding Agency Requirements","text":"<p>In 1999, the U.S. Office of Management and Budget amended OMB Circular A-110 to require research data produced with funding from Federal agencies be made publicly available through procedures established through the Freedom of Information Act (FOIA).</p> <p>Federally funded research - access to publications and data</p> <p>The White House Office of Science and Technology Policy (OSTP) released a policy memorandum, \u201cIncreasing Access to the Results of Federally Funded Scientific Research,\u201d on February 22, 2013. See the Federal Agency Policies for Public Access website for brief descriptions of the policies for top UArizona federal funding agencies and where to get further help.</p> <p>The Library has created a summary table of public access plans for all of the 19 federal funding agencies that fall under the OSTP memo.</p> <p>The University offers other resources for grant applications here: https://research.arizona.edu/development/training-grants-resources</p>"},{"location":"support_and_training/people/","title":"Support Staff","text":""},{"location":"support_and_training/people/#research-computing-support-staff","title":"Research Computing Support Staff","text":"<p>Welcome to our Support Staff page, where you can get acquainted with the individuals who make up our HPC consulting team. For contact information, see our Consulting Services page. </p>"},{"location":"support_and_training/people/#research-computing-facilitation-manager","title":"Research Computing Facilitation Manager","text":"Chris Reidy <p>Chris Reidy is a dedicated academic professional currently serving at the University of Arizona, where he has distinguished himself through his expertise and passion for his field. With a background in education and administration, Reidy has contributed significantly to the university community, demonstrating a commitment to fostering learning environments conducive to student success. Known for his collaborative spirit and innovative approach to challenges, he has played a pivotal role in various educational initiatives and administrative endeavors within the university. Reidy's dedication to excellence and his unwavering support for both students and colleagues alike make him an invaluable asset to the University of Arizona community. And by the way, ChatGPT wrote this for me.</p>"},{"location":"support_and_training/people/#hpc-consulting","title":"HPC Consulting","text":"Ethan Jahn <p>Ethan has been a scientific programmer for over 10 years, and a user of High Performance Computing resources for over 7 years. They earned their PhD in Computational Astrophysics from UC Riverside in 2021, and spent two years as an Adjunct Professor of Physics at Riverside City College before joining the UArizona HPC team in August 2023. They continue to incorporate education, training, and outreach into their consulting practice, and have an interest in interdisciplinary approaches to science communication.</p> Soham Pal <p>Soham is originally from India. He has been a scientific programmer and a HPC resource user for about 10 years. He earned his PhD in Computational Nuclear Physics from Iowa State University in 2022, developing software for the computation of electromagnetic properties of atomic nuclei. He worked at Amazon Web Services, before jonining the U of A HPC team in July 2023.</p> Sara Willis <p>Sara Willis is originally from the San Francisco Bay Area and graduated from The University of Arizona with a double major in math and physics. Following graduation, she transitioned to a role as a bioinformatician performing research in protein evolution. Finding she was weirdly passionate about software installations, writing and optimizing code, and debugging, she joined the HPC consulting team in 2019 to assist researchers with their computation.</p> Derrick Zwickl <p>Derrick has worked with HPC resources for nearly 25 years as a researcher, software developer and support staff.  He  joined the UArizona HPC consulting team in early 2022.  Derrick is originally from Tucson, and was an undergraduate at  the University of Arizona.  He obtained a Ph.D. in Evolutional Biology from the University of Texas, developing software for the estimation of evolutionary trees from DNA sequences. </p>"},{"location":"support_and_training/people/#visualization-consulting","title":"Visualization Consulting","text":"Devin Bayly <p>I'm a member of the Research Technologies Data &amp; Visualization team at the University of Arizona. I work with researchers, students and faculty to bring your data to life through drop ins, 1:1 consultations, collaborative projects both short &amp; long term, and general protocols enhancing the visualization offerings of our High Performance Computing (HPC).</p>"},{"location":"support_and_training/website_accessibility/","title":"Website Accessibility","text":""},{"location":"support_and_training/website_accessibility/#keyboard-navigation","title":"Keyboard Navigation","text":"<p>Navigation through this documentation can be done using only a keyboard. To navigate between menu items and links, use the Tab key. To navigate between different tabs (for example, what's shown below), use the arrow keys &lt; and &gt;.</p> <p>If you are having issues with mouseless navigation, you may need to enable certain features in your browser. Below</p> FirefoxSafari <ol> <li>In your address bar, type about:config</li> <li>In the search bar, enter accessibility.tabfocus</li> <li>Select Number on the right (the other options are Boolean and String) and click the plus sign on the right +</li> <li>Enter 7 and click the blue check mark</li> </ol> <ol> <li>Go to Settings in the browser</li> <li>Select Advanced</li> <li> <p>Under Accessibility select Press Tab to highlight each item on a webpage</p> <p></p> </li> </ol>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/research/","title":"Research","text":""},{"location":"blog/category/conferences/","title":"Conferences","text":""},{"location":"blog/category/storage/","title":"Storage","text":""},{"location":"blog/category/ai/","title":"AI","text":""}]}